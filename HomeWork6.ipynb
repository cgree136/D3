{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8njbgxzIrk1RTZgOiCy3k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cgree136/D3/blob/main/HomeWork6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Load the housing dataset (replace 'your_dataset.csv' with the actual file path)\n",
        "data = pd.DataFrame({\n",
        "    'price': [13300000,\t12250000,\t12250000,\t12215000,\t11410000,\t10850000,\t10150000,\t10150000,\t9870000,\t9800000,\t9800000,\t9681000,\t9310000,\t9240000,\t9240000,\t9100000,\t9100000,\t8960000,\t8890000,\t8855000,\t8750000,\t8680000,\t8645000,\t8645000,\t8575000,\t8540000,\t8463000,\t8400000,\t8400000,\t8400000,\t8400000,\t8400000,\t8295000,\t8190000,\t8120000,\t8080940,\t8043000,\t7980000,\t7962500,\t7910000,\t7875000,\t7840000,\t7700000,\t7700000,\t7560000,\t7560000,\t7525000,\t7490000,\t7455000,\t7420000,\t7420000,\t7420000,\t7350000,\t7350000,\t7350000,\t7350000,\t7343000,\t7245000,\t7210000,\t7210000,\t7140000,\t7070000,\t7070000,\t7035000,\t7000000,\t6930000,\t6930000,\t6895000,\t6860000,\t6790000,\t6790000,\t6755000,\t6720000,\t6685000,\t6650000,\t6650000,\t6650000,\t6650000,\t6650000,\t6650000,\t6629000,\t6615000,\t6615000,\t6580000,\t6510000,\t6510000,\t6510000,\t6475000,\t6475000,\t6440000,\t6440000,\t6419000,\t6405000,\t6300000,\t6300000,\t6300000,\t6300000,\t6300000,\t6293000,\t6265000,\t6230000,\t6230000,\t6195000,\t6195000,\t6195000,\t6160000,\t6160000,\t6125000,\t6107500,\t6090000,\t6090000,\t6090000,\t6083000,\t6083000,\t6020000,\t6020000,\t6020000,\t5950000,\t5950000,\t5950000,\t5950000,\t5950000,\t5950000,\t5950000,\t5950000,\t5943000,\t5880000,\t5880000,\t5873000,\t5873000,\t5866000,\t5810000,\t5810000,\t5810000,\t5803000,\t5775000,\t5740000,\t5740000,\t5740000,\t5740000,\t5740000,\t5652500,\t5600000,\t5600000,\t5600000,\t5600000,\t5600000,\t5600000,\t5600000,\t5600000,\t5600000,\t5565000,\t5565000,\t5530000,\t5530000,\t5530000,\t5523000,\t5495000,\t5495000,\t5460000,\t5460000,\t5460000,\t5460000,\t5425000,\t5390000,\t5383000,\t5320000,\t5285000,\t5250000,\t5250000,\t5250000,\t5250000,\t5250000,\t5250000,\t5250000,\t5250000,\t5250000,\t5243000,\t5229000,\t5215000,\t5215000,\t5215000,\t5145000,\t5145000,\t5110000,\t5110000,\t5110000,\t5110000,\t5075000,\t5040000,\t5040000,\t5040000,\t5040000,\t5033000,\t5005000,\t4970000,\t4970000,\t4956000,\t4935000,\t4907000,\t4900000,\t4900000,\t4900000,\t4900000,\t4900000,\t4900000,\t4900000,\t4900000,\t4900000,\t4900000,\t4900000,\t4900000,\t4893000,\t4893000,\t4865000,\t4830000,\t4830000,\t4830000,\t4830000,\t4795000,\t4795000,\t4767000,\t4760000,\t4760000,\t4760000,\t4753000,\t4690000,\t4690000,\t4690000,\t4690000,\t4690000,\t4690000,\t4655000,\t4620000,\t4620000,\t4620000,\t4620000,\t4620000,\t4613000,\t4585000,\t4585000,\t4550000,\t4550000,\t4550000,\t4550000,\t4550000,\t4550000,\t4550000,\t4543000,\t4543000,\t4515000,\t4515000,\t4515000,\t4515000,\t4480000,\t4480000,\t4480000,\t4480000,\t4480000,\t4473000,\t4473000,\t4473000,\t4445000,\t4410000,\t4410000,\t4403000,\t4403000,\t4403000,\t4382000,\t4375000,\t4340000,\t4340000,\t4340000,\t4340000,\t4340000,\t4319000,\t4305000,\t4305000,\t4277000,\t4270000,\t4270000,\t4270000,\t4270000,\t4270000,\t4270000,\t4235000,\t4235000,\t4200000,\t4200000,\t4200000,\t4200000,\t4200000,\t4200000,\t4200000,\t4200000,\t4200000,\t4200000,\t4200000,\t4200000,\t4200000,\t4200000,\t4200000,\t4200000,\t4200000,\t4193000,\t4193000,\t4165000,\t4165000,\t4165000,\t4130000,\t4130000,\t4123000,\t4098500,\t4095000,\t4095000,\t4095000,\t4060000,\t4060000,\t4060000,\t4060000,\t4060000,\t4025000,\t4025000,\t4025000,\t4007500,\t4007500,\t3990000,\t3990000,\t3990000,\t3990000,\t3990000,\t3920000,\t3920000,\t3920000,\t3920000,\t3920000,\t3920000,\t3920000,\t3885000,\t3885000,\t3850000,\t3850000,\t3850000,\t3850000,\t3850000,\t3850000,\t3850000,\t3836000,\t3815000,\t3780000,\t3780000,\t3780000,\t3780000,\t3780000,\t3780000,\t3773000,\t3773000,\t3773000,\t3745000,\t3710000,\t3710000,\t3710000,\t3710000,\t3710000,\t3703000,\t3703000,\t3675000,\t3675000,\t3675000,\t3675000,\t3640000,\t3640000,\t3640000,\t3640000,\t3640000,\t3640000,\t3640000,\t3640000,\t3640000,\t3633000,\t3605000,\t3605000,\t3570000,\t3570000,\t3570000,\t3570000,\t3535000,\t3500000,\t3500000,\t3500000,\t3500000,\t3500000,\t3500000,\t3500000,\t3500000,\t3500000,\t3500000,\t3500000,\t3500000,\t3500000,\t3500000,\t3500000,\t3500000,\t3500000,\t3493000,\t3465000,\t3465000,\t3465000,\t3430000,\t3430000,\t3430000,\t3430000,\t3430000,\t3430000,\t3423000,\t3395000,\t3395000,\t3395000,\t3360000,\t3360000,\t3360000,\t3360000,\t3360000,\t3360000,\t3360000,\t3360000,\t3353000,\t3332000,\t3325000,\t3325000,\t3290000,\t3290000,\t3290000,\t3290000,\t3290000,\t3290000,\t3290000,\t3290000,\t3255000,\t3255000,\t3234000,\t3220000,\t3220000,\t3220000,\t3220000,\t3150000,\t3150000,\t3150000,\t3150000,\t3150000,\t3150000,\t3150000,\t3150000,\t3150000,\t3143000,\t3129000,\t3118850,\t3115000,\t3115000,\t3115000,\t3087000,\t3080000,\t3080000,\t3080000,\t3080000,\t3045000,\t3010000,\t3010000,\t3010000,\t3010000,\t3010000,\t3010000,\t3010000,\t3003000,\t2975000,\t2961000,\t2940000,\t2940000,\t2940000,\t2940000,\t2940000,\t2940000,\t2940000,\t2940000,\t2870000,\t2870000,\t2870000,\t2870000,\t2852500,\t2835000,\t2835000,\t2835000,\t2800000,\t2800000,\t2730000,\t2730000,\t2695000,\t2660000,\t2660000,\t2660000,\t2660000,\t2660000,\t2660000,\t2660000,\t2653000,\t2653000,\t2604000,\t2590000,\t2590000,\t2590000,\t2520000,\t2520000,\t2520000,\t2485000,\t2485000,\t2450000,\t2450000,\t2450000,\t2450000,\t2450000,\t2450000,\t2408000,\t2380000,\t2380000,\t2380000,\t2345000,\t2310000,\t2275000,\t2275000,\t2275000,\t2240000,\t2233000,\t2135000,\t2100000,\t2100000,\t2100000,\t1960000,\t1890000,\t1890000,\t1855000,\t1820000,\t1767150,\t1750000,\t1750000,\t1750000],\n",
        "    'area': [7420,\t8960,\t9960,\t7500,\t7420,\t7500,\t8580,\t16200,\t8100,\t5750,\t13200,\t6000,\t6550,\t3500,\t7800,\t6000,\t6600,\t8500,\t4600,\t6420,\t4320,\t7155,\t8050,\t4560,\t8800,\t6540,\t6000,\t8875,\t7950,\t5500,\t7475,\t7000,\t4880,\t5960,\t6840,\t7000,\t7482,\t9000,\t6000,\t6000,\t6550,\t6360,\t6480,\t6000,\t6000,\t6000,\t6000,\t6600,\t4300,\t7440,\t7440,\t6325,\t6000,\t5150,\t6000,\t6000,\t11440,\t9000,\t7680,\t6000,\t6000,\t8880,\t6240,\t6360,\t11175,\t8880,\t13200,\t7700,\t6000,\t12090,\t4000,\t6000,\t5020,\t6600,\t4040,\t4260,\t6420,\t6500,\t5700,\t6000,\t6000,\t4000,\t10500,\t6000,\t3760,\t8250,\t6670,\t3960,\t7410,\t8580,\t5000,\t6750,\t4800,\t7200,\t6000,\t4100,\t9000,\t6400,\t6600,\t6000,\t6600,\t5500,\t5500,\t6350,\t5500,\t4500,\t5450,\t6420,\t3240,\t6615,\t6600,\t8372,\t4300,\t9620,\t6800,\t8000,\t6900,\t3700,\t6420,\t7020,\t6540,\t7231,\t6254,\t7320,\t6525,\t15600,\t7160,\t6500,\t5500,\t11460,\t4800,\t5828,\t5200,\t4800,\t7000,\t6000,\t5400,\t4640,\t5000,\t6360,\t5800,\t6660,\t10500,\t4800,\t4700,\t5000,\t10500,\t5500,\t6360,\t6600,\t5136,\t4400,\t5400,\t3300,\t3650,\t6100,\t6900,\t2817,\t7980,\t3150,\t6210,\t6100,\t6600,\t6825,\t6710,\t6450,\t7800,\t4600,\t4260,\t6540,\t5500,\t10269,\t8400,\t5300,\t3800,\t9800,\t8520,\t6050,\t7085,\t3180,\t4500,\t7200,\t3410,\t7980,\t3000,\t3000,\t11410,\t6100,\t5720,\t3540,\t7600,\t10700,\t6600,\t4800,\t8150,\t4410,\t7686,\t2800,\t5948,\t4200,\t4520,\t4095,\t4120,\t5400,\t4770,\t6300,\t5800,\t3000,\t2970,\t6720,\t4646,\t12900,\t3420,\t4995,\t4350,\t4160,\t6040,\t6862,\t4815,\t7000,\t8100,\t3420,\t9166,\t6321,\t10240,\t6440,\t5170,\t6000,\t3630,\t9667,\t5400,\t4320,\t3745,\t4160,\t3880,\t5680,\t2870,\t5010,\t4510,\t4000,\t3840,\t3760,\t3640,\t2550,\t5320,\t5360,\t3520,\t8400,\t4100,\t4990,\t3510,\t3450,\t9860,\t3520,\t4510,\t5885,\t4000,\t8250,\t4040,\t6360,\t3162,\t3510,\t3750,\t3968,\t4900,\t2880,\t4880,\t4920,\t4950,\t3900,\t4500,\t1905,\t4075,\t3500,\t6450,\t4032,\t4400,\t10360,\t3400,\t6360,\t6360,\t4500,\t2175,\t4360,\t7770,\t6650,\t2787,\t5500,\t5040,\t5850,\t2610,\t2953,\t2747,\t4410,\t4000,\t2325,\t4600,\t3640,\t5800,\t7000,\t4079,\t3520,\t2145,\t4500,\t8250,\t3450,\t4840,\t4080,\t4046,\t4632,\t5985,\t6060,\t3600,\t3680,\t4040,\t5600,\t5900,\t4992,\t4340,\t3000,\t4320,\t3630,\t3460,\t5400,\t4500,\t3460,\t4100,\t6480,\t4500,\t3960,\t4050,\t7260,\t5500,\t3000,\t3290,\t3816,\t8080,\t2145,\t3780,\t3180,\t5300,\t3180,\t7152,\t4080,\t3850,\t2015,\t2176,\t3350,\t3150,\t4820,\t3420,\t3600,\t5830,\t2856,\t8400,\t8250,\t2520,\t6930,\t3480,\t3600,\t4040,\t6020,\t4050,\t3584,\t3120,\t5450,\t3630,\t3630,\t5640,\t3600,\t4280,\t3570,\t3180,\t3000,\t3520,\t5960,\t4130,\t2850,\t2275,\t3520,\t4500,\t4000,\t3150,\t4500,\t4500,\t3640,\t3850,\t4240,\t3650,\t4600,\t2135,\t3036,\t3990,\t7424,\t3480,\t3600,\t3640,\t5900,\t3120,\t7350,\t3512,\t9500,\t5880,\t12944,\t4900,\t3060,\t5320,\t2145,\t4000,\t3185,\t3850,\t2145,\t2610,\t1950,\t4040,\t4785,\t3450,\t3640,\t3500,\t4960,\t4120,\t4750,\t3720,\t3750,\t3100,\t3185,\t2700,\t2145,\t4040,\t4775,\t2500,\t3180,\t6060,\t3480,\t3792,\t4040,\t2145,\t5880,\t4500,\t3930,\t3640,\t4370,\t2684,\t4320,\t3120,\t3450,\t3986,\t3500,\t4095,\t1650,\t3450,\t6750,\t9000,\t3069,\t4500,\t5495,\t2398,\t3000,\t3850,\t3500,\t8100,\t4960,\t2160,\t3090,\t4500,\t3800,\t3090,\t3240,\t2835,\t4600,\t5076,\t3750,\t3630,\t8050,\t4352,\t3000,\t5850,\t4960,\t3600,\t3660,\t3480,\t2700,\t3150,\t6615,\t3040,\t3630,\t6000,\t5400,\t5200,\t3300,\t4350,\t2640,\t2650,\t3960,\t6800,\t4000,\t4000,\t3934,\t2000,\t3630,\t2800,\t2430,\t3480,\t4000,\t3185,\t4000,\t2910,\t3600,\t4400,\t3600,\t2880,\t3180,\t3000,\t4400,\t3000,\t3210,\t3240,\t3000,\t3500,\t4840,\t7700,\t3635,\t2475,\t2787,\t3264,\t3640,\t3180,\t1836,\t3970,\t3970,\t1950,\t5300,\t3000,\t2400,\t3000,\t3360,\t3420,\t1700,\t3649,\t2990,\t3000,\t2400,\t3620,\t2910,\t3850],\n",
        "    'bedrooms': [4,\t4,\t3,\t4,\t4,\t3,\t4,\t5,\t4,\t3,\t3,\t4,\t4,\t4,\t3,\t4,\t4,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t4,\t3,\t3,\t5,\t4,\t3,\t3,\t4,\t3,\t5,\t3,\t3,\t4,\t3,\t4,\t3,\t3,\t3,\t4,\t4,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t4,\t3,\t3,\t3,\t4,\t4,\t4,\t3,\t3,\t2,\t4,\t4,\t3,\t3,\t2,\t3,\t3,\t4,\t3,\t4,\t3,\t2,\t3,\t4,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t5,\t3,\t2,\t3,\t3,\t4,\t3,\t3,\t3,\t3,\t4,\t3,\t3,\t3,\t3,\t3,\t3,\t4,\t3,\t4,\t4,\t3,\t3,\t6,\t3,\t2,\t3,\t3,\t4,\t3,\t3,\t3,\t3,\t4,\t4,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t4,\t3,\t3,\t3,\t3,\t4,\t4,\t3,\t3,\t3,\t4,\t4,\t5,\t4,\t3,\t2,\t3,\t3,\t4,\t3,\t4,\t5,\t3,\t3,\t3,\t3,\t4,\t3,\t3,\t4,\t3,\t4,\t3,\t3,\t3,\t3,\t2,\t4,\t4,\t3,\t3,\t3,\t4,\t3,\t4,\t3,\t3,\t3,\t3,\t4,\t3,\t3,\t3,\t3,\t3,\t2,\t3,\t2,\t2,\t4,\t3,\t3,\t2,\t3,\t4,\t3,\t3,\t3,\t3,\t3,\t3,\t2,\t4,\t3,\t3,\t2,\t3,\t3,\t3,\t3,\t3,\t4,\t4,\t2,\t3,\t3,\t3,\t2,\t3,\t4,\t4,\t2,\t3,\t2,\t2,\t3,\t2,\t3,\t4,\t2,\t3,\t3,\t3,\t3,\t3,\t2,\t3,\t4,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t4,\t2,\t4,\t3,\t3,\t3,\t2,\t4,\t2,\t3,\t3,\t3,\t2,\t3,\t3,\t2,\t3,\t2,\t3,\t3,\t3,\t4,\t3,\t3,\t5,\t3,\t4,\t4,\t2,\t2,\t2,\t3,\t2,\t2,\t2,\t3,\t4,\t2,\t3,\t3,\t3,\t3,\t2,\t4,\t3,\t4,\t2,\t4,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t3,\t4,\t3,\t2,\t3,\t3,\t2,\t2,\t4,\t3,\t3,\t4,\t3,\t3,\t3,\t3,\t3,\t4,\t4,\t3,\t3,\t3,\t2,\t3,\t4,\t3,\t2,\t2,\t3,\t4,\t2,\t4,\t5,\t2,\t3,\t2,\t2,\t3,\t2,\t3,\t2,\t3,\t2,\t2,\t2,\t3,\t2,\t3,\t5,\t4,\t2,\t3,\t2,\t3,\t2,\t2,\t3,\t2,\t2,\t2,\t2,\t2,\t2,\t3,\t3,\t2,\t2,\t3,\t3,\t3,\t3,\t3,\t2,\t2,\t3,\t4,\t2,\t2,\t3,\t3,\t3,\t4,\t3,\t3,\t3,\t3,\t3,\t6,\t2,\t2,\t3,\t2,\t2,\t3,\t2,\t3,\t3,\t3,\t2,\t3,\t2,\t2,\t3,\t3,\t3,\t3,\t2,\t3,\t3,\t2,\t4,\t4,\t2,\t2,\t2,\t3,\t3,\t2,\t3,\t3,\t2,\t4,\t2,\t4,\t3,\t4,\t4,\t2,\t3,\t3,\t2,\t2,\t4,\t3,\t2,\t3,\t3,\t1,\t2,\t2,\t2,\t3,\t3,\t2,\t3,\t2,\t3,\t3,\t3,\t3,\t3,\t2,\t2,\t2,\t3,\t2,\t2,\t2,\t3,\t3,\t2,\t2,\t3,\t3,\t4,\t2,\t4,\t2,\t3,\t2,\t3,\t4,\t3,\t2,\t3,\t3,\t2,\t2,\t2,\t4,\t4,\t3,\t3,\t2,\t3,\t3,\t2,\t3,\t2,\t2,\t2,\t3,\t3,\t3,\t2,\t3,\t2,\t3,\t2,\t2,\t2,\t2,\t3,\t3,\t2,\t3,\t3,\t3,\t2,\t2,\t2,\t2,\t2,\t2,\t3,\t4,\t2,\t2,\t2,\t2,\t1,\t3,\t3,\t3,\t2,\t3,\t4,\t2,\t5,\t3,\t2,\t2,\t2,\t3,\t2,\t3,\t3],\n",
        "    'bathrooms': [2,\t4,\t2,\t2,\t1,\t3,\t3,\t3,\t1,\t2,\t1,\t3,\t2,\t2,\t2,\t1,\t2,\t2,\t2,\t2,\t1,\t2,\t1,\t2,\t2,\t2,\t2,\t1,\t2,\t2,\t2,\t1,\t2,\t3,\t1,\t2,\t2,\t2,\t1,\t2,\t1,\t2,\t2,\t2,\t2,\t2,\t2,\t1,\t2,\t2,\t2,\t1,\t2,\t2,\t2,\t1,\t1,\t2,\t2,\t2,\t2,\t1,\t2,\t2,\t1,\t2,\t1,\t2,\t1,\t2,\t2,\t2,\t1,\t2,\t1,\t2,\t2,\t2,\t1,\t2,\t1,\t2,\t2,\t2,\t1,\t2,\t1,\t1,\t1,\t3,\t1,\t1,\t2,\t2,\t2,\t2,\t1,\t1,\t2,\t1,\t2,\t1,\t2,\t2,\t2,\t1,\t2,\t1,\t1,\t2,\t1,\t1,\t2,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t2,\t2,\t2,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t2,\t1,\t1,\t1,\t2,\t2,\t2,\t2,\t1,\t1,\t1,\t2,\t1,\t2,\t1,\t1,\t1,\t3,\t2,\t2,\t1,\t2,\t1,\t2,\t1,\t1,\t2,\t1,\t2,\t2,\t1,\t2,\t1,\t2,\t2,\t1,\t1,\t2,\t1,\t2,\t1,\t1,\t1,\t2,\t2,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t3,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t3,\t1,\t2,\t1,\t2,\t1,\t2,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t2,\t2,\t1,\t1,\t1,\t2,\t2,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t2,\t2,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t2,\t2,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t3,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1],\n",
        "    'stories': [3,\t4,\t2,\t2,\t2,\t1,\t4,\t2,\t2,\t4,\t2,\t2,\t2,\t2,\t2,\t2,\t2,\t4,\t2,\t2,\t2,\t1,\t1,\t2,\t2,\t2,\t4,\t1,\t2,\t2,\t4,\t4,\t2,\t2,\t2,\t4,\t3,\t4,\t4,\t4,\t2,\t4,\t4,\t4,\t4,\t3,\t4,\t4,\t2,\t1,\t4,\t4,\t4,\t4,\t2,\t2,\t2,\t4,\t4,\t4,\t2,\t1,\t2,\t3,\t1,\t2,\t1,\t1,\t1,\t2,\t2,\t4,\t4,\t4,\t2,\t2,\t3,\t3,\t1,\t3,\t2,\t2,\t1,\t4,\t2,\t3,\t3,\t1,\t1,\t2,\t2,\t1,\t4,\t1,\t4,\t3,\t1,\t1,\t3,\t3,\t1,\t3,\t4,\t3,\t1,\t4,\t1,\t3,\t3,\t2,\t1,\t3,\t2,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t2,\t1,\t2,\t4,\t1,\t1,\t3,\t3,\t3,\t1,\t4,\t3,\t3,\t1,\t4,\t2,\t2,\t3,\t1,\t4,\t2,\t2,\t3,\t2,\t4,\t1,\t2,\t3,\t1,\t2,\t2,\t2,\t2,\t2,\t1,\t1,\t2,\t1,\t1,\t4,\t3,\t2,\t1,\t2,\t1,\t1,\t1,\t2,\t2,\t1,\t1,\t2,\t1,\t2,\t2,\t1,\t1,\t1,\t2,\t1,\t2,\t2,\t1,\t2,\t2,\t2,\t1,\t2,\t1,\t2,\t2,\t1,\t1,\t1,\t2,\t1,\t2,\t2,\t2,\t2,\t2,\t1,\t2,\t1,\t1,\t1,\t2,\t3,\t1,\t2,\t1,\t2,\t1,\t1,\t3,\t1,\t2,\t1,\t2,\t4,\t2,\t1,\t2,\t1,\t1,\t4,\t1,\t2,\t2,\t2,\t1,\t2,\t1,\t2,\t2,\t2,\t2,\t2,\t2,\t2,\t1,\t2,\t2,\t2,\t2,\t1,\t4,\t1,\t2,\t3,\t2,\t1,\t2,\t2,\t1,\t2,\t1,\t2,\t1,\t2,\t2,\t1,\t2,\t2,\t2,\t1,\t2,\t2,\t2,\t3,\t2,\t1,\t2,\t2,\t1,\t1,\t1,\t2,\t1,\t2,\t1,\t2,\t2,\t1,\t2,\t1,\t2,\t2,\t1,\t2,\t2,\t2,\t1,\t2,\t2,\t2,\t2,\t1,\t1,\t3,\t2,\t3,\t1,\t1,\t2,\t2,\t2,\t2,\t2,\t1,\t1,\t1,\t2,\t2,\t1,\t2,\t2,\t1,\t3,\t2,\t2,\t1,\t1,\t2,\t2,\t1,\t2,\t2,\t2,\t2,\t1,\t2,\t2,\t1,\t1,\t1,\t1,\t2,\t2,\t2,\t1,\t2,\t1,\t1,\t2,\t2,\t2,\t1,\t2,\t2,\t1,\t1,\t3,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t2,\t2,\t1,\t2,\t2,\t2,\t3,\t1,\t1,\t1,\t2,\t2,\t1,\t1,\t1,\t2,\t2,\t2,\t2,\t2,\t2,\t1,\t1,\t2,\t1,\t1,\t2,\t1,\t1,\t2,\t1,\t1,\t2,\t1,\t1,\t3,\t1,\t1,\t1,\t3,\t2,\t2,\t1,\t2,\t1,\t1,\t2,\t3,\t2,\t1,\t1,\t1,\t2,\t1,\t1,\t2,\t1,\t2,\t1,\t2,\t1,\t2,\t2,\t1,\t2,\t1,\t1,\t1,\t2,\t2,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t2,\t2,\t1,\t2,\t1,\t2,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t2,\t1,\t2,\t1,\t2,\t2,\t1,\t1,\t1,\t2,\t2,\t1,\t2,\t2,\t2,\t1,\t2,\t2,\t2,\t1,\t2,\t2,\t1,\t1,\t1,\t2,\t3,\t2,\t2,\t1,\t2,\t1,\t1,\t2,\t1,\t1,\t2,\t2,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t2,\t1,\t1,\t2,\t2,\t2,\t2,\t1,\t1,\t1,\t2,\t1,\t1,\t2,\t2,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t2,\t2,\t1,\t2,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t2],\n",
        "    'parking': [2,\t3,\t2,\t3,\t2,\t2,\t2,\t0,\t2,\t1,\t2,\t2,\t1,\t2,\t0,\t2,\t1,\t2,\t2,\t1,\t2,\t2,\t1,\t1,\t2,\t2,\t0,\t1,\t2,\t1,\t2,\t2,\t1,\t1,\t1,\t2,\t1,\t2,\t2,\t1,\t0,\t0,\t2,\t2,\t1,\t0,\t1,\t3,\t1,\t0,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t2,\t1,\t1,\t1,\t2,\t1,\t2,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t2,\t0,\t1,\t1,\t1,\t0,\t2,\t0,\t0,\t2,\t2,\t2,\t0,\t2,\t0,\t3,\t1,\t2,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t2,\t0,\t0,\t0,\t1,\t1,\t2,\t2,\t0,\t2,\t2,\t2,\t0,\t0,\t0,\t2,\t2,\t0,\t1,\t0,\t1,\t2,\t2,\t0,\t1,\t2,\t0,\t0,\t0,\t0,\t2,\t0,\t2,\t1,\t0,\t2,\t0,\t1,\t1,\t0,\t1,\t0,\t1,\t1,\t0,\t0,\t0,\t2,\t0,\t0,\t2,\t2,\t0,\t1,\t2,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t2,\t2,\t0,\t0,\t0,\t1,\t2,\t0,\t1,\t2,\t2,\t0,\t2,\t2,\t2,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t2,\t0,\t1,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t2,\t2,\t2,\t0,\t0,\t0,\t2,\t2,\t0,\t0,\t2,\t0,\t2,\t1,\t2,\t3,\t0,\t1,\t2,\t1,\t0,\t0,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t1,\t1,\t2,\t0,\t0,\t0,\t2,\t0,\t3,\t0,\t0,\t0,\t1,\t0,\t0,\t2,\t1,\t2,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t2,\t1,\t0,\t0,\t1,\t0,\t2,\t2,\t0,\t0,\t1,\t1,\t2,\t0,\t0,\t2,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t2,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t2,\t3,\t0,\t0,\t1,\t0,\t3,\t1,\t1,\t2,\t1,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t1,\t2,\t0,\t2,\t2,\t2,\t1,\t3,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t3,\t0,\t0,\t1,\t2,\t2,\t0,\t0,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t2,\t0,\t1,\t2,\t1,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t0,\t2,\t0,\t0,\t2,\t0,\t0,\t0,\t2,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t3,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t0,\t1,\t2,\t0,\t2,\t0,\t0,\t0,\t0,\t0,\t0,\t2,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t2,\t0,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t3,\t0,\t1,\t0,\t1,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t2,\t0,\t0,\t0,\t0],\n",
        "    'mainroad': [1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t0,\t1,\t0,\t1,\t0,\t1,\t1,\t1,\t0,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t0,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t0,\t0,\t1,\t1,\t0,\t1,\t0,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t1,\t1,\t0,\t1,\t0,\t1,\t0,\t1],\n",
        "    'guestroom': [0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t1,\t1,\t1,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t1,\t1,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0],\n",
        "    'basement': [0,\t0,\t1,\t1,\t1,\t1,\t0,\t0,\t1,\t0,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t0,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t1,\t1,\t0,\t1,\t1,\t0,\t0,\t0,\t1,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t1,\t1,\t0,\t1,\t1,\t1,\t0,\t1,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t0,\t1,\t0,\t1,\t0,\t1,\t0,\t0,\t1,\t1,\t0,\t1,\t1,\t0,\t0,\t1,\t1,\t0,\t1,\t0,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t1,\t1,\t1,\t0,\t1,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t1,\t1,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t1,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t1,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t1,\t0,\t1,\t0,\t1,\t0,\t1,\t1,\t1,\t0,\t0,\t1,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t1,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0],\n",
        "    'hotwaterheating': [0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0],\n",
        "    'aircondition': [1,\t1,\t0,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t0,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t0,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t1,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t0,\t1,\t0,\t0,\t1,\t0,\t1,\t0,\t1,\t1,\t0,\t1,\t0,\t1,\t1,\t0,\t1,\t0,\t1,\t1,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t1,\t1,\t1,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t1,\t1,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t1,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t1,\t1,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0],\n",
        "    'prefarea': [1,\t0,\t1,\t1,\t0,\t1,\t1,\t0,\t1,\t1,\t1,\t0,\t1,\t0,\t1,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t1,\t1,\t0,\t0,\t1,\t1,\t1,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t1,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0],\n",
        "    })\n",
        "\n",
        "# Define input features and target variable\n",
        "X = data[['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'aircondition', 'prefarea']].values  # Convert to NumPy array\n",
        "y = data['price'].values\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Create a Fully Connected Neural Network with one hidden layer\n",
        "model_1a = Sequential()\n",
        "model_1a.add(Dense(32, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model_1a.add(Dense(1, activation='linear'))  # Assuming you're predicting a continuous variable\n",
        "\n",
        "# Compile the model\n",
        "model_1a.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history_1a = model_1a.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_val_scaled, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "train_loss_1a = history_1a.history['loss'][-1]\n",
        "val_loss_1a = history_1a.history['val_loss'][-1]\n",
        "\n",
        "# Linear Regression as a baseline\n",
        "linear_reg = LinearRegression()\n",
        "linear_reg.fit(X_train_scaled, y_train)\n",
        "linear_reg_pred = linear_reg.predict(X_val_scaled)\n",
        "linear_reg_mse = mean_squared_error(y_val, linear_reg_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGCXCNyw3FRl",
        "outputId": "5cb1d17d-7eff-4901-be67-4efe462024ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "14/14 [==============================] - 1s 16ms/step - loss: 25234799329280.0000 - val_loss: 30130000887808.0000\n",
            "Epoch 2/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234797232128.0000 - val_loss: 30129996693504.0000\n",
            "Epoch 3/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234795134976.0000 - val_loss: 30129992499200.0000\n",
            "Epoch 4/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234793037824.0000 - val_loss: 30129988304896.0000\n",
            "Epoch 5/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234786746368.0000 - val_loss: 30129988304896.0000\n",
            "Epoch 6/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 25234786746368.0000 - val_loss: 30129984110592.0000\n",
            "Epoch 7/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234784649216.0000 - val_loss: 30129984110592.0000\n",
            "Epoch 8/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234780454912.0000 - val_loss: 30129977819136.0000\n",
            "Epoch 9/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234778357760.0000 - val_loss: 30129975721984.0000\n",
            "Epoch 10/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234774163456.0000 - val_loss: 30129971527680.0000\n",
            "Epoch 11/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234772066304.0000 - val_loss: 30129969430528.0000\n",
            "Epoch 12/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234767872000.0000 - val_loss: 30129963139072.0000\n",
            "Epoch 13/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234765774848.0000 - val_loss: 30129961041920.0000\n",
            "Epoch 14/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234759483392.0000 - val_loss: 30129956847616.0000\n",
            "Epoch 15/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234755289088.0000 - val_loss: 30129954750464.0000\n",
            "Epoch 16/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234751094784.0000 - val_loss: 30129946361856.0000\n",
            "Epoch 17/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234746900480.0000 - val_loss: 30129942167552.0000\n",
            "Epoch 18/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234742706176.0000 - val_loss: 30129933778944.0000\n",
            "Epoch 19/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234738511872.0000 - val_loss: 30129929584640.0000\n",
            "Epoch 20/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234736414720.0000 - val_loss: 30129925390336.0000\n",
            "Epoch 21/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234725928960.0000 - val_loss: 30129917001728.0000\n",
            "Epoch 22/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234723831808.0000 - val_loss: 30129906515968.0000\n",
            "Epoch 23/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234715443200.0000 - val_loss: 30129900224512.0000\n",
            "Epoch 24/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234711248896.0000 - val_loss: 30129891835904.0000\n",
            "Epoch 25/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 25234698665984.0000 - val_loss: 30129883447296.0000\n",
            "Epoch 26/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 25234694471680.0000 - val_loss: 30129870864384.0000\n",
            "Epoch 27/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234681888768.0000 - val_loss: 30129864572928.0000\n",
            "Epoch 28/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 25234673500160.0000 - val_loss: 30129854087168.0000\n",
            "Epoch 29/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234665111552.0000 - val_loss: 30129841504256.0000\n",
            "Epoch 30/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234654625792.0000 - val_loss: 30129831018496.0000\n",
            "Epoch 31/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234644140032.0000 - val_loss: 30129816338432.0000\n",
            "Epoch 32/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 25234631557120.0000 - val_loss: 30129803755520.0000\n",
            "Epoch 33/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 25234625265664.0000 - val_loss: 30129789075456.0000\n",
            "Epoch 34/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234612682752.0000 - val_loss: 30129774395392.0000\n",
            "Epoch 35/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234598002688.0000 - val_loss: 30129759715328.0000\n",
            "Epoch 36/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234581225472.0000 - val_loss: 30129742938112.0000\n",
            "Epoch 37/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234570739712.0000 - val_loss: 30129724063744.0000\n",
            "Epoch 38/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234556059648.0000 - val_loss: 30129709383680.0000\n",
            "Epoch 39/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234539282432.0000 - val_loss: 30129692606464.0000\n",
            "Epoch 40/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234524602368.0000 - val_loss: 30129673732096.0000\n",
            "Epoch 41/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234509922304.0000 - val_loss: 30129656954880.0000\n",
            "Epoch 42/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234491047936.0000 - val_loss: 30129635983360.0000\n",
            "Epoch 43/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234472173568.0000 - val_loss: 30129615011840.0000\n",
            "Epoch 44/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 25234459590656.0000 - val_loss: 30129591943168.0000\n",
            "Epoch 45/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234438619136.0000 - val_loss: 30129573068800.0000\n",
            "Epoch 46/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234417647616.0000 - val_loss: 30129550000128.0000\n",
            "Epoch 47/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234398773248.0000 - val_loss: 30129524834304.0000\n",
            "Epoch 48/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 25234377801728.0000 - val_loss: 30129503862784.0000\n",
            "Epoch 49/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234358927360.0000 - val_loss: 30129476599808.0000\n",
            "Epoch 50/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234340052992.0000 - val_loss: 30129451433984.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Fully Connected Neural Network with three hidden layers\n",
        "model_1b = Sequential()\n",
        "model_1b.add(Dense(32, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model_1b.add(Dense(64, activation='relu'))\n",
        "model_1b.add(Dense(16, activation='relu'))\n",
        "model_1b.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model_1b.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history_1b = model_1b.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_val_scaled, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "train_loss_1b = history_1b.history['loss'][-1]\n",
        "val_loss_1b = history_1b.history['val_loss'][-1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX9XFYsD4zBm",
        "outputId": "3cf9218c-6799-4dc2-a71e-bf28625992d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "14/14 [==============================] - 1s 14ms/step - loss: 25234786746368.0000 - val_loss: 30129988304896.0000\n",
            "Epoch 2/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234782552064.0000 - val_loss: 30129971527680.0000\n",
            "Epoch 3/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25234769969152.0000 - val_loss: 30129954750464.0000\n",
            "Epoch 4/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234740609024.0000 - val_loss: 30129906515968.0000\n",
            "Epoch 5/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234688180224.0000 - val_loss: 30129831018496.0000\n",
            "Epoch 6/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 25234598002688.0000 - val_loss: 30129692606464.0000\n",
            "Epoch 7/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234447007744.0000 - val_loss: 30129461919744.0000\n",
            "Epoch 8/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25234193252352.0000 - val_loss: 30129115889664.0000\n",
            "Epoch 9/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25233828347904.0000 - val_loss: 30128579018752.0000\n",
            "Epoch 10/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25233285185536.0000 - val_loss: 30127819849728.0000\n",
            "Epoch 11/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25232498753536.0000 - val_loss: 30126752399360.0000\n",
            "Epoch 12/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25231427108864.0000 - val_loss: 30125292781568.0000\n",
            "Epoch 13/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25229967491072.0000 - val_loss: 30123334041600.0000\n",
            "Epoch 14/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25228071665664.0000 - val_loss: 30120769224704.0000\n",
            "Epoch 15/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25225563471872.0000 - val_loss: 30117497667584.0000\n",
            "Epoch 16/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25222419841024.0000 - val_loss: 30113301266432.0000\n",
            "Epoch 17/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25218558984192.0000 - val_loss: 30108018540544.0000\n",
            "Epoch 18/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25213653745664.0000 - val_loss: 30101731278848.0000\n",
            "Epoch 19/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25207710416896.0000 - val_loss: 30094053605376.0000\n",
            "Epoch 20/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25200777232384.0000 - val_loss: 30084492689408.0000\n",
            "Epoch 21/50\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 25192220852224.0000 - val_loss: 30073350520832.0000\n",
            "Epoch 22/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 25182011916288.0000 - val_loss: 30060551602176.0000\n",
            "Epoch 23/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 25170293030912.0000 - val_loss: 30045382901760.0000\n",
            "Epoch 24/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 25156741234688.0000 - val_loss: 30027439669248.0000\n",
            "Epoch 25/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25140964360192.0000 - val_loss: 30006862413824.0000\n",
            "Epoch 26/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 25122547171328.0000 - val_loss: 29984087343104.0000\n",
            "Epoch 27/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 25102007664640.0000 - val_loss: 29957478678528.0000\n",
            "Epoch 28/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 25078590865408.0000 - val_loss: 29927009157120.0000\n",
            "Epoch 29/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 25052022046720.0000 - val_loss: 29892666195968.0000\n",
            "Epoch 30/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 25022150213632.0000 - val_loss: 29854690967552.0000\n",
            "Epoch 31/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 24989671620608.0000 - val_loss: 29811479150592.0000\n",
            "Epoch 32/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 24952073879552.0000 - val_loss: 29764838490112.0000\n",
            "Epoch 33/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 24911317827584.0000 - val_loss: 29712768303104.0000\n",
            "Epoch 34/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 24865824309248.0000 - val_loss: 29655415390208.0000\n",
            "Epoch 35/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 24817411555328.0000 - val_loss: 29590793748480.0000\n",
            "Epoch 36/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 24760287232000.0000 - val_loss: 29523758284800.0000\n",
            "Epoch 37/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 24702688952320.0000 - val_loss: 29445662441472.0000\n",
            "Epoch 38/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 24635649294336.0000 - val_loss: 29363196133376.0000\n",
            "Epoch 39/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 24566644604928.0000 - val_loss: 29270057418752.0000\n",
            "Epoch 40/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 24486432735232.0000 - val_loss: 29175530389504.0000\n",
            "Epoch 41/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 24406753542144.0000 - val_loss: 29068407865344.0000\n",
            "Epoch 42/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 24316030746624.0000 - val_loss: 28955880980480.0000\n",
            "Epoch 43/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 24221059121152.0000 - val_loss: 28834046935040.0000\n",
            "Epoch 44/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 24117474492416.0000 - val_loss: 28706267463680.0000\n",
            "Epoch 45/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 24007325777920.0000 - val_loss: 28568794955776.0000\n",
            "Epoch 46/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 23892202618880.0000 - val_loss: 28417716125696.0000\n",
            "Epoch 47/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 23764028882944.0000 - val_loss: 28261409095680.0000\n",
            "Epoch 48/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 23633850269696.0000 - val_loss: 28092898738176.0000\n",
            "Epoch 49/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 23493810847744.0000 - val_loss: 27915022499840.0000\n",
            "Epoch 50/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 23341018644480.0000 - val_loss: 27732918403072.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import time\n",
        "\n",
        "# Load CIFAR-10 data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "X_train = X_train.reshape((X_train.shape[0], -1)).astype('float32') / 255\n",
        "X_test = X_test.reshape((X_test.shape[0], -1)).astype('float32') / 255\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Create a Fully Connected Neural Network with one hidden layer\n",
        "model_2a = Sequential()\n",
        "model_2a.add(Dense(512, input_dim=X_train.shape[1], activation='relu'))\n",
        "model_2a.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_2a.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "history_2a = model_2a.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "end_time = time.time()\n",
        "\n",
        "# Evaluate the model\n",
        "train_loss_2a, train_acc_2a = model_2a.evaluate(X_train, y_train)\n",
        "val_loss_2a, val_acc_2a = model_2a.evaluate(X_test, y_test)\n",
        "training_time_2a = end_time - start_time\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6smX7JaE425L",
        "outputId": "7251bc82-2bfe-4906-c8ec-9c6e890cc8d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n",
            "Epoch 1/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 1.9215 - accuracy: 0.3150 - val_loss: 1.7854 - val_accuracy: 0.3697\n",
            "Epoch 2/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.7337 - accuracy: 0.3810 - val_loss: 1.7313 - val_accuracy: 0.3799\n",
            "Epoch 3/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.6641 - accuracy: 0.4061 - val_loss: 1.6717 - val_accuracy: 0.4040\n",
            "Epoch 4/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.6248 - accuracy: 0.4208 - val_loss: 1.6369 - val_accuracy: 0.4260\n",
            "Epoch 5/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.5899 - accuracy: 0.4332 - val_loss: 1.6189 - val_accuracy: 0.4272\n",
            "Epoch 6/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.5637 - accuracy: 0.4431 - val_loss: 1.6368 - val_accuracy: 0.4176\n",
            "Epoch 7/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.5439 - accuracy: 0.4493 - val_loss: 1.5665 - val_accuracy: 0.4482\n",
            "Epoch 8/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.5279 - accuracy: 0.4572 - val_loss: 1.5873 - val_accuracy: 0.4399\n",
            "Epoch 9/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.5150 - accuracy: 0.4602 - val_loss: 1.5502 - val_accuracy: 0.4595\n",
            "Epoch 10/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.4946 - accuracy: 0.4688 - val_loss: 1.5912 - val_accuracy: 0.4436\n",
            "Epoch 11/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.4869 - accuracy: 0.4707 - val_loss: 1.5739 - val_accuracy: 0.4462\n",
            "Epoch 12/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.4767 - accuracy: 0.4742 - val_loss: 1.5429 - val_accuracy: 0.4553\n",
            "Epoch 13/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.4657 - accuracy: 0.4770 - val_loss: 1.5223 - val_accuracy: 0.4652\n",
            "Epoch 14/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.4567 - accuracy: 0.4823 - val_loss: 1.5421 - val_accuracy: 0.4602\n",
            "Epoch 15/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.4499 - accuracy: 0.4863 - val_loss: 1.5670 - val_accuracy: 0.4430\n",
            "Epoch 16/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.4402 - accuracy: 0.4890 - val_loss: 1.5422 - val_accuracy: 0.4590\n",
            "Epoch 17/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.4309 - accuracy: 0.4922 - val_loss: 1.5238 - val_accuracy: 0.4651\n",
            "Epoch 18/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.4289 - accuracy: 0.4930 - val_loss: 1.5428 - val_accuracy: 0.4601\n",
            "Epoch 19/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.4249 - accuracy: 0.4913 - val_loss: 1.5361 - val_accuracy: 0.4589\n",
            "Epoch 20/50\n",
            "1250/1250 [==============================] - 28s 23ms/step - loss: 1.4146 - accuracy: 0.4970 - val_loss: 1.5490 - val_accuracy: 0.4591\n",
            "Epoch 21/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.4071 - accuracy: 0.4976 - val_loss: 1.5351 - val_accuracy: 0.4675\n",
            "Epoch 22/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3959 - accuracy: 0.5020 - val_loss: 1.5769 - val_accuracy: 0.4566\n",
            "Epoch 23/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3917 - accuracy: 0.5058 - val_loss: 1.5320 - val_accuracy: 0.4659\n",
            "Epoch 24/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3874 - accuracy: 0.5046 - val_loss: 1.5497 - val_accuracy: 0.4552\n",
            "Epoch 25/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3804 - accuracy: 0.5071 - val_loss: 1.5647 - val_accuracy: 0.4581\n",
            "Epoch 26/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3758 - accuracy: 0.5116 - val_loss: 1.5353 - val_accuracy: 0.4645\n",
            "Epoch 27/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3700 - accuracy: 0.5124 - val_loss: 1.5737 - val_accuracy: 0.4571\n",
            "Epoch 28/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3675 - accuracy: 0.5135 - val_loss: 1.5283 - val_accuracy: 0.4704\n",
            "Epoch 29/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3602 - accuracy: 0.5138 - val_loss: 1.5577 - val_accuracy: 0.4528\n",
            "Epoch 30/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3607 - accuracy: 0.5153 - val_loss: 1.5427 - val_accuracy: 0.4663\n",
            "Epoch 31/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3519 - accuracy: 0.5177 - val_loss: 1.4972 - val_accuracy: 0.4812\n",
            "Epoch 32/50\n",
            "1250/1250 [==============================] - 28s 23ms/step - loss: 1.3509 - accuracy: 0.5196 - val_loss: 1.5348 - val_accuracy: 0.4628\n",
            "Epoch 33/50\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 1.3485 - accuracy: 0.5195 - val_loss: 1.5008 - val_accuracy: 0.4798\n",
            "Epoch 34/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3406 - accuracy: 0.5206 - val_loss: 1.5194 - val_accuracy: 0.4652\n",
            "Epoch 35/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3364 - accuracy: 0.5210 - val_loss: 1.5439 - val_accuracy: 0.4673\n",
            "Epoch 36/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3327 - accuracy: 0.5246 - val_loss: 1.5060 - val_accuracy: 0.4793\n",
            "Epoch 37/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3330 - accuracy: 0.5246 - val_loss: 1.5111 - val_accuracy: 0.4753\n",
            "Epoch 38/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.3295 - accuracy: 0.5264 - val_loss: 1.5536 - val_accuracy: 0.4650\n",
            "Epoch 39/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3201 - accuracy: 0.5268 - val_loss: 1.5324 - val_accuracy: 0.4666\n",
            "Epoch 40/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.3194 - accuracy: 0.5298 - val_loss: 1.5651 - val_accuracy: 0.4646\n",
            "Epoch 41/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3157 - accuracy: 0.5319 - val_loss: 1.6231 - val_accuracy: 0.4465\n",
            "Epoch 42/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3119 - accuracy: 0.5315 - val_loss: 1.5349 - val_accuracy: 0.4690\n",
            "Epoch 43/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3102 - accuracy: 0.5309 - val_loss: 1.5723 - val_accuracy: 0.4626\n",
            "Epoch 44/50\n",
            "1250/1250 [==============================] - 31s 24ms/step - loss: 1.3056 - accuracy: 0.5332 - val_loss: 1.6061 - val_accuracy: 0.4589\n",
            "Epoch 45/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3035 - accuracy: 0.5343 - val_loss: 1.5271 - val_accuracy: 0.4702\n",
            "Epoch 46/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.2972 - accuracy: 0.5385 - val_loss: 1.5389 - val_accuracy: 0.4739\n",
            "Epoch 47/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.3025 - accuracy: 0.5334 - val_loss: 1.5229 - val_accuracy: 0.4761\n",
            "Epoch 48/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.2956 - accuracy: 0.5362 - val_loss: 1.5441 - val_accuracy: 0.4707\n",
            "Epoch 49/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.2928 - accuracy: 0.5383 - val_loss: 1.5848 - val_accuracy: 0.4562\n",
            "Epoch 50/50\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 1.2892 - accuracy: 0.5383 - val_loss: 1.5712 - val_accuracy: 0.4687\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.3444 - accuracy: 0.5290\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 1.5488 - accuracy: 0.4727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Fully Connected Neural Network with three hidden layers\n",
        "model_2b = Sequential()\n",
        "model_2b.add(Dense(512, input_dim=X_train.shape[1], activation='relu'))\n",
        "model_2b.add(Dense(256, activation='relu'))\n",
        "model_2b.add(Dense(128, activation='relu'))\n",
        "model_2b.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_2b.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "history_2b = model_2b.fit(X_train, y_train, epochs=300, batch_size=32, validation_split=0.2)\n",
        "end_time = time.time()\n",
        "\n",
        "# Evaluate the model\n",
        "train_loss_2b, train_acc_2b = model_2b.evaluate(X_train, y_train)\n",
        "val_loss_2b, val_acc_2b = model_2b.evaluate(X_test, y_test)\n",
        "training_time_2b = end_time - start_time\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHT3Brh0465V",
        "outputId": "ed2ddaee-c246-4a34-e6f0-9656fd0aa215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 1.8975 - accuracy: 0.3083 - val_loss: 1.7715 - val_accuracy: 0.3650\n",
            "Epoch 2/300\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 1.7149 - accuracy: 0.3817 - val_loss: 1.6756 - val_accuracy: 0.4022\n",
            "Epoch 3/300\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 1.6241 - accuracy: 0.4187 - val_loss: 1.6717 - val_accuracy: 0.3898\n",
            "Epoch 4/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 1.5673 - accuracy: 0.4381 - val_loss: 1.6312 - val_accuracy: 0.4188\n",
            "Epoch 5/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 1.5237 - accuracy: 0.4536 - val_loss: 1.5544 - val_accuracy: 0.4462\n",
            "Epoch 6/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 1.4877 - accuracy: 0.4642 - val_loss: 1.5396 - val_accuracy: 0.4561\n",
            "Epoch 7/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 1.4600 - accuracy: 0.4739 - val_loss: 1.5214 - val_accuracy: 0.4586\n",
            "Epoch 8/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 1.4312 - accuracy: 0.4864 - val_loss: 1.5213 - val_accuracy: 0.4615\n",
            "Epoch 9/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 1.3988 - accuracy: 0.4979 - val_loss: 1.5339 - val_accuracy: 0.4603\n",
            "Epoch 10/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 1.3813 - accuracy: 0.5030 - val_loss: 1.5027 - val_accuracy: 0.4742\n",
            "Epoch 11/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 1.3565 - accuracy: 0.5125 - val_loss: 1.4758 - val_accuracy: 0.4751\n",
            "Epoch 12/300\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 1.3382 - accuracy: 0.5189 - val_loss: 1.5295 - val_accuracy: 0.4616\n",
            "Epoch 13/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 1.3160 - accuracy: 0.5294 - val_loss: 1.5407 - val_accuracy: 0.4623\n",
            "Epoch 14/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 1.2982 - accuracy: 0.5326 - val_loss: 1.5006 - val_accuracy: 0.4727\n",
            "Epoch 15/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 1.2715 - accuracy: 0.5408 - val_loss: 1.4829 - val_accuracy: 0.4819\n",
            "Epoch 16/300\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 1.2593 - accuracy: 0.5480 - val_loss: 1.4985 - val_accuracy: 0.4791\n",
            "Epoch 17/300\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 1.2440 - accuracy: 0.5508 - val_loss: 1.5476 - val_accuracy: 0.4724\n",
            "Epoch 18/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 1.2270 - accuracy: 0.5570 - val_loss: 1.4895 - val_accuracy: 0.4882\n",
            "Epoch 19/300\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 1.2103 - accuracy: 0.5634 - val_loss: 1.5070 - val_accuracy: 0.4791\n",
            "Epoch 20/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 1.1953 - accuracy: 0.5695 - val_loss: 1.5009 - val_accuracy: 0.4894\n",
            "Epoch 21/300\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 1.1791 - accuracy: 0.5734 - val_loss: 1.5365 - val_accuracy: 0.4826\n",
            "Epoch 22/300\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 1.1660 - accuracy: 0.5791 - val_loss: 1.5113 - val_accuracy: 0.4838\n",
            "Epoch 23/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 1.1420 - accuracy: 0.5878 - val_loss: 1.5337 - val_accuracy: 0.4882\n",
            "Epoch 24/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 1.1325 - accuracy: 0.5898 - val_loss: 1.5541 - val_accuracy: 0.4820\n",
            "Epoch 25/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 1.1229 - accuracy: 0.5934 - val_loss: 1.6021 - val_accuracy: 0.4780\n",
            "Epoch 26/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 1.1062 - accuracy: 0.5982 - val_loss: 1.6023 - val_accuracy: 0.4789\n",
            "Epoch 27/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 1.0901 - accuracy: 0.6060 - val_loss: 1.6206 - val_accuracy: 0.4723\n",
            "Epoch 28/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 1.0794 - accuracy: 0.6100 - val_loss: 1.6096 - val_accuracy: 0.4823\n",
            "Epoch 29/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 1.0628 - accuracy: 0.6148 - val_loss: 1.6076 - val_accuracy: 0.4872\n",
            "Epoch 30/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 1.0517 - accuracy: 0.6195 - val_loss: 1.6149 - val_accuracy: 0.4861\n",
            "Epoch 31/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 1.0408 - accuracy: 0.6229 - val_loss: 1.6294 - val_accuracy: 0.4915\n",
            "Epoch 32/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 1.0291 - accuracy: 0.6272 - val_loss: 1.7559 - val_accuracy: 0.4716\n",
            "Epoch 33/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 1.0197 - accuracy: 0.6296 - val_loss: 1.7034 - val_accuracy: 0.4787\n",
            "Epoch 34/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 1.0072 - accuracy: 0.6355 - val_loss: 1.7037 - val_accuracy: 0.4805\n",
            "Epoch 35/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.9991 - accuracy: 0.6381 - val_loss: 1.7098 - val_accuracy: 0.4816\n",
            "Epoch 36/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.9873 - accuracy: 0.6413 - val_loss: 1.7342 - val_accuracy: 0.4784\n",
            "Epoch 37/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.9756 - accuracy: 0.6442 - val_loss: 1.7461 - val_accuracy: 0.4837\n",
            "Epoch 38/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.9627 - accuracy: 0.6511 - val_loss: 1.7025 - val_accuracy: 0.4842\n",
            "Epoch 39/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.9564 - accuracy: 0.6513 - val_loss: 1.8543 - val_accuracy: 0.4741\n",
            "Epoch 40/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.9404 - accuracy: 0.6586 - val_loss: 1.8103 - val_accuracy: 0.4786\n",
            "Epoch 41/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.9340 - accuracy: 0.6605 - val_loss: 1.8731 - val_accuracy: 0.4651\n",
            "Epoch 42/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.9293 - accuracy: 0.6623 - val_loss: 1.8590 - val_accuracy: 0.4746\n",
            "Epoch 43/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.9187 - accuracy: 0.6660 - val_loss: 1.8127 - val_accuracy: 0.4719\n",
            "Epoch 44/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.9072 - accuracy: 0.6693 - val_loss: 1.8622 - val_accuracy: 0.4728\n",
            "Epoch 45/300\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.8947 - accuracy: 0.6740 - val_loss: 1.9532 - val_accuracy: 0.4685\n",
            "Epoch 46/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.8853 - accuracy: 0.6776 - val_loss: 1.9081 - val_accuracy: 0.4772\n",
            "Epoch 47/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.8846 - accuracy: 0.6780 - val_loss: 1.9455 - val_accuracy: 0.4669\n",
            "Epoch 48/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.8753 - accuracy: 0.6829 - val_loss: 1.9158 - val_accuracy: 0.4717\n",
            "Epoch 49/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.8606 - accuracy: 0.6873 - val_loss: 2.0096 - val_accuracy: 0.4737\n",
            "Epoch 50/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.8590 - accuracy: 0.6878 - val_loss: 1.9515 - val_accuracy: 0.4773\n",
            "Epoch 51/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.8464 - accuracy: 0.6938 - val_loss: 1.9514 - val_accuracy: 0.4810\n",
            "Epoch 52/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.8402 - accuracy: 0.6962 - val_loss: 2.0238 - val_accuracy: 0.4772\n",
            "Epoch 53/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.8359 - accuracy: 0.6959 - val_loss: 2.0625 - val_accuracy: 0.4786\n",
            "Epoch 54/300\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.8190 - accuracy: 0.7014 - val_loss: 2.0522 - val_accuracy: 0.4719\n",
            "Epoch 55/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.8197 - accuracy: 0.7004 - val_loss: 2.0999 - val_accuracy: 0.4801\n",
            "Epoch 56/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.8045 - accuracy: 0.7082 - val_loss: 2.1607 - val_accuracy: 0.4726\n",
            "Epoch 57/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.8075 - accuracy: 0.7056 - val_loss: 2.1062 - val_accuracy: 0.4793\n",
            "Epoch 58/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.8036 - accuracy: 0.7044 - val_loss: 2.1112 - val_accuracy: 0.4766\n",
            "Epoch 59/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.7897 - accuracy: 0.7129 - val_loss: 2.1537 - val_accuracy: 0.4724\n",
            "Epoch 60/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.7830 - accuracy: 0.7168 - val_loss: 2.1643 - val_accuracy: 0.4773\n",
            "Epoch 61/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.7849 - accuracy: 0.7150 - val_loss: 2.1849 - val_accuracy: 0.4756\n",
            "Epoch 62/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.7701 - accuracy: 0.7204 - val_loss: 2.2333 - val_accuracy: 0.4666\n",
            "Epoch 63/300\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.7600 - accuracy: 0.7240 - val_loss: 2.2520 - val_accuracy: 0.4628\n",
            "Epoch 64/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7595 - accuracy: 0.7216 - val_loss: 2.3211 - val_accuracy: 0.4687\n",
            "Epoch 65/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7495 - accuracy: 0.7275 - val_loss: 2.2701 - val_accuracy: 0.4750\n",
            "Epoch 66/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.7472 - accuracy: 0.7287 - val_loss: 2.2900 - val_accuracy: 0.4627\n",
            "Epoch 67/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.7483 - accuracy: 0.7249 - val_loss: 2.2531 - val_accuracy: 0.4769\n",
            "Epoch 68/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7368 - accuracy: 0.7304 - val_loss: 2.4874 - val_accuracy: 0.4613\n",
            "Epoch 69/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.7370 - accuracy: 0.7333 - val_loss: 2.3146 - val_accuracy: 0.4706\n",
            "Epoch 70/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7255 - accuracy: 0.7347 - val_loss: 2.3717 - val_accuracy: 0.4614\n",
            "Epoch 71/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.7294 - accuracy: 0.7352 - val_loss: 2.3755 - val_accuracy: 0.4710\n",
            "Epoch 72/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.7193 - accuracy: 0.7400 - val_loss: 2.4619 - val_accuracy: 0.4647\n",
            "Epoch 73/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7204 - accuracy: 0.7372 - val_loss: 2.4799 - val_accuracy: 0.4623\n",
            "Epoch 74/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.7123 - accuracy: 0.7394 - val_loss: 2.5416 - val_accuracy: 0.4744\n",
            "Epoch 75/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.7133 - accuracy: 0.7406 - val_loss: 2.4894 - val_accuracy: 0.4684\n",
            "Epoch 76/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.6903 - accuracy: 0.7491 - val_loss: 2.5838 - val_accuracy: 0.4673\n",
            "Epoch 77/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.7045 - accuracy: 0.7429 - val_loss: 2.5694 - val_accuracy: 0.4648\n",
            "Epoch 78/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7026 - accuracy: 0.7448 - val_loss: 2.4927 - val_accuracy: 0.4669\n",
            "Epoch 79/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.6824 - accuracy: 0.7503 - val_loss: 2.6140 - val_accuracy: 0.4704\n",
            "Epoch 80/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6765 - accuracy: 0.7542 - val_loss: 2.5973 - val_accuracy: 0.4542\n",
            "Epoch 81/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.6818 - accuracy: 0.7544 - val_loss: 2.6737 - val_accuracy: 0.4606\n",
            "Epoch 82/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.6687 - accuracy: 0.7565 - val_loss: 2.7016 - val_accuracy: 0.4687\n",
            "Epoch 83/300\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.6708 - accuracy: 0.7551 - val_loss: 2.7035 - val_accuracy: 0.4713\n",
            "Epoch 84/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6618 - accuracy: 0.7596 - val_loss: 2.6425 - val_accuracy: 0.4688\n",
            "Epoch 85/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.6533 - accuracy: 0.7612 - val_loss: 2.7794 - val_accuracy: 0.4613\n",
            "Epoch 86/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6470 - accuracy: 0.7631 - val_loss: 2.8173 - val_accuracy: 0.4638\n",
            "Epoch 87/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.6593 - accuracy: 0.7628 - val_loss: 2.6917 - val_accuracy: 0.4649\n",
            "Epoch 88/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.6482 - accuracy: 0.7650 - val_loss: 2.7751 - val_accuracy: 0.4675\n",
            "Epoch 89/300\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.6490 - accuracy: 0.7641 - val_loss: 2.7628 - val_accuracy: 0.4663\n",
            "Epoch 90/300\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.6404 - accuracy: 0.7653 - val_loss: 2.7441 - val_accuracy: 0.4652\n",
            "Epoch 91/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.6357 - accuracy: 0.7693 - val_loss: 2.8296 - val_accuracy: 0.4613\n",
            "Epoch 92/300\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.6191 - accuracy: 0.7740 - val_loss: 2.8464 - val_accuracy: 0.4660\n",
            "Epoch 93/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.6352 - accuracy: 0.7695 - val_loss: 2.8696 - val_accuracy: 0.4629\n",
            "Epoch 94/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.6262 - accuracy: 0.7713 - val_loss: 2.8847 - val_accuracy: 0.4672\n",
            "Epoch 95/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.6266 - accuracy: 0.7750 - val_loss: 2.7939 - val_accuracy: 0.4672\n",
            "Epoch 96/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6240 - accuracy: 0.7744 - val_loss: 2.8960 - val_accuracy: 0.4647\n",
            "Epoch 97/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6188 - accuracy: 0.7762 - val_loss: 2.8620 - val_accuracy: 0.4552\n",
            "Epoch 98/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.6100 - accuracy: 0.7771 - val_loss: 3.0596 - val_accuracy: 0.4620\n",
            "Epoch 99/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.6119 - accuracy: 0.7771 - val_loss: 3.0377 - val_accuracy: 0.4643\n",
            "Epoch 100/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.5992 - accuracy: 0.7831 - val_loss: 3.0798 - val_accuracy: 0.4604\n",
            "Epoch 101/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.6076 - accuracy: 0.7826 - val_loss: 2.9828 - val_accuracy: 0.4594\n",
            "Epoch 102/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.6014 - accuracy: 0.7845 - val_loss: 3.2536 - val_accuracy: 0.4650\n",
            "Epoch 103/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5986 - accuracy: 0.7828 - val_loss: 3.0288 - val_accuracy: 0.4608\n",
            "Epoch 104/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.5842 - accuracy: 0.7882 - val_loss: 3.1429 - val_accuracy: 0.4614\n",
            "Epoch 105/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5922 - accuracy: 0.7857 - val_loss: 3.2271 - val_accuracy: 0.4532\n",
            "Epoch 106/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.5876 - accuracy: 0.7864 - val_loss: 3.2176 - val_accuracy: 0.4647\n",
            "Epoch 107/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5974 - accuracy: 0.7828 - val_loss: 3.1233 - val_accuracy: 0.4623\n",
            "Epoch 108/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5755 - accuracy: 0.7908 - val_loss: 3.2365 - val_accuracy: 0.4629\n",
            "Epoch 109/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.5781 - accuracy: 0.7907 - val_loss: 3.2071 - val_accuracy: 0.4568\n",
            "Epoch 110/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.5810 - accuracy: 0.7898 - val_loss: 3.2434 - val_accuracy: 0.4638\n",
            "Epoch 111/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.5771 - accuracy: 0.7889 - val_loss: 3.1615 - val_accuracy: 0.4634\n",
            "Epoch 112/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.5611 - accuracy: 0.7962 - val_loss: 3.3300 - val_accuracy: 0.4579\n",
            "Epoch 113/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.5764 - accuracy: 0.7892 - val_loss: 3.2783 - val_accuracy: 0.4608\n",
            "Epoch 114/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.5609 - accuracy: 0.7973 - val_loss: 3.2178 - val_accuracy: 0.4535\n",
            "Epoch 115/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5670 - accuracy: 0.7962 - val_loss: 3.1918 - val_accuracy: 0.4566\n",
            "Epoch 116/300\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.5585 - accuracy: 0.7978 - val_loss: 3.2786 - val_accuracy: 0.4559\n",
            "Epoch 117/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.5525 - accuracy: 0.7982 - val_loss: 3.2710 - val_accuracy: 0.4624\n",
            "Epoch 118/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5546 - accuracy: 0.7994 - val_loss: 3.2971 - val_accuracy: 0.4614\n",
            "Epoch 119/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.5679 - accuracy: 0.7954 - val_loss: 3.3659 - val_accuracy: 0.4563\n",
            "Epoch 120/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5482 - accuracy: 0.8020 - val_loss: 3.4313 - val_accuracy: 0.4598\n",
            "Epoch 121/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5367 - accuracy: 0.8040 - val_loss: 3.5844 - val_accuracy: 0.4564\n",
            "Epoch 122/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5439 - accuracy: 0.8038 - val_loss: 3.4151 - val_accuracy: 0.4655\n",
            "Epoch 123/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5412 - accuracy: 0.8030 - val_loss: 3.6399 - val_accuracy: 0.4557\n",
            "Epoch 124/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5322 - accuracy: 0.8078 - val_loss: 3.5647 - val_accuracy: 0.4554\n",
            "Epoch 125/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5354 - accuracy: 0.8058 - val_loss: 3.3987 - val_accuracy: 0.4544\n",
            "Epoch 126/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5354 - accuracy: 0.8059 - val_loss: 3.5631 - val_accuracy: 0.4575\n",
            "Epoch 127/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.5215 - accuracy: 0.8102 - val_loss: 3.6321 - val_accuracy: 0.4575\n",
            "Epoch 128/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5463 - accuracy: 0.8041 - val_loss: 3.4650 - val_accuracy: 0.4537\n",
            "Epoch 129/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5198 - accuracy: 0.8117 - val_loss: 3.6537 - val_accuracy: 0.4547\n",
            "Epoch 130/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5247 - accuracy: 0.8121 - val_loss: 3.5757 - val_accuracy: 0.4564\n",
            "Epoch 131/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5321 - accuracy: 0.8068 - val_loss: 3.6633 - val_accuracy: 0.4579\n",
            "Epoch 132/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5106 - accuracy: 0.8169 - val_loss: 3.7767 - val_accuracy: 0.4615\n",
            "Epoch 133/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5295 - accuracy: 0.8090 - val_loss: 3.5958 - val_accuracy: 0.4594\n",
            "Epoch 134/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5091 - accuracy: 0.8156 - val_loss: 3.7022 - val_accuracy: 0.4520\n",
            "Epoch 135/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5150 - accuracy: 0.8133 - val_loss: 3.8819 - val_accuracy: 0.4600\n",
            "Epoch 136/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5165 - accuracy: 0.8137 - val_loss: 3.6033 - val_accuracy: 0.4652\n",
            "Epoch 137/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.5132 - accuracy: 0.8139 - val_loss: 3.7665 - val_accuracy: 0.4555\n",
            "Epoch 138/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5097 - accuracy: 0.8137 - val_loss: 3.7221 - val_accuracy: 0.4612\n",
            "Epoch 139/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5127 - accuracy: 0.8142 - val_loss: 3.7723 - val_accuracy: 0.4526\n",
            "Epoch 140/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5204 - accuracy: 0.8128 - val_loss: 3.7709 - val_accuracy: 0.4610\n",
            "Epoch 141/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.5049 - accuracy: 0.8168 - val_loss: 3.8044 - val_accuracy: 0.4566\n",
            "Epoch 142/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5082 - accuracy: 0.8164 - val_loss: 3.7856 - val_accuracy: 0.4586\n",
            "Epoch 143/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4994 - accuracy: 0.8203 - val_loss: 3.7663 - val_accuracy: 0.4558\n",
            "Epoch 144/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5046 - accuracy: 0.8218 - val_loss: 3.8803 - val_accuracy: 0.4527\n",
            "Epoch 145/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.5037 - accuracy: 0.8194 - val_loss: 3.8682 - val_accuracy: 0.4544\n",
            "Epoch 146/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4988 - accuracy: 0.8219 - val_loss: 3.8178 - val_accuracy: 0.4595\n",
            "Epoch 147/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.4872 - accuracy: 0.8262 - val_loss: 3.9455 - val_accuracy: 0.4527\n",
            "Epoch 148/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5050 - accuracy: 0.8190 - val_loss: 3.9671 - val_accuracy: 0.4556\n",
            "Epoch 149/300\n",
            "1250/1250 [==============================] - 34s 28ms/step - loss: 0.4963 - accuracy: 0.8213 - val_loss: 3.7338 - val_accuracy: 0.4629\n",
            "Epoch 150/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.4843 - accuracy: 0.8245 - val_loss: 3.8917 - val_accuracy: 0.4630\n",
            "Epoch 151/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.4851 - accuracy: 0.8267 - val_loss: 3.9965 - val_accuracy: 0.4497\n",
            "Epoch 152/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4719 - accuracy: 0.8281 - val_loss: 4.0175 - val_accuracy: 0.4622\n",
            "Epoch 153/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4907 - accuracy: 0.8251 - val_loss: 4.0241 - val_accuracy: 0.4514\n",
            "Epoch 154/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4991 - accuracy: 0.8203 - val_loss: 4.1916 - val_accuracy: 0.4573\n",
            "Epoch 155/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4739 - accuracy: 0.8302 - val_loss: 3.8269 - val_accuracy: 0.4484\n",
            "Epoch 156/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.4868 - accuracy: 0.8249 - val_loss: 3.9486 - val_accuracy: 0.4519\n",
            "Epoch 157/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4625 - accuracy: 0.8324 - val_loss: 4.0739 - val_accuracy: 0.4576\n",
            "Epoch 158/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4843 - accuracy: 0.8273 - val_loss: 4.1330 - val_accuracy: 0.4542\n",
            "Epoch 159/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4756 - accuracy: 0.8300 - val_loss: 4.2074 - val_accuracy: 0.4518\n",
            "Epoch 160/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4805 - accuracy: 0.8269 - val_loss: 4.1565 - val_accuracy: 0.4555\n",
            "Epoch 161/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4678 - accuracy: 0.8309 - val_loss: 4.1246 - val_accuracy: 0.4659\n",
            "Epoch 162/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4785 - accuracy: 0.8297 - val_loss: 4.1120 - val_accuracy: 0.4461\n",
            "Epoch 163/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4691 - accuracy: 0.8308 - val_loss: 4.1907 - val_accuracy: 0.4539\n",
            "Epoch 164/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4647 - accuracy: 0.8329 - val_loss: 4.2845 - val_accuracy: 0.4527\n",
            "Epoch 165/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4659 - accuracy: 0.8332 - val_loss: 4.3281 - val_accuracy: 0.4590\n",
            "Epoch 166/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.4775 - accuracy: 0.8290 - val_loss: 4.3356 - val_accuracy: 0.4600\n",
            "Epoch 167/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4631 - accuracy: 0.8347 - val_loss: 4.1629 - val_accuracy: 0.4564\n",
            "Epoch 168/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4610 - accuracy: 0.8344 - val_loss: 4.1743 - val_accuracy: 0.4491\n",
            "Epoch 169/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4821 - accuracy: 0.8306 - val_loss: 4.1615 - val_accuracy: 0.4530\n",
            "Epoch 170/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.4626 - accuracy: 0.8352 - val_loss: 4.2599 - val_accuracy: 0.4487\n",
            "Epoch 171/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4592 - accuracy: 0.8364 - val_loss: 4.2696 - val_accuracy: 0.4487\n",
            "Epoch 172/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4381 - accuracy: 0.8442 - val_loss: 4.4345 - val_accuracy: 0.4525\n",
            "Epoch 173/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4654 - accuracy: 0.8342 - val_loss: 4.4742 - val_accuracy: 0.4554\n",
            "Epoch 174/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4491 - accuracy: 0.8376 - val_loss: 4.4364 - val_accuracy: 0.4465\n",
            "Epoch 175/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4615 - accuracy: 0.8358 - val_loss: 4.5112 - val_accuracy: 0.4522\n",
            "Epoch 176/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.4409 - accuracy: 0.8410 - val_loss: 4.5585 - val_accuracy: 0.4529\n",
            "Epoch 177/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.4423 - accuracy: 0.8422 - val_loss: 4.4816 - val_accuracy: 0.4566\n",
            "Epoch 178/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4597 - accuracy: 0.8384 - val_loss: 4.4710 - val_accuracy: 0.4552\n",
            "Epoch 179/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4520 - accuracy: 0.8386 - val_loss: 4.8326 - val_accuracy: 0.4572\n",
            "Epoch 180/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4416 - accuracy: 0.8419 - val_loss: 4.7719 - val_accuracy: 0.4533\n",
            "Epoch 181/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4584 - accuracy: 0.8354 - val_loss: 4.6985 - val_accuracy: 0.4502\n",
            "Epoch 182/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4445 - accuracy: 0.8404 - val_loss: 4.5180 - val_accuracy: 0.4469\n",
            "Epoch 183/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4538 - accuracy: 0.8372 - val_loss: 4.5474 - val_accuracy: 0.4508\n",
            "Epoch 184/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4460 - accuracy: 0.8411 - val_loss: 4.4348 - val_accuracy: 0.4521\n",
            "Epoch 185/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.4318 - accuracy: 0.8473 - val_loss: 4.7280 - val_accuracy: 0.4466\n",
            "Epoch 186/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4396 - accuracy: 0.8441 - val_loss: 4.8949 - val_accuracy: 0.4537\n",
            "Epoch 187/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.4541 - accuracy: 0.8402 - val_loss: 4.6621 - val_accuracy: 0.4493\n",
            "Epoch 188/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4231 - accuracy: 0.8499 - val_loss: 4.6042 - val_accuracy: 0.4502\n",
            "Epoch 189/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4412 - accuracy: 0.8434 - val_loss: 4.8517 - val_accuracy: 0.4491\n",
            "Epoch 190/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4201 - accuracy: 0.8500 - val_loss: 4.9420 - val_accuracy: 0.4443\n",
            "Epoch 191/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4373 - accuracy: 0.8432 - val_loss: 4.6389 - val_accuracy: 0.4557\n",
            "Epoch 192/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4346 - accuracy: 0.8465 - val_loss: 4.9313 - val_accuracy: 0.4477\n",
            "Epoch 193/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4311 - accuracy: 0.8468 - val_loss: 4.9243 - val_accuracy: 0.4494\n",
            "Epoch 194/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4460 - accuracy: 0.8415 - val_loss: 4.7740 - val_accuracy: 0.4460\n",
            "Epoch 195/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4324 - accuracy: 0.8464 - val_loss: 4.7919 - val_accuracy: 0.4512\n",
            "Epoch 196/300\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.4202 - accuracy: 0.8496 - val_loss: 4.8022 - val_accuracy: 0.4490\n",
            "Epoch 197/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4356 - accuracy: 0.8476 - val_loss: 4.8882 - val_accuracy: 0.4443\n",
            "Epoch 198/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.4203 - accuracy: 0.8499 - val_loss: 4.6721 - val_accuracy: 0.4569\n",
            "Epoch 199/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.4537 - accuracy: 0.8397 - val_loss: 4.7407 - val_accuracy: 0.4584\n",
            "Epoch 200/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4096 - accuracy: 0.8530 - val_loss: 4.8335 - val_accuracy: 0.4475\n",
            "Epoch 201/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.4401 - accuracy: 0.8440 - val_loss: 4.5797 - val_accuracy: 0.4536\n",
            "Epoch 202/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4134 - accuracy: 0.8538 - val_loss: 4.8617 - val_accuracy: 0.4539\n",
            "Epoch 203/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4236 - accuracy: 0.8483 - val_loss: 5.0029 - val_accuracy: 0.4551\n",
            "Epoch 204/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4149 - accuracy: 0.8514 - val_loss: 4.8576 - val_accuracy: 0.4546\n",
            "Epoch 205/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4211 - accuracy: 0.8518 - val_loss: 4.9304 - val_accuracy: 0.4466\n",
            "Epoch 206/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4270 - accuracy: 0.8479 - val_loss: 5.0569 - val_accuracy: 0.4531\n",
            "Epoch 207/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4227 - accuracy: 0.8495 - val_loss: 5.0654 - val_accuracy: 0.4545\n",
            "Epoch 208/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4418 - accuracy: 0.8491 - val_loss: 4.7529 - val_accuracy: 0.4467\n",
            "Epoch 209/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.4106 - accuracy: 0.8535 - val_loss: 4.9856 - val_accuracy: 0.4524\n",
            "Epoch 210/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4171 - accuracy: 0.8523 - val_loss: 4.8581 - val_accuracy: 0.4510\n",
            "Epoch 211/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4307 - accuracy: 0.8485 - val_loss: 5.1382 - val_accuracy: 0.4551\n",
            "Epoch 212/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4121 - accuracy: 0.8527 - val_loss: 5.0551 - val_accuracy: 0.4586\n",
            "Epoch 213/300\n",
            "1250/1250 [==============================] - 34s 28ms/step - loss: 0.4006 - accuracy: 0.8569 - val_loss: 5.0674 - val_accuracy: 0.4573\n",
            "Epoch 214/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.4110 - accuracy: 0.8523 - val_loss: 5.0579 - val_accuracy: 0.4528\n",
            "Epoch 215/300\n",
            "1250/1250 [==============================] - 34s 28ms/step - loss: 0.4313 - accuracy: 0.8493 - val_loss: 5.0322 - val_accuracy: 0.4562\n",
            "Epoch 216/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4020 - accuracy: 0.8575 - val_loss: 5.0450 - val_accuracy: 0.4544\n",
            "Epoch 217/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.3908 - accuracy: 0.8609 - val_loss: 5.1899 - val_accuracy: 0.4503\n",
            "Epoch 218/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.4264 - accuracy: 0.8497 - val_loss: 4.7928 - val_accuracy: 0.4550\n",
            "Epoch 219/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.3896 - accuracy: 0.8609 - val_loss: 5.0099 - val_accuracy: 0.4566\n",
            "Epoch 220/300\n",
            "1250/1250 [==============================] - 34s 28ms/step - loss: 0.4049 - accuracy: 0.8557 - val_loss: 4.9386 - val_accuracy: 0.4540\n",
            "Epoch 221/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4191 - accuracy: 0.8530 - val_loss: 5.3350 - val_accuracy: 0.4500\n",
            "Epoch 222/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4049 - accuracy: 0.8554 - val_loss: 5.4421 - val_accuracy: 0.4536\n",
            "Epoch 223/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.4202 - accuracy: 0.8533 - val_loss: 4.9760 - val_accuracy: 0.4526\n",
            "Epoch 224/300\n",
            "1250/1250 [==============================] - 34s 28ms/step - loss: 0.3876 - accuracy: 0.8621 - val_loss: 5.1754 - val_accuracy: 0.4391\n",
            "Epoch 225/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4344 - accuracy: 0.8491 - val_loss: 5.2404 - val_accuracy: 0.4507\n",
            "Epoch 226/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3805 - accuracy: 0.8655 - val_loss: 5.0953 - val_accuracy: 0.4497\n",
            "Epoch 227/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3937 - accuracy: 0.8592 - val_loss: 5.5681 - val_accuracy: 0.4509\n",
            "Epoch 228/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3967 - accuracy: 0.8608 - val_loss: 5.3062 - val_accuracy: 0.4514\n",
            "Epoch 229/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3961 - accuracy: 0.8594 - val_loss: 5.6777 - val_accuracy: 0.4489\n",
            "Epoch 230/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4129 - accuracy: 0.8548 - val_loss: 5.2127 - val_accuracy: 0.4463\n",
            "Epoch 231/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.3970 - accuracy: 0.8600 - val_loss: 5.5658 - val_accuracy: 0.4567\n",
            "Epoch 232/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.4076 - accuracy: 0.8576 - val_loss: 5.3785 - val_accuracy: 0.4491\n",
            "Epoch 233/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.3895 - accuracy: 0.8620 - val_loss: 5.3391 - val_accuracy: 0.4466\n",
            "Epoch 234/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3978 - accuracy: 0.8592 - val_loss: 5.3031 - val_accuracy: 0.4527\n",
            "Epoch 235/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3865 - accuracy: 0.8639 - val_loss: 5.5287 - val_accuracy: 0.4477\n",
            "Epoch 236/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.4304 - accuracy: 0.8497 - val_loss: 5.5358 - val_accuracy: 0.4502\n",
            "Epoch 237/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3655 - accuracy: 0.8701 - val_loss: 5.5424 - val_accuracy: 0.4479\n",
            "Epoch 238/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.4075 - accuracy: 0.8576 - val_loss: 5.3551 - val_accuracy: 0.4450\n",
            "Epoch 239/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3846 - accuracy: 0.8640 - val_loss: 5.6362 - val_accuracy: 0.4504\n",
            "Epoch 240/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3874 - accuracy: 0.8626 - val_loss: 5.4872 - val_accuracy: 0.4471\n",
            "Epoch 241/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3905 - accuracy: 0.8625 - val_loss: 5.3354 - val_accuracy: 0.4568\n",
            "Epoch 242/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3982 - accuracy: 0.8599 - val_loss: 5.6502 - val_accuracy: 0.4545\n",
            "Epoch 243/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3968 - accuracy: 0.8615 - val_loss: 5.6117 - val_accuracy: 0.4452\n",
            "Epoch 244/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3966 - accuracy: 0.8620 - val_loss: 5.6163 - val_accuracy: 0.4451\n",
            "Epoch 245/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.3765 - accuracy: 0.8662 - val_loss: 5.6670 - val_accuracy: 0.4407\n",
            "Epoch 246/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.3765 - accuracy: 0.8665 - val_loss: 5.4550 - val_accuracy: 0.4482\n",
            "Epoch 247/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3876 - accuracy: 0.8657 - val_loss: 5.4475 - val_accuracy: 0.4500\n",
            "Epoch 248/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4070 - accuracy: 0.8573 - val_loss: 5.6115 - val_accuracy: 0.4520\n",
            "Epoch 249/300\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.3826 - accuracy: 0.8664 - val_loss: 5.7153 - val_accuracy: 0.4504\n",
            "Epoch 250/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3920 - accuracy: 0.8620 - val_loss: 5.6111 - val_accuracy: 0.4443\n",
            "Epoch 251/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3842 - accuracy: 0.8659 - val_loss: 5.7080 - val_accuracy: 0.4504\n",
            "Epoch 252/300\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 0.3870 - accuracy: 0.8640 - val_loss: 5.7219 - val_accuracy: 0.4537\n",
            "Epoch 253/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3839 - accuracy: 0.8651 - val_loss: 5.8436 - val_accuracy: 0.4536\n",
            "Epoch 254/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3991 - accuracy: 0.8607 - val_loss: 5.5146 - val_accuracy: 0.4457\n",
            "Epoch 255/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3919 - accuracy: 0.8638 - val_loss: 5.6898 - val_accuracy: 0.4501\n",
            "Epoch 256/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3764 - accuracy: 0.8675 - val_loss: 5.8216 - val_accuracy: 0.4513\n",
            "Epoch 257/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3950 - accuracy: 0.8648 - val_loss: 5.7186 - val_accuracy: 0.4509\n",
            "Epoch 258/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3725 - accuracy: 0.8699 - val_loss: 5.7283 - val_accuracy: 0.4541\n",
            "Epoch 259/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3617 - accuracy: 0.8716 - val_loss: 6.2561 - val_accuracy: 0.4491\n",
            "Epoch 260/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.4035 - accuracy: 0.8591 - val_loss: 5.8636 - val_accuracy: 0.4492\n",
            "Epoch 261/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3682 - accuracy: 0.8722 - val_loss: 5.7075 - val_accuracy: 0.4457\n",
            "Epoch 262/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3864 - accuracy: 0.8661 - val_loss: 5.7399 - val_accuracy: 0.4534\n",
            "Epoch 263/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3681 - accuracy: 0.8709 - val_loss: 5.8671 - val_accuracy: 0.4429\n",
            "Epoch 264/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3628 - accuracy: 0.8715 - val_loss: 5.6166 - val_accuracy: 0.4474\n",
            "Epoch 265/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3871 - accuracy: 0.8640 - val_loss: 5.9181 - val_accuracy: 0.4542\n",
            "Epoch 266/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3609 - accuracy: 0.8748 - val_loss: 5.6178 - val_accuracy: 0.4541\n",
            "Epoch 267/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3912 - accuracy: 0.8656 - val_loss: 6.3062 - val_accuracy: 0.4383\n",
            "Epoch 268/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3781 - accuracy: 0.8717 - val_loss: 5.7632 - val_accuracy: 0.4509\n",
            "Epoch 269/300\n",
            "1250/1250 [==============================] - 34s 28ms/step - loss: 0.3569 - accuracy: 0.8756 - val_loss: 6.4100 - val_accuracy: 0.4496\n",
            "Epoch 270/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3921 - accuracy: 0.8668 - val_loss: 6.0377 - val_accuracy: 0.4430\n",
            "Epoch 271/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3710 - accuracy: 0.8720 - val_loss: 6.2423 - val_accuracy: 0.4445\n",
            "Epoch 272/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3533 - accuracy: 0.8763 - val_loss: 6.0985 - val_accuracy: 0.4466\n",
            "Epoch 273/300\n",
            "1250/1250 [==============================] - 36s 29ms/step - loss: 0.3905 - accuracy: 0.8683 - val_loss: 5.9942 - val_accuracy: 0.4574\n",
            "Epoch 274/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3824 - accuracy: 0.8689 - val_loss: 6.0954 - val_accuracy: 0.4524\n",
            "Epoch 275/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3767 - accuracy: 0.8695 - val_loss: 6.2045 - val_accuracy: 0.4502\n",
            "Epoch 276/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3497 - accuracy: 0.8769 - val_loss: 5.9266 - val_accuracy: 0.4516\n",
            "Epoch 277/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3657 - accuracy: 0.8733 - val_loss: 6.2758 - val_accuracy: 0.4457\n",
            "Epoch 278/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3761 - accuracy: 0.8718 - val_loss: 6.1457 - val_accuracy: 0.4419\n",
            "Epoch 279/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3547 - accuracy: 0.8747 - val_loss: 6.1789 - val_accuracy: 0.4501\n",
            "Epoch 280/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3824 - accuracy: 0.8684 - val_loss: 6.4751 - val_accuracy: 0.4528\n",
            "Epoch 281/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3912 - accuracy: 0.8656 - val_loss: 6.2772 - val_accuracy: 0.4458\n",
            "Epoch 282/300\n",
            "1250/1250 [==============================] - 36s 29ms/step - loss: 0.3716 - accuracy: 0.8731 - val_loss: 6.2240 - val_accuracy: 0.4491\n",
            "Epoch 283/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3520 - accuracy: 0.8769 - val_loss: 6.1661 - val_accuracy: 0.4564\n",
            "Epoch 284/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3554 - accuracy: 0.8763 - val_loss: 6.0807 - val_accuracy: 0.4531\n",
            "Epoch 285/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3758 - accuracy: 0.8683 - val_loss: 6.4798 - val_accuracy: 0.4472\n",
            "Epoch 286/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3573 - accuracy: 0.8765 - val_loss: 6.2102 - val_accuracy: 0.4513\n",
            "Epoch 287/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3761 - accuracy: 0.8706 - val_loss: 6.3022 - val_accuracy: 0.4494\n",
            "Epoch 288/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3590 - accuracy: 0.8733 - val_loss: 6.0763 - val_accuracy: 0.4524\n",
            "Epoch 289/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3778 - accuracy: 0.8712 - val_loss: 6.2048 - val_accuracy: 0.4579\n",
            "Epoch 290/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3530 - accuracy: 0.8786 - val_loss: 6.1126 - val_accuracy: 0.4526\n",
            "Epoch 291/300\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 0.3746 - accuracy: 0.8723 - val_loss: 6.3693 - val_accuracy: 0.4562\n",
            "Epoch 292/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3581 - accuracy: 0.8773 - val_loss: 6.4040 - val_accuracy: 0.4459\n",
            "Epoch 293/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3763 - accuracy: 0.8723 - val_loss: 6.4737 - val_accuracy: 0.4560\n",
            "Epoch 294/300\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 0.3443 - accuracy: 0.8793 - val_loss: 6.0780 - val_accuracy: 0.4430\n",
            "Epoch 295/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3771 - accuracy: 0.8718 - val_loss: 6.3627 - val_accuracy: 0.4485\n",
            "Epoch 296/300\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3575 - accuracy: 0.8767 - val_loss: 6.5945 - val_accuracy: 0.4436\n",
            "Epoch 297/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3688 - accuracy: 0.8732 - val_loss: 6.2387 - val_accuracy: 0.4509\n",
            "Epoch 298/300\n",
            "1250/1250 [==============================] - 36s 29ms/step - loss: 0.3508 - accuracy: 0.8806 - val_loss: 6.3397 - val_accuracy: 0.4540\n",
            "Epoch 299/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3500 - accuracy: 0.8767 - val_loss: 6.7068 - val_accuracy: 0.4458\n",
            "Epoch 300/300\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3672 - accuracy: 0.8743 - val_loss: 6.7651 - val_accuracy: 0.4433\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 1.7338 - accuracy: 0.7684\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 6.5670 - accuracy: 0.4364\n"
          ]
        }
      ]
    }
  ]
}