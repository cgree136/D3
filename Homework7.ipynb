{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cgree136/D3/blob/main/Homework7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Part a: Simple CNN\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3)\n",
        "        self.fc1 = nn.Linear(128 * 6 * 6, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 128 * 6 * 6)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Move model to GPU\n",
        "simple_net = SimpleCNN()\n",
        "simple_net.to(device)\n",
        "\n",
        "# Use standard normalization for test set\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# Load and preprocess data for test set\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# Training function\n",
        "def train_simple_cnn(model, criterion, optimizer, num_epochs=300):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(testloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(testloader)}')\n",
        "\n",
        "# Train Simple CNN\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(simple_net.parameters(), lr=0.001, momentum=0.9)\n",
        "train_simple_cnn(simple_net, criterion, optimizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUvmiWlErUnD",
        "outputId": "0a56e56e-1f3d-4eda-fb86-776382f2f8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 2.2814568470997414\n",
            "Epoch 2, Loss: 2.1651400039150457\n",
            "Epoch 3, Loss: 2.009562626006497\n",
            "Epoch 4, Loss: 1.9033127559977732\n",
            "Epoch 5, Loss: 1.8293546605262028\n",
            "Epoch 6, Loss: 1.764668012120921\n",
            "Epoch 7, Loss: 1.6994384307010917\n",
            "Epoch 8, Loss: 1.6366083933289644\n",
            "Epoch 9, Loss: 1.582728905282962\n",
            "Epoch 10, Loss: 1.5393202775602888\n",
            "Epoch 11, Loss: 1.5031241747983701\n",
            "Epoch 12, Loss: 1.4715197055962435\n",
            "Epoch 13, Loss: 1.4426143374412683\n",
            "Epoch 14, Loss: 1.415368520530166\n",
            "Epoch 15, Loss: 1.3891604592086404\n",
            "Epoch 16, Loss: 1.3636787605893081\n",
            "Epoch 17, Loss: 1.3391534226715185\n",
            "Epoch 18, Loss: 1.3151610884696814\n",
            "Epoch 19, Loss: 1.2916245452917305\n",
            "Epoch 20, Loss: 1.268617462580371\n",
            "Epoch 21, Loss: 1.245958460364372\n",
            "Epoch 22, Loss: 1.2234134989179624\n",
            "Epoch 23, Loss: 1.200993997276209\n",
            "Epoch 24, Loss: 1.178492415482831\n",
            "Epoch 25, Loss: 1.1558477574852621\n",
            "Epoch 26, Loss: 1.1330595415109281\n",
            "Epoch 27, Loss: 1.1100539341094389\n",
            "Epoch 28, Loss: 1.0867681002161305\n",
            "Epoch 29, Loss: 1.0632662085970497\n",
            "Epoch 30, Loss: 1.0393691319189253\n",
            "Epoch 31, Loss: 1.0155683617303326\n",
            "Epoch 32, Loss: 0.9915966292855086\n",
            "Epoch 33, Loss: 0.9673405295344675\n",
            "Epoch 34, Loss: 0.9430976445507852\n",
            "Epoch 35, Loss: 0.9186532797327467\n",
            "Epoch 36, Loss: 0.8939461774507146\n",
            "Epoch 37, Loss: 0.8692291128407618\n",
            "Epoch 38, Loss: 0.8439255588373561\n",
            "Epoch 39, Loss: 0.8187163429465264\n",
            "Epoch 40, Loss: 0.7932082409881482\n",
            "Epoch 41, Loss: 0.7671378879410446\n",
            "Epoch 42, Loss: 0.7408058229525378\n",
            "Epoch 43, Loss: 0.7141999665908753\n",
            "Epoch 44, Loss: 0.6872789536122303\n",
            "Epoch 45, Loss: 0.6602375475084705\n",
            "Epoch 46, Loss: 0.6329226665625907\n",
            "Epoch 47, Loss: 0.6052282420787841\n",
            "Epoch 48, Loss: 0.5772775125446593\n",
            "Epoch 49, Loss: 0.5489331771423862\n",
            "Epoch 50, Loss: 0.5207015484286721\n",
            "Epoch 51, Loss: 0.49208646106302356\n",
            "Epoch 52, Loss: 0.4638727637138336\n",
            "Epoch 53, Loss: 0.4362211688214047\n",
            "Epoch 54, Loss: 0.4087947033298243\n",
            "Epoch 55, Loss: 0.3823343998260179\n",
            "Epoch 56, Loss: 0.35802238709797524\n",
            "Epoch 57, Loss: 0.33554948909077675\n",
            "Epoch 58, Loss: 0.3159840457426135\n",
            "Epoch 59, Loss: 0.3014413052398688\n",
            "Epoch 60, Loss: 0.2954298069665007\n",
            "Epoch 61, Loss: 0.30379263572632126\n",
            "Epoch 62, Loss: 0.3217612408149015\n",
            "Epoch 63, Loss: 0.3286945527526224\n",
            "Epoch 64, Loss: 0.3089127022251012\n",
            "Epoch 65, Loss: 0.2676588318719985\n",
            "Epoch 66, Loss: 0.24422460385141478\n",
            "Epoch 67, Loss: 0.2299674922351245\n",
            "Epoch 68, Loss: 0.2217898032491564\n",
            "Epoch 69, Loss: 0.21074627380770672\n",
            "Epoch 70, Loss: 0.1953689089507624\n",
            "Epoch 71, Loss: 0.1847321623699016\n",
            "Epoch 72, Loss: 0.16858223448418508\n",
            "Epoch 73, Loss: 0.16167965840999107\n",
            "Epoch 74, Loss: 0.15996583183385005\n",
            "Epoch 75, Loss: 0.1538186900934596\n",
            "Epoch 76, Loss: 0.1370583548311405\n",
            "Epoch 77, Loss: 0.12487658956177106\n",
            "Epoch 78, Loss: 0.11161670775099355\n",
            "Epoch 79, Loss: 0.09946603698145812\n",
            "Epoch 80, Loss: 0.09349024294620487\n",
            "Epoch 81, Loss: 0.08158707155424888\n",
            "Epoch 82, Loss: 0.06427569166537683\n",
            "Epoch 83, Loss: 0.05074491656747213\n",
            "Epoch 84, Loss: 0.04571423484427724\n",
            "Epoch 85, Loss: 0.041021445362715965\n",
            "Epoch 86, Loss: 0.03491413451072754\n",
            "Epoch 87, Loss: 0.030009465562001725\n",
            "Epoch 88, Loss: 0.02431562366142015\n",
            "Epoch 89, Loss: 0.02020042784553567\n",
            "Epoch 90, Loss: 0.017753130229854374\n",
            "Epoch 91, Loss: 0.015942066366588518\n",
            "Epoch 92, Loss: 0.01450014126688764\n",
            "Epoch 93, Loss: 0.013283958836535738\n",
            "Epoch 94, Loss: 0.01227262207591657\n",
            "Epoch 95, Loss: 0.01141180919415086\n",
            "Epoch 96, Loss: 0.010662704252394711\n",
            "Epoch 97, Loss: 0.010015245630838878\n",
            "Epoch 98, Loss: 0.009456495804842681\n",
            "Epoch 99, Loss: 0.008959253430574136\n",
            "Epoch 100, Loss: 0.008515861210158201\n",
            "Epoch 101, Loss: 0.008121170180665839\n",
            "Epoch 102, Loss: 0.0077601766000964505\n",
            "Epoch 103, Loss: 0.0074314922924823825\n",
            "Epoch 104, Loss: 0.007123408717500746\n",
            "Epoch 105, Loss: 0.006845522416491583\n",
            "Epoch 106, Loss: 0.006578861545937456\n",
            "Epoch 107, Loss: 0.006338032201276558\n",
            "Epoch 108, Loss: 0.006107363950722157\n",
            "Epoch 109, Loss: 0.005894514229326586\n",
            "Epoch 110, Loss: 0.005694879939672864\n",
            "Epoch 111, Loss: 0.005505727420139844\n",
            "Epoch 112, Loss: 0.0053299542436388075\n",
            "Epoch 113, Loss: 0.00516232983083138\n",
            "Epoch 114, Loss: 0.005006012279142857\n",
            "Epoch 115, Loss: 0.004857584216986385\n",
            "Epoch 116, Loss: 0.004715236597716702\n",
            "Epoch 117, Loss: 0.004582434615519218\n",
            "Epoch 118, Loss: 0.004455176366724454\n",
            "Epoch 119, Loss: 0.004335763314768529\n",
            "Epoch 120, Loss: 0.004221461746681505\n",
            "Epoch 121, Loss: 0.004112315326550928\n",
            "Epoch 122, Loss: 0.004008265493020926\n",
            "Epoch 123, Loss: 0.00390961468623874\n",
            "Epoch 124, Loss: 0.0038147104140682513\n",
            "Epoch 125, Loss: 0.003724346803627303\n",
            "Epoch 126, Loss: 0.0036374597910681893\n",
            "Epoch 127, Loss: 0.003555999812039778\n",
            "Epoch 128, Loss: 0.0034761969817603587\n",
            "Epoch 129, Loss: 0.003400494418867751\n",
            "Epoch 130, Loss: 0.0033285793309636817\n",
            "Epoch 131, Loss: 0.0032576582502799145\n",
            "Epoch 132, Loss: 0.003191066419618252\n",
            "Epoch 133, Loss: 0.003126237241500858\n",
            "Epoch 134, Loss: 0.0030637267295932575\n",
            "Epoch 135, Loss: 0.0030042013653204034\n",
            "Epoch 136, Loss: 0.0029463336265803117\n",
            "Epoch 137, Loss: 0.002891379484449288\n",
            "Epoch 138, Loss: 0.0028372174964003136\n",
            "Epoch 139, Loss: 0.0027852258826665294\n",
            "Epoch 140, Loss: 0.002735554716037438\n",
            "Epoch 141, Loss: 0.002687186593705919\n",
            "Epoch 142, Loss: 0.0026405032068471702\n",
            "Epoch 143, Loss: 0.002595026072175509\n",
            "Epoch 144, Loss: 0.00255140811874113\n",
            "Epoch 145, Loss: 0.0025084125632338617\n",
            "Epoch 146, Loss: 0.0024676487748110727\n",
            "Epoch 147, Loss: 0.002428073815483849\n",
            "Epoch 148, Loss: 0.002388984963626328\n",
            "Epoch 149, Loss: 0.00235169157628667\n",
            "Epoch 150, Loss: 0.0023148218909453504\n",
            "Epoch 151, Loss: 0.0022795486743495187\n",
            "Epoch 152, Loss: 0.0022453247286063458\n",
            "Epoch 153, Loss: 0.002211818893741858\n",
            "Epoch 154, Loss: 0.002179177166062858\n",
            "Epoch 155, Loss: 0.002147250424400951\n",
            "Epoch 156, Loss: 0.002116721743628068\n",
            "Epoch 157, Loss: 0.0020866545336543445\n",
            "Epoch 158, Loss: 0.0020573168482659623\n",
            "Epoch 159, Loss: 0.002028861843404704\n",
            "Epoch 160, Loss: 0.00200087086399157\n",
            "Epoch 161, Loss: 0.0019735347774883756\n",
            "Epoch 162, Loss: 0.0019473602531283535\n",
            "Epoch 163, Loss: 0.0019211166577782946\n",
            "Epoch 164, Loss: 0.0018960135039957952\n",
            "Epoch 165, Loss: 0.0018716546868872799\n",
            "Epoch 166, Loss: 0.0018474423732707918\n",
            "Epoch 167, Loss: 0.0018239964574025232\n",
            "Epoch 168, Loss: 0.0018011529133380776\n",
            "Epoch 169, Loss: 0.0017786930942875303\n",
            "Epoch 170, Loss: 0.001756814092317608\n",
            "Epoch 171, Loss: 0.001735253417386175\n",
            "Epoch 172, Loss: 0.0017141216340182633\n",
            "Epoch 173, Loss: 0.0016939916234028935\n",
            "Epoch 174, Loss: 0.001673921103797598\n",
            "Epoch 175, Loss: 0.001653950893788029\n",
            "Epoch 176, Loss: 0.0016349746893303896\n",
            "Epoch 177, Loss: 0.0016160720430431698\n",
            "Epoch 178, Loss: 0.0015974031766108386\n",
            "Epoch 179, Loss: 0.0015795158079949913\n",
            "Epoch 180, Loss: 0.0015617661058730712\n",
            "Epoch 181, Loss: 0.0015444961161175477\n",
            "Epoch 182, Loss: 0.0015274028097847358\n",
            "Epoch 183, Loss: 0.0015108045262405441\n",
            "Epoch 184, Loss: 0.0014944721077632598\n",
            "Epoch 185, Loss: 0.0014784419726062074\n",
            "Epoch 186, Loss: 0.001462614288467211\n",
            "Epoch 187, Loss: 0.0014473218115819261\n",
            "Epoch 188, Loss: 0.0014321540128183847\n",
            "Epoch 189, Loss: 0.0014173526372586282\n",
            "Epoch 190, Loss: 0.001402702509439669\n",
            "Epoch 191, Loss: 0.0013885291866163995\n",
            "Epoch 192, Loss: 0.0013745439719986378\n",
            "Epoch 193, Loss: 0.0013607870010198628\n",
            "Epoch 194, Loss: 0.0013472643282036831\n",
            "Epoch 195, Loss: 0.0013340078342938054\n",
            "Epoch 196, Loss: 0.0013207915118727891\n",
            "Epoch 197, Loss: 0.0013080411993362348\n",
            "Epoch 198, Loss: 0.0012955026285204545\n",
            "Epoch 199, Loss: 0.001283105301698907\n",
            "Epoch 200, Loss: 0.0012711358322816824\n",
            "Epoch 201, Loss: 0.0012590959685306645\n",
            "Epoch 202, Loss: 0.0012473249927983964\n",
            "Epoch 203, Loss: 0.0012357573011118153\n",
            "Epoch 204, Loss: 0.001224359458638281\n",
            "Epoch 205, Loss: 0.0012132369814078015\n",
            "Epoch 206, Loss: 0.0012021837493269051\n",
            "Epoch 207, Loss: 0.0011915593912452756\n",
            "Epoch 208, Loss: 0.0011808677800507642\n",
            "Epoch 209, Loss: 0.0011704096021067653\n",
            "Epoch 210, Loss: 0.0011600577655736992\n",
            "Epoch 211, Loss: 0.0011500240458641943\n",
            "Epoch 212, Loss: 0.0011401672706776891\n",
            "Epoch 213, Loss: 0.0011303079105334528\n",
            "Epoch 214, Loss: 0.0011206623482863437\n",
            "Epoch 215, Loss: 0.001111096676795337\n",
            "Epoch 216, Loss: 0.0011018100514867829\n",
            "Epoch 217, Loss: 0.0010926754318698975\n",
            "Epoch 218, Loss: 0.0010835415990759951\n",
            "Epoch 219, Loss: 0.001074680377088582\n",
            "Epoch 220, Loss: 0.001065842097724868\n",
            "Epoch 221, Loss: 0.0010571727548817848\n",
            "Epoch 222, Loss: 0.0010486137869253986\n",
            "Epoch 223, Loss: 0.001040206113285207\n",
            "Epoch 224, Loss: 0.001031972280114735\n",
            "Epoch 225, Loss: 0.001023906259537989\n",
            "Epoch 226, Loss: 0.0010157531590028932\n",
            "Epoch 227, Loss: 0.0010078534961991\n",
            "Epoch 228, Loss: 0.001000019267843191\n",
            "Epoch 229, Loss: 0.0009923570019257505\n",
            "Epoch 230, Loss: 0.0009846255945758805\n",
            "Epoch 231, Loss: 0.0009772132501094364\n",
            "Epoch 232, Loss: 0.0009698981596609269\n",
            "Epoch 233, Loss: 0.0009625341440977345\n",
            "Epoch 234, Loss: 0.0009554381187835406\n",
            "Epoch 235, Loss: 0.0009482761258837785\n",
            "Epoch 236, Loss: 0.0009413140386681868\n",
            "Epoch 237, Loss: 0.0009344895863864766\n",
            "Epoch 238, Loss: 0.0009276623343904266\n",
            "Epoch 239, Loss: 0.0009209002713597925\n",
            "Epoch 240, Loss: 0.0009143416903550428\n",
            "Epoch 241, Loss: 0.0009078108819122248\n",
            "Epoch 242, Loss: 0.0009013964980599696\n",
            "Epoch 243, Loss: 0.0008949210008908104\n",
            "Epoch 244, Loss: 0.0008887484144426105\n",
            "Epoch 245, Loss: 0.0008825000016253062\n",
            "Epoch 246, Loss: 0.0008764081464221725\n",
            "Epoch 247, Loss: 0.0008703571282031317\n",
            "Epoch 248, Loss: 0.0008644245467708553\n",
            "Epoch 249, Loss: 0.000858520003542479\n",
            "Epoch 250, Loss: 0.0008526697373822195\n",
            "Epoch 251, Loss: 0.0008469826918522453\n",
            "Epoch 252, Loss: 0.0008413539576076511\n",
            "Epoch 253, Loss: 0.000835714625305152\n",
            "Epoch 254, Loss: 0.000830169947093848\n",
            "Epoch 255, Loss: 0.0008247219172349366\n",
            "Epoch 256, Loss: 0.000819371276346753\n",
            "Epoch 257, Loss: 0.0008140148871739923\n",
            "Epoch 258, Loss: 0.0008087073325140218\n",
            "Epoch 259, Loss: 0.0008035245927359915\n",
            "Epoch 260, Loss: 0.0007984258677453769\n",
            "Epoch 261, Loss: 0.0007933317908867423\n",
            "Epoch 262, Loss: 0.0007883672062385453\n",
            "Epoch 263, Loss: 0.0007833752031404685\n",
            "Epoch 264, Loss: 0.0007785023531634034\n",
            "Epoch 265, Loss: 0.0007736713809567759\n",
            "Epoch 266, Loss: 0.0007688512772697641\n",
            "Epoch 267, Loss: 0.0007641106536875146\n",
            "Epoch 268, Loss: 0.0007594017126370565\n",
            "Epoch 269, Loss: 0.0007548417193832908\n",
            "Epoch 270, Loss: 0.000750220907002296\n",
            "Epoch 271, Loss: 0.0007457062772587891\n",
            "Epoch 272, Loss: 0.000741284786677952\n",
            "Epoch 273, Loss: 0.0007368175684182258\n",
            "Epoch 274, Loss: 0.0007324232370638997\n",
            "Epoch 275, Loss: 0.0007281370512335888\n",
            "Epoch 276, Loss: 0.0007237985500167524\n",
            "Epoch 277, Loss: 0.000719602779476202\n",
            "Epoch 278, Loss: 0.0007154553824678659\n",
            "Epoch 279, Loss: 0.0007112426712966649\n",
            "Epoch 280, Loss: 0.000707182259728745\n",
            "Epoch 281, Loss: 0.0007031076549688446\n",
            "Epoch 282, Loss: 0.0006991251348211967\n",
            "Epoch 283, Loss: 0.0006951254627852371\n",
            "Epoch 284, Loss: 0.0006912194571639842\n",
            "Epoch 285, Loss: 0.0006873036071720791\n",
            "Epoch 286, Loss: 0.0006834741025216949\n",
            "Epoch 287, Loss: 0.0006796309807080891\n",
            "Epoch 288, Loss: 0.0006759029882487316\n",
            "Epoch 289, Loss: 0.0006721860113111399\n",
            "Epoch 290, Loss: 0.0006684583269909045\n",
            "Epoch 291, Loss: 0.0006648439283343328\n",
            "Epoch 292, Loss: 0.0006612284813837406\n",
            "Epoch 293, Loss: 0.0006576183071620629\n",
            "Epoch 294, Loss: 0.0006540812783971708\n",
            "Epoch 295, Loss: 0.0006505764010925866\n",
            "Epoch 296, Loss: 0.0006471023942163336\n",
            "Epoch 297, Loss: 0.0006436928814052008\n",
            "Epoch 298, Loss: 0.0006402298700765879\n",
            "Epoch 299, Loss: 0.0006369205834889032\n",
            "Epoch 300, Loss: 0.0006335492647050177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Check if GPU is available and set the device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Part b: Extended CNN\n",
        "class ExtendedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ExtendedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3)\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 256 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Move model to GPU\n",
        "extended_net = ExtendedCNN()\n",
        "extended_net.to(device)\n",
        "\n",
        "# Load and preprocess data with data augmentation\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4)\n",
        "\n",
        "# Training function\n",
        "def train_simple_cnn(model, criterion, optimizer, num_epochs=50):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(testloader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(testloader)}')\n",
        "\n",
        "# Train Simple CNN\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(simple_net.parameters(), lr=0.001, momentum=0.9)\n",
        "train_simple_cnn(simple_net, criterion, optimizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHqo6fZCshQZ",
        "outputId": "f4d23645-c63b-42ed-dbd8-93a775fee223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 6.926172449359242e-05\n",
            "Epoch 2, Loss: 6.816903524693103e-05\n",
            "Epoch 3, Loss: 6.73539671321727e-05\n",
            "Epoch 4, Loss: 6.653546611873793e-05\n",
            "Epoch 5, Loss: 6.573598385143526e-05\n",
            "Epoch 6, Loss: 6.495107459059994e-05\n",
            "Epoch 7, Loss: 6.418978604959732e-05\n",
            "Epoch 8, Loss: 6.344221555465452e-05\n",
            "Epoch 9, Loss: 6.270058760989201e-05\n",
            "Epoch 10, Loss: 6.199241425117207e-05\n",
            "Epoch 11, Loss: 6.129042577760088e-05\n",
            "Epoch 12, Loss: 6.059620544250066e-05\n",
            "Epoch 13, Loss: 5.992504767963993e-05\n",
            "Epoch 14, Loss: 5.9264864088841305e-05\n",
            "Epoch 15, Loss: 5.862401114915912e-05\n",
            "Epoch 16, Loss: 5.798958128077203e-05\n",
            "Epoch 17, Loss: 5.736636471599041e-05\n",
            "Epoch 18, Loss: 5.675572140639957e-05\n",
            "Epoch 19, Loss: 5.6165053046562716e-05\n",
            "Epoch 20, Loss: 5.557283418353478e-05\n",
            "Epoch 21, Loss: 5.500386785512461e-05\n",
            "Epoch 22, Loss: 5.4438785414585314e-05\n",
            "Epoch 23, Loss: 5.3892652229298224e-05\n",
            "Epoch 24, Loss: 5.3343373946161424e-05\n",
            "Epoch 25, Loss: 5.28149852567779e-05\n",
            "Epoch 26, Loss: 5.2290292288803643e-05\n",
            "Epoch 27, Loss: 5.1773775945856925e-05\n",
            "Epoch 28, Loss: 5.12722988561805e-05\n",
            "Epoch 29, Loss: 5.078256039233366e-05\n",
            "Epoch 30, Loss: 5.029271505928392e-05\n",
            "Epoch 31, Loss: 4.981611083174613e-05\n",
            "Epoch 32, Loss: 4.934874223145798e-05\n",
            "Epoch 33, Loss: 4.8886260751211095e-05\n",
            "Epoch 34, Loss: 4.8432764712836016e-05\n",
            "Epoch 35, Loss: 4.7986574969655035e-05\n",
            "Epoch 36, Loss: 4.755507999641893e-05\n",
            "Epoch 37, Loss: 4.712054608356695e-05\n",
            "Epoch 38, Loss: 4.6696380682794114e-05\n",
            "Epoch 39, Loss: 4.628108206370527e-05\n",
            "Epoch 40, Loss: 4.587105128514324e-05\n",
            "Epoch 41, Loss: 4.546900646796459e-05\n",
            "Epoch 42, Loss: 4.5072801287277996e-05\n",
            "Epoch 43, Loss: 4.4684282585671295e-05\n",
            "Epoch 44, Loss: 4.429913745112728e-05\n",
            "Epoch 45, Loss: 4.392077389917333e-05\n",
            "Epoch 46, Loss: 4.355102728771883e-05\n",
            "Epoch 47, Loss: 4.318654790378815e-05\n",
            "Epoch 48, Loss: 4.2824798097804263e-05\n",
            "Epoch 49, Loss: 4.246834030431756e-05\n",
            "Epoch 50, Loss: 4.212100004749146e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL8J7aBRg70c",
        "outputId": "72560a20-eeb1-492b-9241-8bc269951f5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 4.177727050872164e-05\n",
            "Epoch 2, Loss: 4.143953689088704e-05\n",
            "Epoch 3, Loss: 4.1106737068896845e-05\n",
            "Epoch 4, Loss: 4.0776833178536266e-05\n",
            "Epoch 5, Loss: 4.0452924407343e-05\n",
            "Epoch 6, Loss: 4.013489120804223e-05\n",
            "Epoch 7, Loss: 3.9820338822458725e-05\n",
            "Epoch 8, Loss: 3.950948051451491e-05\n",
            "Epoch 9, Loss: 3.920696593759061e-05\n",
            "Epoch 10, Loss: 3.890157864591259e-05\n",
            "Epoch 11, Loss: 3.860788343420012e-05\n",
            "Epoch 12, Loss: 3.8313890550737995e-05\n",
            "Epoch 13, Loss: 3.802492683988561e-05\n",
            "Epoch 14, Loss: 3.773820372252743e-05\n",
            "Epoch 15, Loss: 3.7459884009102494e-05\n",
            "Epoch 16, Loss: 3.718193348288281e-05\n",
            "Epoch 17, Loss: 3.6909858405515196e-05\n",
            "Epoch 18, Loss: 3.6640882908143624e-05\n",
            "Epoch 19, Loss: 3.637437453672447e-05\n",
            "Epoch 20, Loss: 3.6112060797957214e-05\n",
            "Epoch 21, Loss: 3.585220276075489e-05\n",
            "Epoch 22, Loss: 3.559659988296318e-05\n",
            "Epoch 23, Loss: 3.5346384427732855e-05\n",
            "Epoch 24, Loss: 3.509746762157064e-05\n",
            "Epoch 25, Loss: 3.485019599489405e-05\n",
            "Epoch 26, Loss: 3.460905042732847e-05\n",
            "Epoch 27, Loss: 3.4371623750515567e-05\n",
            "Epoch 28, Loss: 3.413332696312281e-05\n",
            "Epoch 29, Loss: 3.390116846025748e-05\n",
            "Epoch 30, Loss: 3.367105974563955e-05\n",
            "Epoch 31, Loss: 3.344017641501509e-05\n",
            "Epoch 32, Loss: 3.321792248359188e-05\n",
            "Epoch 33, Loss: 3.299651463086874e-05\n",
            "Epoch 34, Loss: 3.277884945170726e-05\n",
            "Epoch 35, Loss: 3.256012368065839e-05\n",
            "Epoch 36, Loss: 3.2345962601583976e-05\n",
            "Epoch 37, Loss: 3.2136163713243346e-05\n",
            "Epoch 38, Loss: 3.1929285579962393e-05\n",
            "Epoch 39, Loss: 3.17217633022544e-05\n",
            "Epoch 40, Loss: 3.151753059081486e-05\n",
            "Epoch 41, Loss: 3.131676693574193e-05\n",
            "Epoch 42, Loss: 3.1116968041543916e-05\n",
            "Epoch 43, Loss: 3.0919660716937616e-05\n",
            "Epoch 44, Loss: 3.072759760848314e-05\n",
            "Epoch 45, Loss: 3.053446187721178e-05\n",
            "Epoch 46, Loss: 3.0344782684417026e-05\n",
            "Epoch 47, Loss: 3.0156795753830323e-05\n",
            "Epoch 48, Loss: 2.9972289641195716e-05\n",
            "Epoch 49, Loss: 2.978889166137577e-05\n",
            "Epoch 50, Loss: 2.9605016642101134e-05\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the ResNet block\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define the ResNet-10 model\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet10, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            ResNetBlock(64, 64),\n",
        "            ResNetBlock(64, 64),\n",
        "            ResNetBlock(64, 64),\n",
        "            ResNetBlock(64, 64),\n",
        "            ResNetBlock(64, 64),\n",
        "            ResNetBlock(64, 64),\n",
        "            ResNetBlock(64, 64),\n",
        "            ResNetBlock(64, 64),\n",
        "            ResNetBlock(64, 64),\n",
        "            ResNetBlock(64, 64),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.blocks(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Data loading and preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = ResNet10()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Training function\n",
        "def train_simple_cnn(model, criterion, optimizer, num_epochs=50):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(testloader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(testloader)}')\n",
        "\n",
        "# Train Simple CNN\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(simple_net.parameters(), lr=0.001, momentum=0.9)\n",
        "train_simple_cnn(simple_net, criterion, optimizer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# Replace the existing device check and set\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Load CIFAR-10 dataset with standard normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Define Residual Block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.stride != 1 or identity.shape[1] != out.shape[1]:\n",
        "            identity = self.conv1(identity)\n",
        "            identity = self.bn1(identity)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Define ResNet-10\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(ResNet10, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels, stride=1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define transform for the test set\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load and preprocess data for test set\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# Training function\n",
        "def train_resnet(model, criterion, optimizer, train_loader, num_epochs=10, device='cpu'):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    start_time = time.time()\n",
        "    all_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        num = epoch + 1  # Initialize num for the current epoch\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Store training loss for each epoch and print after the inner loop\n",
        "            all_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "        # Print training loss after each epoch\n",
        "            print(f'Epoch {num}, Loss: {all_losses[-1]}')\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    print(f'Training Time: {training_time} seconds')\n",
        "    return all_losses\n",
        "\n",
        "\n",
        "# Instantiate ResNet-10 and set up optimizer and criterion\n",
        "resnet_model = ResNet10(ResidualBlock, [1, 1, 1, 1])\n",
        "criterion_resnet = nn.CrossEntropyLoss()\n",
        "optimizer_resnet = optim.SGD(resnet_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train ResNet-10 on CPU\n",
        "train_resnet(resnet_model, criterion_resnet, optimizer_resnet, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H1rR--_hLR6f",
        "outputId": "ce9f7fe9-7e78-487a-d15f-23a95a812404"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 0.003073425244187455\n",
            "Epoch 1, Loss: 0.0059756247893623686\n",
            "Epoch 1, Loss: 0.009014006161019016\n",
            "Epoch 1, Loss: 0.011978732350537234\n",
            "Epoch 1, Loss: 0.015093729624053096\n",
            "Epoch 1, Loss: 0.018026305281597634\n",
            "Epoch 1, Loss: 0.02099433213548587\n",
            "Epoch 1, Loss: 0.023879539936094943\n",
            "Epoch 1, Loss: 0.026811930224718644\n",
            "Epoch 1, Loss: 0.02971915210909246\n",
            "Epoch 1, Loss: 0.0326024720735867\n",
            "Epoch 1, Loss: 0.035414600006454744\n",
            "Epoch 1, Loss: 0.038270483541366696\n",
            "Epoch 1, Loss: 0.04104091749166894\n",
            "Epoch 1, Loss: 0.0437794624996917\n",
            "Epoch 1, Loss: 0.046555610873815045\n",
            "Epoch 1, Loss: 0.04928005198993341\n",
            "Epoch 1, Loss: 0.052042759897763774\n",
            "Epoch 1, Loss: 0.054706024086993675\n",
            "Epoch 1, Loss: 0.057436589084927686\n",
            "Epoch 1, Loss: 0.06015394136423955\n",
            "Epoch 1, Loss: 0.06286822712939719\n",
            "Epoch 1, Loss: 0.06558737395059727\n",
            "Epoch 1, Loss: 0.06825701019648091\n",
            "Epoch 1, Loss: 0.07072139075954857\n",
            "Epoch 1, Loss: 0.07361381331368176\n",
            "Epoch 1, Loss: 0.07620612876799406\n",
            "Epoch 1, Loss: 0.07885541132344004\n",
            "Epoch 1, Loss: 0.08128644224932736\n",
            "Epoch 1, Loss: 0.08391237106469586\n",
            "Epoch 1, Loss: 0.08641514174468681\n",
            "Epoch 1, Loss: 0.08908757041482364\n",
            "Epoch 1, Loss: 0.09160609836773494\n",
            "Epoch 1, Loss: 0.0940445185927174\n",
            "Epoch 1, Loss: 0.09644105900888858\n",
            "Epoch 1, Loss: 0.0990408410501602\n",
            "Epoch 1, Loss: 0.10182785759191684\n",
            "Epoch 1, Loss: 0.10415337366216323\n",
            "Epoch 1, Loss: 0.1065862308377805\n",
            "Epoch 1, Loss: 0.10911250038220145\n",
            "Epoch 1, Loss: 0.11149729792114414\n",
            "Epoch 1, Loss: 0.11404101851651126\n",
            "Epoch 1, Loss: 0.11660458044627743\n",
            "Epoch 1, Loss: 0.11915429976895033\n",
            "Epoch 1, Loss: 0.12159422642129766\n",
            "Epoch 1, Loss: 0.12388302587792087\n",
            "Epoch 1, Loss: 0.12624731499825598\n",
            "Epoch 1, Loss: 0.12866482893219383\n",
            "Epoch 1, Loss: 0.1311144115369948\n",
            "Epoch 1, Loss: 0.13354561761821931\n",
            "Epoch 1, Loss: 0.13590525986288515\n",
            "Epoch 1, Loss: 0.1380711851827324\n",
            "Epoch 1, Loss: 0.1404816075359159\n",
            "Epoch 1, Loss: 0.14281462418758656\n",
            "Epoch 1, Loss: 0.1452395728482005\n",
            "Epoch 1, Loss: 0.14765674592283987\n",
            "Epoch 1, Loss: 0.15000373506180162\n",
            "Epoch 1, Loss: 0.15247594304096973\n",
            "Epoch 1, Loss: 0.1547473505939669\n",
            "Epoch 1, Loss: 0.15705893320195816\n",
            "Epoch 1, Loss: 0.1592895458726322\n",
            "Epoch 1, Loss: 0.1617023167402848\n",
            "Epoch 1, Loss: 0.16403113073095335\n",
            "Epoch 1, Loss: 0.16656758870615068\n",
            "Epoch 1, Loss: 0.16878259181976318\n",
            "Epoch 1, Loss: 0.17098331649590026\n",
            "Epoch 1, Loss: 0.17310723349871232\n",
            "Epoch 1, Loss: 0.17564712735393162\n",
            "Epoch 1, Loss: 0.17805280267734966\n",
            "Epoch 1, Loss: 0.18042386965373594\n",
            "Epoch 1, Loss: 0.1826826696810515\n",
            "Epoch 1, Loss: 0.184940502466753\n",
            "Epoch 1, Loss: 0.1871866995416334\n",
            "Epoch 1, Loss: 0.18929528260170042\n",
            "Epoch 1, Loss: 0.1915876177875587\n",
            "Epoch 1, Loss: 0.19393455860255016\n",
            "Epoch 1, Loss: 0.19611138562717095\n",
            "Epoch 1, Loss: 0.19834438202631138\n",
            "Epoch 1, Loss: 0.20059044266600742\n",
            "Epoch 1, Loss: 0.20297103419023402\n",
            "Epoch 1, Loss: 0.2053589852874541\n",
            "Epoch 1, Loss: 0.20752353131618645\n",
            "Epoch 1, Loss: 0.20968979048302105\n",
            "Epoch 1, Loss: 0.21205928975054064\n",
            "Epoch 1, Loss: 0.21422542391530694\n",
            "Epoch 1, Loss: 0.21651851429658778\n",
            "Epoch 1, Loss: 0.21871068867880974\n",
            "Epoch 1, Loss: 0.22070967815721126\n",
            "Epoch 1, Loss: 0.22281656408553843\n",
            "Epoch 1, Loss: 0.22486407555582577\n",
            "Epoch 1, Loss: 0.22697667042007835\n",
            "Epoch 1, Loss: 0.22912590171370056\n",
            "Epoch 1, Loss: 0.23124167757570896\n",
            "Epoch 1, Loss: 0.23329098907578022\n",
            "Epoch 1, Loss: 0.23542484694429675\n",
            "Epoch 1, Loss: 0.23760645468826488\n",
            "Epoch 1, Loss: 0.23985667408579756\n",
            "Epoch 1, Loss: 0.24194311074283728\n",
            "Epoch 1, Loss: 0.24396358472307014\n",
            "Epoch 1, Loss: 0.2460683503419237\n",
            "Epoch 1, Loss: 0.24810684763866922\n",
            "Epoch 1, Loss: 0.25016047780775963\n",
            "Epoch 1, Loss: 0.25251659728072184\n",
            "Epoch 1, Loss: 0.25461843388769634\n",
            "Epoch 1, Loss: 0.2569220213938857\n",
            "Epoch 1, Loss: 0.2592886571993913\n",
            "Epoch 1, Loss: 0.26127481872163466\n",
            "Epoch 1, Loss: 0.2633651726691009\n",
            "Epoch 1, Loss: 0.265402773152227\n",
            "Epoch 1, Loss: 0.26742406528624124\n",
            "Epoch 1, Loss: 0.2695059643682007\n",
            "Epoch 1, Loss: 0.2718151464791554\n",
            "Epoch 1, Loss: 0.27385025423810916\n",
            "Epoch 1, Loss: 0.276015757599755\n",
            "Epoch 1, Loss: 0.27815866317895366\n",
            "Epoch 1, Loss: 0.2800895509207645\n",
            "Epoch 1, Loss: 0.2822146831875872\n",
            "Epoch 1, Loss: 0.2845259487171612\n",
            "Epoch 1, Loss: 0.2866809730944426\n",
            "Epoch 1, Loss: 0.2886758161627728\n",
            "Epoch 1, Loss: 0.2907581443676863\n",
            "Epoch 1, Loss: 0.2929533451719357\n",
            "Epoch 1, Loss: 0.2950725884693663\n",
            "Epoch 1, Loss: 0.2972076330953242\n",
            "Epoch 1, Loss: 0.2991228565535582\n",
            "Epoch 1, Loss: 0.3013097773427549\n",
            "Epoch 1, Loss: 0.3034780708420307\n",
            "Epoch 1, Loss: 0.30547831811563436\n",
            "Epoch 1, Loss: 0.30746445982047665\n",
            "Epoch 1, Loss: 0.3096434515150612\n",
            "Epoch 1, Loss: 0.3117580230888503\n",
            "Epoch 1, Loss: 0.31375805191371753\n",
            "Epoch 1, Loss: 0.315794388351538\n",
            "Epoch 1, Loss: 0.31791061178192764\n",
            "Epoch 1, Loss: 0.31988764312261203\n",
            "Epoch 1, Loss: 0.321885011385164\n",
            "Epoch 1, Loss: 0.3239659200543943\n",
            "Epoch 1, Loss: 0.3259428747169807\n",
            "Epoch 1, Loss: 0.3279102953803509\n",
            "Epoch 1, Loss: 0.33015890926351327\n",
            "Epoch 1, Loss: 0.33217182046617083\n",
            "Epoch 1, Loss: 0.33429516383144253\n",
            "Epoch 1, Loss: 0.3363383109002467\n",
            "Epoch 1, Loss: 0.33837233449492005\n",
            "Epoch 1, Loss: 0.34056834293448407\n",
            "Epoch 1, Loss: 0.3425266410383727\n",
            "Epoch 1, Loss: 0.34466093046890806\n",
            "Epoch 1, Loss: 0.3467859831612433\n",
            "Epoch 1, Loss: 0.3487426470917509\n",
            "Epoch 1, Loss: 0.35070718249396593\n",
            "Epoch 1, Loss: 0.35280225420242073\n",
            "Epoch 1, Loss: 0.35484632887803685\n",
            "Epoch 1, Loss: 0.35678653323741827\n",
            "Epoch 1, Loss: 0.35899645013882375\n",
            "Epoch 1, Loss: 0.3610587851775577\n",
            "Epoch 1, Loss: 0.3630988975924909\n",
            "Epoch 1, Loss: 0.3651167483585875\n",
            "Epoch 1, Loss: 0.36717854314447973\n",
            "Epoch 1, Loss: 0.36948556226232776\n",
            "Epoch 1, Loss: 0.3713556340588328\n",
            "Epoch 1, Loss: 0.3733517013852249\n",
            "Epoch 1, Loss: 0.3752948945135717\n",
            "Epoch 1, Loss: 0.3773393182803298\n",
            "Epoch 1, Loss: 0.37924579891097515\n",
            "Epoch 1, Loss: 0.38136534861591465\n",
            "Epoch 1, Loss: 0.3833551274236206\n",
            "Epoch 1, Loss: 0.38543992274252653\n",
            "Epoch 1, Loss: 0.38751411681894754\n",
            "Epoch 1, Loss: 0.3896064130241609\n",
            "Epoch 1, Loss: 0.39169258153651987\n",
            "Epoch 1, Loss: 0.3935196825000636\n",
            "Epoch 1, Loss: 0.39559047042256423\n",
            "Epoch 1, Loss: 0.3975920539987666\n",
            "Epoch 1, Loss: 0.3994203072679622\n",
            "Epoch 1, Loss: 0.40153130408748033\n",
            "Epoch 1, Loss: 0.40374228487844055\n",
            "Epoch 1, Loss: 0.4058008197018558\n",
            "Epoch 1, Loss: 0.4078808134169225\n",
            "Epoch 1, Loss: 0.409987901025416\n",
            "Epoch 1, Loss: 0.4120316548115762\n",
            "Epoch 1, Loss: 0.4141118098097994\n",
            "Epoch 1, Loss: 0.4160762497836062\n",
            "Epoch 1, Loss: 0.41813130360430156\n",
            "Epoch 1, Loss: 0.420041401367968\n",
            "Epoch 1, Loss: 0.4218051925949428\n",
            "Epoch 1, Loss: 0.42370882744679367\n",
            "Epoch 1, Loss: 0.4257271372143875\n",
            "Epoch 1, Loss: 0.42813446530905525\n",
            "Epoch 1, Loss: 0.4300337706685371\n",
            "Epoch 1, Loss: 0.43191621873689734\n",
            "Epoch 1, Loss: 0.4339974717715817\n",
            "Epoch 1, Loss: 0.43617451846447136\n",
            "Epoch 1, Loss: 0.4384209550250217\n",
            "Epoch 1, Loss: 0.44041531470120715\n",
            "Epoch 1, Loss: 0.4422175561070747\n",
            "Epoch 1, Loss: 0.4442413042268485\n",
            "Epoch 1, Loss: 0.4461747381998145\n",
            "Epoch 1, Loss: 0.4481752741977077\n",
            "Epoch 1, Loss: 0.44999880391313596\n",
            "Epoch 1, Loss: 0.45196838726472977\n",
            "Epoch 1, Loss: 0.45397214069390845\n",
            "Epoch 1, Loss: 0.45612121421052976\n",
            "Epoch 1, Loss: 0.45823538349107706\n",
            "Epoch 1, Loss: 0.46008604475299414\n",
            "Epoch 1, Loss: 0.4620280857281307\n",
            "Epoch 1, Loss: 0.464060638261878\n",
            "Epoch 1, Loss: 0.46630099591086893\n",
            "Epoch 1, Loss: 0.46829518424275585\n",
            "Epoch 1, Loss: 0.47032389372510985\n",
            "Epoch 1, Loss: 0.47241213529006293\n",
            "Epoch 1, Loss: 0.4744007364868203\n",
            "Epoch 1, Loss: 0.47643213351364333\n",
            "Epoch 1, Loss: 0.47859126131247987\n",
            "Epoch 1, Loss: 0.48040121199224917\n",
            "Epoch 1, Loss: 0.4822988644280397\n",
            "Epoch 1, Loss: 0.48439415839626965\n",
            "Epoch 1, Loss: 0.4866166448654116\n",
            "Epoch 1, Loss: 0.48833109289788834\n",
            "Epoch 1, Loss: 0.4903138874436888\n",
            "Epoch 1, Loss: 0.4922411405217007\n",
            "Epoch 1, Loss: 0.4942765110898811\n",
            "Epoch 1, Loss: 0.49622560248655434\n",
            "Epoch 1, Loss: 0.4979908876406872\n",
            "Epoch 1, Loss: 0.5001790264378423\n",
            "Epoch 1, Loss: 0.5019863093905437\n",
            "Epoch 1, Loss: 0.5039264049066607\n",
            "Epoch 1, Loss: 0.5058567542249285\n",
            "Epoch 1, Loss: 0.5078383371653155\n",
            "Epoch 1, Loss: 0.5097231133209775\n",
            "Epoch 1, Loss: 0.5115992748523917\n",
            "Epoch 1, Loss: 0.5133179991751375\n",
            "Epoch 1, Loss: 0.5153359786018996\n",
            "Epoch 1, Loss: 0.5172639298621956\n",
            "Epoch 1, Loss: 0.5191169387239325\n",
            "Epoch 1, Loss: 0.5209000601488001\n",
            "Epoch 1, Loss: 0.5227834841479426\n",
            "Epoch 1, Loss: 0.5249328300776079\n",
            "Epoch 1, Loss: 0.5266631503239312\n",
            "Epoch 1, Loss: 0.5283895339197515\n",
            "Epoch 1, Loss: 0.5303225131595836\n",
            "Epoch 1, Loss: 0.5321884478449517\n",
            "Epoch 1, Loss: 0.5339401388717124\n",
            "Epoch 1, Loss: 0.5359266912540817\n",
            "Epoch 1, Loss: 0.5377620751290675\n",
            "Epoch 1, Loss: 0.539731672352842\n",
            "Epoch 1, Loss: 0.5416134955633022\n",
            "Epoch 1, Loss: 0.5434138271814722\n",
            "Epoch 1, Loss: 0.5453980379092419\n",
            "Epoch 1, Loss: 0.547275657086726\n",
            "Epoch 1, Loss: 0.5491918270545237\n",
            "Epoch 1, Loss: 0.5509518743171107\n",
            "Epoch 1, Loss: 0.5529070909675735\n",
            "Epoch 1, Loss: 0.5548536754630106\n",
            "Epoch 1, Loss: 0.5567188796484867\n",
            "Epoch 1, Loss: 0.558678507347546\n",
            "Epoch 1, Loss: 0.5603052223734843\n",
            "Epoch 1, Loss: 0.5621911339137865\n",
            "Epoch 1, Loss: 0.5639019129831163\n",
            "Epoch 1, Loss: 0.5658148254275017\n",
            "Epoch 1, Loss: 0.5679014184895683\n",
            "Epoch 1, Loss: 0.5695725389758645\n",
            "Epoch 1, Loss: 0.571343292513162\n",
            "Epoch 1, Loss: 0.5731319103704389\n",
            "Epoch 1, Loss: 0.5748596921601259\n",
            "Epoch 1, Loss: 0.5769121309985286\n",
            "Epoch 1, Loss: 0.5790141278215687\n",
            "Epoch 1, Loss: 0.5808812390507945\n",
            "Epoch 1, Loss: 0.5826890107310946\n",
            "Epoch 1, Loss: 0.584845551596883\n",
            "Epoch 1, Loss: 0.5866309985175462\n",
            "Epoch 1, Loss: 0.5884672418579726\n",
            "Epoch 1, Loss: 0.5904937984083619\n",
            "Epoch 1, Loss: 0.5924353098015651\n",
            "Epoch 1, Loss: 0.5941814092723915\n",
            "Epoch 1, Loss: 0.5962877709542393\n",
            "Epoch 1, Loss: 0.5981453655625854\n",
            "Epoch 1, Loss: 0.6001033177766044\n",
            "Epoch 1, Loss: 0.6018942637211832\n",
            "Epoch 1, Loss: 0.6039322532351364\n",
            "Epoch 1, Loss: 0.605718254280822\n",
            "Epoch 1, Loss: 0.6076527803450289\n",
            "Epoch 1, Loss: 0.6093642364072678\n",
            "Epoch 1, Loss: 0.6110341404100208\n",
            "Epoch 1, Loss: 0.6127195024429379\n",
            "Epoch 1, Loss: 0.6144256297584689\n",
            "Epoch 1, Loss: 0.6163822677739136\n",
            "Epoch 1, Loss: 0.6182120605502897\n",
            "Epoch 1, Loss: 0.6198932542215527\n",
            "Epoch 1, Loss: 0.6220625158770919\n",
            "Epoch 1, Loss: 0.62387203179357\n",
            "Epoch 1, Loss: 0.6256655604028336\n",
            "Epoch 1, Loss: 0.6275957059067534\n",
            "Epoch 1, Loss: 0.6296978938914931\n",
            "Epoch 1, Loss: 0.6317145789370817\n",
            "Epoch 1, Loss: 0.6337277224606566\n",
            "Epoch 1, Loss: 0.6355095513336494\n",
            "Epoch 1, Loss: 0.6373077431298277\n",
            "Epoch 1, Loss: 0.6389248392466084\n",
            "Epoch 1, Loss: 0.640787818852593\n",
            "Epoch 1, Loss: 0.6424782492620561\n",
            "Epoch 1, Loss: 0.6442613673332097\n",
            "Epoch 1, Loss: 0.6460358695605831\n",
            "Epoch 1, Loss: 0.6477994760284034\n",
            "Epoch 1, Loss: 0.6496775499390214\n",
            "Epoch 1, Loss: 0.6517747279323275\n",
            "Epoch 1, Loss: 0.6533853913207188\n",
            "Epoch 1, Loss: 0.6554024551835511\n",
            "Epoch 1, Loss: 0.6575889459351445\n",
            "Epoch 1, Loss: 0.6594129759637292\n",
            "Epoch 1, Loss: 0.6614191612929029\n",
            "Epoch 1, Loss: 0.6631067762594394\n",
            "Epoch 1, Loss: 0.6649854465213882\n",
            "Epoch 1, Loss: 0.6670643859507178\n",
            "Epoch 1, Loss: 0.6685680891851635\n",
            "Epoch 1, Loss: 0.6703464814159267\n",
            "Epoch 1, Loss: 0.672170019820523\n",
            "Epoch 1, Loss: 0.6739534461284842\n",
            "Epoch 1, Loss: 0.6756774882221466\n",
            "Epoch 1, Loss: 0.6774728978076554\n",
            "Epoch 1, Loss: 0.6791032633513135\n",
            "Epoch 1, Loss: 0.6809496582316621\n",
            "Epoch 1, Loss: 0.6826085865954914\n",
            "Epoch 1, Loss: 0.6845868362490174\n",
            "Epoch 1, Loss: 0.6860389601239159\n",
            "Epoch 1, Loss: 0.687840735515975\n",
            "Epoch 1, Loss: 0.6897900484102156\n",
            "Epoch 1, Loss: 0.6914655047914257\n",
            "Epoch 1, Loss: 0.6930330180755967\n",
            "Epoch 1, Loss: 0.6949068393243854\n",
            "Epoch 1, Loss: 0.6967569164302952\n",
            "Epoch 1, Loss: 0.6984297171273195\n",
            "Epoch 1, Loss: 0.7000631347031849\n",
            "Epoch 1, Loss: 0.7017199316293078\n",
            "Epoch 1, Loss: 0.7033385663386196\n",
            "Epoch 1, Loss: 0.7050398806171954\n",
            "Epoch 1, Loss: 0.7067501000736071\n",
            "Epoch 1, Loss: 0.7085366477746793\n",
            "Epoch 1, Loss: 0.71076238353539\n",
            "Epoch 1, Loss: 0.7125450179095159\n",
            "Epoch 1, Loss: 0.7144149182092808\n",
            "Epoch 1, Loss: 0.7160752077236809\n",
            "Epoch 1, Loss: 0.717929482460022\n",
            "Epoch 1, Loss: 0.7196577583127619\n",
            "Epoch 1, Loss: 0.7216275444116129\n",
            "Epoch 1, Loss: 0.7234481234684624\n",
            "Epoch 1, Loss: 0.7252031554041616\n",
            "Epoch 1, Loss: 0.7270066642090488\n",
            "Epoch 1, Loss: 0.7289656614098707\n",
            "Epoch 1, Loss: 0.7305332384146083\n",
            "Epoch 1, Loss: 0.7321422193056483\n",
            "Epoch 1, Loss: 0.7341205842049835\n",
            "Epoch 1, Loss: 0.7360302071132319\n",
            "Epoch 1, Loss: 0.7377851125224472\n",
            "Epoch 1, Loss: 0.7395686531615684\n",
            "Epoch 1, Loss: 0.7413295156815473\n",
            "Epoch 1, Loss: 0.7430963778434811\n",
            "Epoch 1, Loss: 0.7450944034339827\n",
            "Epoch 1, Loss: 0.7467659350551302\n",
            "Epoch 1, Loss: 0.7484283014331632\n",
            "Epoch 1, Loss: 0.7503079148509618\n",
            "Epoch 1, Loss: 0.7521740585336905\n",
            "Epoch 1, Loss: 0.7539970234531881\n",
            "Epoch 1, Loss: 0.7557789665048994\n",
            "Epoch 1, Loss: 0.7575526344196876\n",
            "Epoch 1, Loss: 0.7591216536738988\n",
            "Epoch 1, Loss: 0.7609736039815351\n",
            "Epoch 1, Loss: 0.7627198933945287\n",
            "Epoch 1, Loss: 0.7645398495752184\n",
            "Epoch 1, Loss: 0.76655078117195\n",
            "Epoch 1, Loss: 0.7682374273724568\n",
            "Epoch 1, Loss: 0.7699362792627281\n",
            "Epoch 1, Loss: 0.7717901555168659\n",
            "Epoch 1, Loss: 0.7735554347257785\n",
            "Epoch 1, Loss: 0.7755256177824171\n",
            "Epoch 1, Loss: 0.7773160364316858\n",
            "Epoch 1, Loss: 0.7792300978280089\n",
            "Epoch 1, Loss: 0.7810348089393753\n",
            "Epoch 1, Loss: 0.7827957624669575\n",
            "Epoch 1, Loss: 0.7843949593546445\n",
            "Epoch 1, Loss: 0.7861429370577683\n",
            "Epoch 1, Loss: 0.7879156259929433\n",
            "Epoch 1, Loss: 0.7895311068390947\n",
            "Epoch 1, Loss: 0.7914274023926776\n",
            "Epoch 1, Loss: 0.7931500837930938\n",
            "Epoch 1, Loss: 0.7949351453415269\n",
            "Epoch 1, Loss: 0.7965437111342349\n",
            "Epoch 1, Loss: 0.7983308655526632\n",
            "Epoch 1, Loss: 0.8000574290295086\n",
            "Epoch 1, Loss: 0.8020044033179807\n",
            "Epoch 1, Loss: 0.8038364249422117\n",
            "Epoch 1, Loss: 0.8056966272156562\n",
            "Epoch 1, Loss: 0.8073818055565095\n",
            "Epoch 1, Loss: 0.8092022963497035\n",
            "Epoch 1, Loss: 0.8111474558215617\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-b08e6c8c4c9d>\u001b[0m in \u001b[0;36m<cell line: 142>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;31m# Train ResNet-10 on CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m \u001b[0mtrain_resnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_resnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_resnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-b08e6c8c4c9d>\u001b[0m in \u001b[0;36mtrain_resnet\u001b[0;34m(model, criterion, optimizer, train_loader, num_epochs, device)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# Replace the existing device check and set\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Load CIFAR-10 dataset with standard normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Define Residual Block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.stride != 1 or identity.shape[1] != out.shape[1]:\n",
        "            identity = self.conv1(identity)\n",
        "            identity = self.bn1(identity)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Define ResNet-10\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(ResNet10, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels, stride=1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train_resnet_weight_decay(model, criterion, optimizer, train_loader, num_epochs=1, device='cpu'):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    start_time = time.time()\n",
        "    all_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            all_losses.append(running_loss / len(train_loader))\n",
        "            print(f'Epoch , Loss: {all_losses[-1]}')\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    print(f'Training Time: {training_time} seconds')\n",
        "    return all_losses\n",
        "\n",
        "# Instantiate ResNet-10 and set up optimizer and criterion with weight decay\n",
        "resnet_model_weight_decay = ResNet10(ResidualBlock, [1, 1, 1, 1])\n",
        "criterion_resnet = nn.CrossEntropyLoss()\n",
        "optimizer_resnet_weight_decay = optim.SGD(resnet_model_weight_decay.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)\n",
        "\n",
        "# Train ResNet-10 with weight decay\n",
        "train_resnet_weight_decay(resnet_model_weight_decay, criterion_resnet, optimizer_resnet_weight_decay, train_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9a-YXlWE5lGv",
        "outputId": "8fbedc6a-9958-4b02-bf33-992ecf03e123"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch , Loss: 0.002962633167081477\n",
            "Epoch , Loss: 0.005987432911572859\n",
            "Epoch , Loss: 0.008967247765387416\n",
            "Epoch , Loss: 0.011915509048325326\n",
            "Epoch , Loss: 0.014814285366126643\n",
            "Epoch , Loss: 0.01775592702733891\n",
            "Epoch , Loss: 0.02067117983727809\n",
            "Epoch , Loss: 0.02354976557709677\n",
            "Epoch , Loss: 0.02641383095470536\n",
            "Epoch , Loss: 0.029264641539824894\n",
            "Epoch , Loss: 0.031989551261257944\n",
            "Epoch , Loss: 0.034843224698625254\n",
            "Epoch , Loss: 0.03768494549919577\n",
            "Epoch , Loss: 0.0405480831175509\n",
            "Epoch , Loss: 0.04331393528472432\n",
            "Epoch , Loss: 0.04614083998648407\n",
            "Epoch , Loss: 0.0487658181763671\n",
            "Epoch , Loss: 0.05139585590118642\n",
            "Epoch , Loss: 0.05399130250486876\n",
            "Epoch , Loss: 0.05663248279210552\n",
            "Epoch , Loss: 0.05925948510084616\n",
            "Epoch , Loss: 0.06178314225448062\n",
            "Epoch , Loss: 0.06438413224256861\n",
            "Epoch , Loss: 0.06708622176933776\n",
            "Epoch , Loss: 0.06968593612656264\n",
            "Epoch , Loss: 0.07232390081181246\n",
            "Epoch , Loss: 0.07503504170786085\n",
            "Epoch , Loss: 0.07777385958625227\n",
            "Epoch , Loss: 0.08025755784700593\n",
            "Epoch , Loss: 0.08281640658903\n",
            "Epoch , Loss: 0.08516258306210608\n",
            "Epoch , Loss: 0.08761327955728906\n",
            "Epoch , Loss: 0.09000589597560561\n",
            "Epoch , Loss: 0.09242570613656202\n",
            "Epoch , Loss: 0.09503537279260738\n",
            "Epoch , Loss: 0.09748850545614882\n",
            "Epoch , Loss: 0.09998253437564196\n",
            "Epoch , Loss: 0.10247473475878197\n",
            "Epoch , Loss: 0.1049738681834677\n",
            "Epoch , Loss: 0.10729862189353884\n",
            "Epoch , Loss: 0.10993348835679272\n",
            "Epoch , Loss: 0.11244040392243954\n",
            "Epoch , Loss: 0.11478836396161247\n",
            "Epoch , Loss: 0.11711514087589196\n",
            "Epoch , Loss: 0.11951004086857867\n",
            "Epoch , Loss: 0.12188209002585057\n",
            "Epoch , Loss: 0.12434629787264577\n",
            "Epoch , Loss: 0.12667332250443872\n",
            "Epoch , Loss: 0.1289560177442058\n",
            "Epoch , Loss: 0.13156145658639387\n",
            "Epoch , Loss: 0.13410900498899964\n",
            "Epoch , Loss: 0.13643923622872822\n",
            "Epoch , Loss: 0.13873629542567845\n",
            "Epoch , Loss: 0.1412059634237948\n",
            "Epoch , Loss: 0.1432757920316418\n",
            "Epoch , Loss: 0.14552349538144554\n",
            "Epoch , Loss: 0.14771920884661663\n",
            "Epoch , Loss: 0.1501617454506857\n",
            "Epoch , Loss: 0.1525272158405665\n",
            "Epoch , Loss: 0.15495258600205716\n",
            "Epoch , Loss: 0.15719984407010285\n",
            "Epoch , Loss: 0.15949358446213902\n",
            "Epoch , Loss: 0.1618457331376917\n",
            "Epoch , Loss: 0.16403710171389763\n",
            "Epoch , Loss: 0.16641764582880317\n",
            "Epoch , Loss: 0.1687749427602724\n",
            "Epoch , Loss: 0.1711837320071657\n",
            "Epoch , Loss: 0.1733742688622926\n",
            "Epoch , Loss: 0.17591176191559227\n",
            "Epoch , Loss: 0.1781939138536868\n",
            "Epoch , Loss: 0.18056483219956498\n",
            "Epoch , Loss: 0.18281067347587526\n",
            "Epoch , Loss: 0.18524014431497324\n",
            "Epoch , Loss: 0.18737947712164096\n",
            "Epoch , Loss: 0.18953426567184956\n",
            "Epoch , Loss: 0.19174637102410005\n",
            "Epoch , Loss: 0.19407056618834395\n",
            "Epoch , Loss: 0.1962996747182763\n",
            "Epoch , Loss: 0.19848779278338108\n",
            "Epoch , Loss: 0.20062100490950563\n",
            "Epoch , Loss: 0.202865584274692\n",
            "Epoch , Loss: 0.2048737360998188\n",
            "Epoch , Loss: 0.20711672504234802\n",
            "Epoch , Loss: 0.2092538860142993\n",
            "Epoch , Loss: 0.21124327807780116\n",
            "Epoch , Loss: 0.213598739155723\n",
            "Epoch , Loss: 0.2158826242017624\n",
            "Epoch , Loss: 0.21821198942106398\n",
            "Epoch , Loss: 0.2203298869645199\n",
            "Epoch , Loss: 0.22249221649316267\n",
            "Epoch , Loss: 0.22455678190416692\n",
            "Epoch , Loss: 0.2268101932752468\n",
            "Epoch , Loss: 0.2290227448239046\n",
            "Epoch , Loss: 0.23140219014014124\n",
            "Epoch , Loss: 0.233443732761666\n",
            "Epoch , Loss: 0.2357191948024818\n",
            "Epoch , Loss: 0.2380494008893552\n",
            "Epoch , Loss: 0.24000244341847843\n",
            "Epoch , Loss: 0.2419216503267703\n",
            "Epoch , Loss: 0.2441751711508807\n",
            "Epoch , Loss: 0.24649950762843842\n",
            "Epoch , Loss: 0.24847021538888098\n",
            "Epoch , Loss: 0.25085239565890766\n",
            "Epoch , Loss: 0.25292501227020303\n",
            "Epoch , Loss: 0.25514544550415197\n",
            "Epoch , Loss: 0.25724216480084394\n",
            "Epoch , Loss: 0.25922867754841095\n",
            "Epoch , Loss: 0.26112585619587425\n",
            "Epoch , Loss: 0.26348054165120627\n",
            "Epoch , Loss: 0.2657370649640213\n",
            "Epoch , Loss: 0.2677545780720918\n",
            "Epoch , Loss: 0.26993435026739565\n",
            "Epoch , Loss: 0.27196380518891317\n",
            "Epoch , Loss: 0.2742483464958113\n",
            "Epoch , Loss: 0.2762620284429292\n",
            "Epoch , Loss: 0.2783346606032623\n",
            "Epoch , Loss: 0.28074107968898687\n",
            "Epoch , Loss: 0.2827627210665847\n",
            "Epoch , Loss: 0.28480344583921113\n",
            "Epoch , Loss: 0.286744918664703\n",
            "Epoch , Loss: 0.2889630826537871\n",
            "Epoch , Loss: 0.2910539611526158\n",
            "Epoch , Loss: 0.29294418191056115\n",
            "Epoch , Loss: 0.29525080910119256\n",
            "Epoch , Loss: 0.29741948309456906\n",
            "Epoch , Loss: 0.29959234145596203\n",
            "Epoch , Loss: 0.3019154244066809\n",
            "Epoch , Loss: 0.30416624122263525\n",
            "Epoch , Loss: 0.3063612598592363\n",
            "Epoch , Loss: 0.3083726448171279\n",
            "Epoch , Loss: 0.31043718019714744\n",
            "Epoch , Loss: 0.3123715378134452\n",
            "Epoch , Loss: 0.3142926914002889\n",
            "Epoch , Loss: 0.3166056266221244\n",
            "Epoch , Loss: 0.3187582623928099\n",
            "Epoch , Loss: 0.32080536257580416\n",
            "Epoch , Loss: 0.3229429727929937\n",
            "Epoch , Loss: 0.32500501590616565\n",
            "Epoch , Loss: 0.3271309404117067\n",
            "Epoch , Loss: 0.3291473722518862\n",
            "Epoch , Loss: 0.33118763070582125\n",
            "Epoch , Loss: 0.3331166668926054\n",
            "Epoch , Loss: 0.3353899584706787\n",
            "Epoch , Loss: 0.3374646914279674\n",
            "Epoch , Loss: 0.33948530794104653\n",
            "Epoch , Loss: 0.34154797911339096\n",
            "Epoch , Loss: 0.3436477408384728\n",
            "Epoch , Loss: 0.3454939806857682\n",
            "Epoch , Loss: 0.34767638204042867\n",
            "Epoch , Loss: 0.34971507507212024\n",
            "Epoch , Loss: 0.3517825255918381\n",
            "Epoch , Loss: 0.35374461308769556\n",
            "Epoch , Loss: 0.3559484539739311\n",
            "Epoch , Loss: 0.3579859523212208\n",
            "Epoch , Loss: 0.360103323788899\n",
            "Epoch , Loss: 0.3620697064777774\n",
            "Epoch , Loss: 0.3641674367668074\n",
            "Epoch , Loss: 0.36624933737318227\n",
            "Epoch , Loss: 0.3684092781427876\n",
            "Epoch , Loss: 0.37049750385381985\n",
            "Epoch , Loss: 0.3723719560581705\n",
            "Epoch , Loss: 0.37444957320952355\n",
            "Epoch , Loss: 0.3764799619879564\n",
            "Epoch , Loss: 0.37842966879115386\n",
            "Epoch , Loss: 0.380624255560853\n",
            "Epoch , Loss: 0.3826371088357228\n",
            "Epoch , Loss: 0.38460112014390013\n",
            "Epoch , Loss: 0.386819517978317\n",
            "Epoch , Loss: 0.3888994917235411\n",
            "Epoch , Loss: 0.39091385904785314\n",
            "Epoch , Loss: 0.3931224713545016\n",
            "Epoch , Loss: 0.39526880915512513\n",
            "Epoch , Loss: 0.39728333562841195\n",
            "Epoch , Loss: 0.3994696884204055\n",
            "Epoch , Loss: 0.4015309955457897\n",
            "Epoch , Loss: 0.4034271680790445\n",
            "Epoch , Loss: 0.40520769921715\n",
            "Epoch , Loss: 0.407432066052771\n",
            "Epoch , Loss: 0.40931873873371605\n",
            "Epoch , Loss: 0.41148283521232704\n",
            "Epoch , Loss: 0.41357606359759863\n",
            "Epoch , Loss: 0.4154426734465772\n",
            "Epoch , Loss: 0.4172409916167979\n",
            "Epoch , Loss: 0.41934680923476547\n",
            "Epoch , Loss: 0.4211523377377054\n",
            "Epoch , Loss: 0.42308921597497845\n",
            "Epoch , Loss: 0.42503204964615804\n",
            "Epoch , Loss: 0.4270970484484797\n",
            "Epoch , Loss: 0.4290807238015372\n",
            "Epoch , Loss: 0.43113422973076704\n",
            "Epoch , Loss: 0.4331052303314209\n",
            "Epoch , Loss: 0.43507689527233545\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-ccca9eecd085>\u001b[0m in \u001b[0;36m<cell line: 125>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;31m# Train ResNet-10 with weight decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mtrain_resnet_weight_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_model_weight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_resnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_resnet_weight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-ccca9eecd085>\u001b[0m in \u001b[0;36mtrain_resnet_weight_decay\u001b[0;34m(model, criterion, optimizer, train_loader, num_epochs, device)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-ccca9eecd085>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-ccca9eecd085>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# Replace the existing device check and set\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Load CIFAR-10 dataset with standard normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Define Residual Block with dropout\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, dropout_prob=0.3):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.dropout = nn.Dropout2d(p=dropout_prob)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        if self.stride != 1 or identity.shape[1] != out.shape[1]:\n",
        "            identity = self.conv1(identity)\n",
        "            identity = self.bn1(identity)\n",
        "            identity = self.dropout(identity)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Define ResNet-10 with dropout\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=10, dropout_prob=0.3):\n",
        "        super(ResNet10, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 64, layers[0], stride=1, dropout_prob=dropout_prob)\n",
        "        self.layer2 = self.make_layer(block, 128, layers[1], stride=2, dropout_prob=dropout_prob)\n",
        "        self.layer3 = self.make_layer(block, 256, layers[2], stride=2, dropout_prob=dropout_prob)\n",
        "        self.layer4 = self.make_layer(block, 512, layers[3], stride=2, dropout_prob=dropout_prob)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, blocks, stride=1, dropout_prob=0.3):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, dropout_prob))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels, stride=1, dropout_prob=dropout_prob))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Training function with dropout\n",
        "def train_resnet_dropout(model, criterion, optimizer, train_loader, num_epochs=300, device='cpu'):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    start_time = time.time()\n",
        "    all_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        num = epoch + 1\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Store training loss for each epoch and print after the inner loop\n",
        "            all_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "        # Print training loss after each epoch\n",
        "            print(f'Epoch {num}, Loss: {all_losses[-1]}')\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    print(f'Training Time: {training_time} seconds')\n",
        "    return all_losses\n",
        "\n",
        "# Instantiate ResNet-10 with dropout and set up optimizer and criterion\n",
        "resnet_model_dropout = ResNet10(ResidualBlock, [1, 1, 1, 1], dropout_prob=0.3)\n",
        "criterion_resnet_dropout = nn.CrossEntropyLoss()\n",
        "optimizer_resnet_dropout = optim.SGD(resnet_model_dropout.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train ResNet-10 with dropout\n",
        "train_resnet_dropout(resnet_model_dropout, criterion_resnet_dropout, optimizer_resnet_dropout, train_loader)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhRVEe3g53ir",
        "outputId": "7cd5ec9d-b4f0-4f0f-bb37-17a893098716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 0.0030231353876840734\n",
            "Epoch 1, Loss: 0.006057209370996032\n",
            "Epoch 1, Loss: 0.009060975535751303\n",
            "Epoch 1, Loss: 0.0120881689174096\n",
            "Epoch 1, Loss: 0.015102638917810777\n",
            "Epoch 1, Loss: 0.018203825292075076\n",
            "Epoch 1, Loss: 0.021216876061676104\n",
            "Epoch 1, Loss: 0.02415691281828429\n",
            "Epoch 1, Loss: 0.027037713533777104\n",
            "Epoch 1, Loss: 0.02998591109614848\n",
            "Epoch 1, Loss: 0.032857113482092346\n",
            "Epoch 1, Loss: 0.035885130657869226\n",
            "Epoch 1, Loss: 0.038802599663014914\n",
            "Epoch 1, Loss: 0.0416792679930587\n",
            "Epoch 1, Loss: 0.04459075183819627\n",
            "Epoch 1, Loss: 0.04750200458194898\n",
            "Epoch 1, Loss: 0.050443507216470625\n",
            "Epoch 1, Loss: 0.05331579193739635\n",
            "Epoch 1, Loss: 0.05619356241982306\n",
            "Epoch 1, Loss: 0.059019009780395974\n",
            "Epoch 1, Loss: 0.061826353487761124\n",
            "Epoch 1, Loss: 0.06482020454943332\n",
            "Epoch 1, Loss: 0.0676634939735198\n",
            "Epoch 1, Loss: 0.07051452011098643\n",
            "Epoch 1, Loss: 0.07337666502998919\n",
            "Epoch 1, Loss: 0.07622184960738472\n",
            "Epoch 1, Loss: 0.07905221290295691\n",
            "Epoch 1, Loss: 0.08191825331324507\n",
            "Epoch 1, Loss: 0.0847434936581975\n",
            "Epoch 1, Loss: 0.08764268858048617\n",
            "Epoch 1, Loss: 0.09045293538466743\n",
            "Epoch 1, Loss: 0.09319392677463229\n",
            "Epoch 1, Loss: 0.09603422011255913\n",
            "Epoch 1, Loss: 0.09888049830561099\n",
            "Epoch 1, Loss: 0.10165352132314306\n",
            "Epoch 1, Loss: 0.10428915914062344\n",
            "Epoch 1, Loss: 0.10709637052872602\n",
            "Epoch 1, Loss: 0.10992279656402901\n",
            "Epoch 1, Loss: 0.11271746750073054\n",
            "Epoch 1, Loss: 0.11540979954897596\n",
            "Epoch 1, Loss: 0.11810936250954943\n",
            "Epoch 1, Loss: 0.12082546690235967\n",
            "Epoch 1, Loss: 0.12352805430322047\n",
            "Epoch 1, Loss: 0.12630946252047254\n",
            "Epoch 1, Loss: 0.12897927285460256\n",
            "Epoch 1, Loss: 0.13153927984749875\n",
            "Epoch 1, Loss: 0.13407930099140958\n",
            "Epoch 1, Loss: 0.13676393809525864\n",
            "Epoch 1, Loss: 0.13927508177964584\n",
            "Epoch 1, Loss: 0.1418034690420341\n",
            "Epoch 1, Loss: 0.1444118103712721\n",
            "Epoch 1, Loss: 0.1469674887864486\n",
            "Epoch 1, Loss: 0.14964194096567685\n",
            "Epoch 1, Loss: 0.15221838390125947\n",
            "Epoch 1, Loss: 0.15473674599776793\n",
            "Epoch 1, Loss: 0.1572229011589304\n",
            "Epoch 1, Loss: 0.15992706937863088\n",
            "Epoch 1, Loss: 0.16254809841780407\n",
            "Epoch 1, Loss: 0.16548815865041044\n",
            "Epoch 1, Loss: 0.16790583569680334\n",
            "Epoch 1, Loss: 0.17074303081273423\n",
            "Epoch 1, Loss: 0.173251182374442\n",
            "Epoch 1, Loss: 0.1759878057043266\n",
            "Epoch 1, Loss: 0.17859704095079465\n",
            "Epoch 1, Loss: 0.1810753414088198\n",
            "Epoch 1, Loss: 0.18353905016199099\n",
            "Epoch 1, Loss: 0.18611178206055976\n",
            "Epoch 1, Loss: 0.18866183629731084\n",
            "Epoch 1, Loss: 0.19131972326342103\n",
            "Epoch 1, Loss: 0.19395087320176538\n",
            "Epoch 1, Loss: 0.1963742739709137\n",
            "Epoch 1, Loss: 0.1987642787606515\n",
            "Epoch 1, Loss: 0.2011794739062219\n",
            "Epoch 1, Loss: 0.20374451909223787\n",
            "Epoch 1, Loss: 0.20623218967481646\n",
            "Epoch 1, Loss: 0.2088489933391971\n",
            "Epoch 1, Loss: 0.21132688936979874\n",
            "Epoch 1, Loss: 0.2139452537307349\n",
            "Epoch 1, Loss: 0.21643765106835328\n",
            "Epoch 1, Loss: 0.21900103311709432\n",
            "Epoch 1, Loss: 0.2217886313757933\n",
            "Epoch 1, Loss: 0.2244009852714246\n",
            "Epoch 1, Loss: 0.22687999946077156\n",
            "Epoch 1, Loss: 0.22951482750875565\n",
            "Epoch 1, Loss: 0.23195827647548198\n",
            "Epoch 1, Loss: 0.23468189867561126\n",
            "Epoch 1, Loss: 0.23740124184152353\n",
            "Epoch 1, Loss: 0.23993996281148222\n",
            "Epoch 1, Loss: 0.2426672658651991\n",
            "Epoch 1, Loss: 0.2454323097872917\n",
            "Epoch 1, Loss: 0.24782677745575185\n",
            "Epoch 1, Loss: 0.2502691299104325\n",
            "Epoch 1, Loss: 0.2528521888091436\n",
            "Epoch 1, Loss: 0.2554953679099412\n",
            "Epoch 1, Loss: 0.2579528988169892\n",
            "Epoch 1, Loss: 0.2605272944625991\n",
            "Epoch 1, Loss: 0.2631093896258518\n",
            "Epoch 1, Loss: 0.2655835758389719\n",
            "Epoch 1, Loss: 0.26809070863382284\n",
            "Epoch 1, Loss: 0.270594665461489\n",
            "Epoch 1, Loss: 0.2729336954748539\n",
            "Epoch 1, Loss: 0.2753234938587374\n",
            "Epoch 1, Loss: 0.27773933002101187\n",
            "Epoch 1, Loss: 0.28025516783794785\n",
            "Epoch 1, Loss: 0.2826962507594272\n",
            "Epoch 1, Loss: 0.28516121273455414\n",
            "Epoch 1, Loss: 0.2876700644602861\n",
            "Epoch 1, Loss: 0.28987543829871565\n",
            "Epoch 1, Loss: 0.2923804771564806\n",
            "Epoch 1, Loss: 0.2948193952555547\n",
            "Epoch 1, Loss: 0.29721734819509793\n",
            "Epoch 1, Loss: 0.2996645757304433\n",
            "Epoch 1, Loss: 0.3021504392709269\n",
            "Epoch 1, Loss: 0.3045857373405905\n",
            "Epoch 1, Loss: 0.30697083579914647\n",
            "Epoch 1, Loss: 0.3092066987091318\n",
            "Epoch 1, Loss: 0.3117455808098054\n",
            "Epoch 1, Loss: 0.31433360655899245\n",
            "Epoch 1, Loss: 0.31658223736316654\n",
            "Epoch 1, Loss: 0.3187764072052353\n",
            "Epoch 1, Loss: 0.32113238948080547\n",
            "Epoch 1, Loss: 0.32368267664823996\n",
            "Epoch 1, Loss: 0.3261022772020696\n",
            "Epoch 1, Loss: 0.32854119774020846\n",
            "Epoch 1, Loss: 0.33097014585724266\n",
            "Epoch 1, Loss: 0.3335072312818464\n",
            "Epoch 1, Loss: 0.3360725993390583\n",
            "Epoch 1, Loss: 0.3385397479357317\n",
            "Epoch 1, Loss: 0.34081455264859795\n",
            "Epoch 1, Loss: 0.3430184982621761\n",
            "Epoch 1, Loss: 0.34540852820476914\n",
            "Epoch 1, Loss: 0.3479935493310699\n",
            "Epoch 1, Loss: 0.350539035961756\n",
            "Epoch 1, Loss: 0.35283280867140004\n",
            "Epoch 1, Loss: 0.3551954596548739\n",
            "Epoch 1, Loss: 0.35748487238383964\n",
            "Epoch 1, Loss: 0.35976477596156126\n",
            "Epoch 1, Loss: 0.36229324737168334\n",
            "Epoch 1, Loss: 0.36475448443761566\n",
            "Epoch 1, Loss: 0.3673314462842234\n",
            "Epoch 1, Loss: 0.36963540559534525\n",
            "Epoch 1, Loss: 0.3720497889896793\n",
            "Epoch 1, Loss: 0.37446254735712503\n",
            "Epoch 1, Loss: 0.376606290602623\n",
            "Epoch 1, Loss: 0.37888166742861423\n",
            "Epoch 1, Loss: 0.3814553959900156\n",
            "Epoch 1, Loss: 0.3837978388647289\n",
            "Epoch 1, Loss: 0.3861929762089039\n",
            "Epoch 1, Loss: 0.38872516917450656\n",
            "Epoch 1, Loss: 0.3911024675039989\n",
            "Epoch 1, Loss: 0.3936390838659633\n",
            "Epoch 1, Loss: 0.396249410593906\n",
            "Epoch 1, Loss: 0.3985852358286338\n",
            "Epoch 1, Loss: 0.40083868759672353\n",
            "Epoch 1, Loss: 0.40317302667881216\n",
            "Epoch 1, Loss: 0.4055462293612683\n",
            "Epoch 1, Loss: 0.4079464786802716\n",
            "Epoch 1, Loss: 0.4102664929826546\n",
            "Epoch 1, Loss: 0.41262597836496884\n",
            "Epoch 1, Loss: 0.4151684756169234\n",
            "Epoch 1, Loss: 0.4175642207455452\n",
            "Epoch 1, Loss: 0.4199990602710363\n",
            "Epoch 1, Loss: 0.42225868028143176\n",
            "Epoch 1, Loss: 0.42463844359073494\n",
            "Epoch 1, Loss: 0.4270936761365827\n",
            "Epoch 1, Loss: 0.42946934014025245\n",
            "Epoch 1, Loss: 0.43181790339062587\n",
            "Epoch 1, Loss: 0.43429293504456423\n",
            "Epoch 1, Loss: 0.4364881253303469\n",
            "Epoch 1, Loss: 0.43900568588920263\n",
            "Epoch 1, Loss: 0.4413871158419363\n",
            "Epoch 1, Loss: 0.4436164740711222\n",
            "Epoch 1, Loss: 0.44595544676646554\n",
            "Epoch 1, Loss: 0.4481254630076611\n",
            "Epoch 1, Loss: 0.45029756967978707\n",
            "Epoch 1, Loss: 0.45282700878884785\n",
            "Epoch 1, Loss: 0.4550693003113008\n",
            "Epoch 1, Loss: 0.45739277762830105\n",
            "Epoch 1, Loss: 0.4597107511956978\n",
            "Epoch 1, Loss: 0.4619112383679051\n",
            "Epoch 1, Loss: 0.46424460121432837\n",
            "Epoch 1, Loss: 0.4664942853895904\n",
            "Epoch 1, Loss: 0.46874740270092663\n",
            "Epoch 1, Loss: 0.4709293990183974\n",
            "Epoch 1, Loss: 0.4731341949509233\n",
            "Epoch 1, Loss: 0.47522796023532254\n",
            "Epoch 1, Loss: 0.4774854451494144\n",
            "Epoch 1, Loss: 0.4796816075549406\n",
            "Epoch 1, Loss: 0.48191864822831604\n",
            "Epoch 1, Loss: 0.4840784615567883\n",
            "Epoch 1, Loss: 0.48642249546392496\n",
            "Epoch 1, Loss: 0.4886440930464079\n",
            "Epoch 1, Loss: 0.49097705664842023\n",
            "Epoch 1, Loss: 0.49317432806619904\n",
            "Epoch 1, Loss: 0.4953219512539446\n",
            "Epoch 1, Loss: 0.4976487069788491\n",
            "Epoch 1, Loss: 0.500155083053862\n",
            "Epoch 1, Loss: 0.5023460039092452\n",
            "Epoch 1, Loss: 0.5046310136690164\n",
            "Epoch 1, Loss: 0.5069651974131689\n",
            "Epoch 1, Loss: 0.5094398101577369\n",
            "Epoch 1, Loss: 0.5117711214458242\n",
            "Epoch 1, Loss: 0.5139907773803262\n",
            "Epoch 1, Loss: 0.5162648532701575\n",
            "Epoch 1, Loss: 0.518493984971205\n",
            "Epoch 1, Loss: 0.520878767265993\n",
            "Epoch 1, Loss: 0.5233530085105116\n",
            "Epoch 1, Loss: 0.5258613681549307\n",
            "Epoch 1, Loss: 0.528177389860763\n",
            "Epoch 1, Loss: 0.5306620895100371\n",
            "Epoch 1, Loss: 0.532956562536147\n",
            "Epoch 1, Loss: 0.5351229919801892\n",
            "Epoch 1, Loss: 0.5376265279167448\n",
            "Epoch 1, Loss: 0.5401251293204324\n",
            "Epoch 1, Loss: 0.5421837103336363\n",
            "Epoch 1, Loss: 0.5443925851446283\n",
            "Epoch 1, Loss: 0.5468366574448393\n",
            "Epoch 1, Loss: 0.5492499908217994\n",
            "Epoch 1, Loss: 0.5513386134906193\n",
            "Epoch 1, Loss: 0.5536697240131895\n",
            "Epoch 1, Loss: 0.556161953512665\n",
            "Epoch 1, Loss: 0.5584244444547102\n",
            "Epoch 1, Loss: 0.5606272498055187\n",
            "Epoch 1, Loss: 0.5630104663731802\n",
            "Epoch 1, Loss: 0.565336156836556\n",
            "Epoch 1, Loss: 0.5677625739665897\n",
            "Epoch 1, Loss: 0.569875895367254\n",
            "Epoch 1, Loss: 0.5720142211450641\n",
            "Epoch 1, Loss: 0.5741395938122059\n",
            "Epoch 1, Loss: 0.5765124076162763\n",
            "Epoch 1, Loss: 0.5787057413164612\n",
            "Epoch 1, Loss: 0.5808083253443393\n",
            "Epoch 1, Loss: 0.5831000643313083\n",
            "Epoch 1, Loss: 0.5853697203309335\n",
            "Epoch 1, Loss: 0.5875396408388377\n",
            "Epoch 1, Loss: 0.5898218810405877\n",
            "Epoch 1, Loss: 0.592113424902377\n",
            "Epoch 1, Loss: 0.5942696143904\n",
            "Epoch 1, Loss: 0.5964801021853982\n",
            "Epoch 1, Loss: 0.5988758206367493\n",
            "Epoch 1, Loss: 0.6009462218150459\n",
            "Epoch 1, Loss: 0.6032345711117815\n",
            "Epoch 1, Loss: 0.6054373689929543\n",
            "Epoch 1, Loss: 0.6077178612999294\n",
            "Epoch 1, Loss: 0.6099351267985371\n",
            "Epoch 1, Loss: 0.6120971910788885\n",
            "Epoch 1, Loss: 0.6143279013121524\n",
            "Epoch 1, Loss: 0.6163199444865937\n",
            "Epoch 1, Loss: 0.6185788382654605\n",
            "Epoch 1, Loss: 0.6208682223354154\n",
            "Epoch 1, Loss: 0.6230136531088358\n",
            "Epoch 1, Loss: 0.6252670661567727\n",
            "Epoch 1, Loss: 0.6274850177947823\n",
            "Epoch 1, Loss: 0.6295612957471471\n",
            "Epoch 1, Loss: 0.6318101017066585\n",
            "Epoch 1, Loss: 0.6339649840084183\n",
            "Epoch 1, Loss: 0.6362461700768727\n",
            "Epoch 1, Loss: 0.638302796941889\n",
            "Epoch 1, Loss: 0.6406333463271255\n",
            "Epoch 1, Loss: 0.6427589548213403\n",
            "Epoch 1, Loss: 0.6450209079496086\n",
            "Epoch 1, Loss: 0.6472055027856851\n",
            "Epoch 1, Loss: 0.6493898508188974\n",
            "Epoch 1, Loss: 0.6517276388909811\n",
            "Epoch 1, Loss: 0.654016825549133\n",
            "Epoch 1, Loss: 0.656075164027836\n",
            "Epoch 1, Loss: 0.6585033086254773\n",
            "Epoch 1, Loss: 0.6609670567085676\n",
            "Epoch 1, Loss: 0.6631138312542225\n",
            "Epoch 1, Loss: 0.6653069365969704\n",
            "Epoch 1, Loss: 0.6673120627622775\n",
            "Epoch 1, Loss: 0.6694131697840093\n",
            "Epoch 1, Loss: 0.6718021762340575\n",
            "Epoch 1, Loss: 0.6739282458639511\n",
            "Epoch 1, Loss: 0.6759998158115865\n",
            "Epoch 1, Loss: 0.6781102801527819\n",
            "Epoch 1, Loss: 0.6804082850970881\n",
            "Epoch 1, Loss: 0.682461612364825\n",
            "Epoch 1, Loss: 0.6844806527847525\n",
            "Epoch 1, Loss: 0.6869310328112844\n",
            "Epoch 1, Loss: 0.689114289363022\n",
            "Epoch 1, Loss: 0.6910128442527693\n",
            "Epoch 1, Loss: 0.693177370311659\n",
            "Epoch 1, Loss: 0.6955318670443562\n",
            "Epoch 1, Loss: 0.6977430155210178\n",
            "Epoch 1, Loss: 0.7003372605804288\n",
            "Epoch 1, Loss: 0.702622878734413\n",
            "Epoch 1, Loss: 0.7050824741573285\n",
            "Epoch 1, Loss: 0.7071903940966672\n",
            "Epoch 1, Loss: 0.7094481415151025\n",
            "Epoch 1, Loss: 0.7120200380339952\n",
            "Epoch 1, Loss: 0.7144176054488668\n",
            "Epoch 1, Loss: 0.7166386714676762\n",
            "Epoch 1, Loss: 0.718806302760873\n",
            "Epoch 1, Loss: 0.7208863426657284\n",
            "Epoch 1, Loss: 0.7229690830725843\n",
            "Epoch 1, Loss: 0.7252966150298448\n",
            "Epoch 1, Loss: 0.7274840722608444\n",
            "Epoch 1, Loss: 0.7298917657578997\n",
            "Epoch 1, Loss: 0.7320961494884832\n",
            "Epoch 1, Loss: 0.7342656757825475\n",
            "Epoch 1, Loss: 0.73641095792546\n",
            "Epoch 1, Loss: 0.7388230007322852\n",
            "Epoch 1, Loss: 0.7411886529849313\n",
            "Epoch 1, Loss: 0.7432502441089172\n",
            "Epoch 1, Loss: 0.7451799807646086\n",
            "Epoch 1, Loss: 0.7470295560329466\n",
            "Epoch 1, Loss: 0.7492247969293229\n",
            "Epoch 1, Loss: 0.751276979665927\n",
            "Epoch 1, Loss: 0.7532497620033791\n",
            "Epoch 1, Loss: 0.755468034683286\n",
            "Epoch 1, Loss: 0.757675541026513\n",
            "Epoch 1, Loss: 0.7596592049464546\n",
            "Epoch 1, Loss: 0.7620006642683083\n",
            "Epoch 1, Loss: 0.7644006443755401\n",
            "Epoch 1, Loss: 0.7664161304683637\n",
            "Epoch 1, Loss: 0.768517526214385\n",
            "Epoch 1, Loss: 0.7708600736639993\n",
            "Epoch 1, Loss: 0.7729299623338158\n",
            "Epoch 1, Loss: 0.7751551985435778\n",
            "Epoch 1, Loss: 0.7773718001592494\n",
            "Epoch 1, Loss: 0.7796117597833618\n",
            "Epoch 1, Loss: 0.7821519688876999\n",
            "Epoch 1, Loss: 0.7842510964559473\n",
            "Epoch 1, Loss: 0.786503543939127\n",
            "Epoch 1, Loss: 0.7887281305954584\n",
            "Epoch 1, Loss: 0.7907739821297434\n",
            "Epoch 1, Loss: 0.7930957382292394\n",
            "Epoch 1, Loss: 0.7952800664450507\n",
            "Epoch 1, Loss: 0.797463111560363\n",
            "Epoch 1, Loss: 0.7996590314313884\n",
            "Epoch 1, Loss: 0.8017125305007485\n",
            "Epoch 1, Loss: 0.8040075113096505\n",
            "Epoch 1, Loss: 0.8060297249528148\n",
            "Epoch 1, Loss: 0.8080950339736841\n",
            "Epoch 1, Loss: 0.8104160955494932\n",
            "Epoch 1, Loss: 0.8126816862379499\n",
            "Epoch 1, Loss: 0.8149088619615111\n",
            "Epoch 1, Loss: 0.8170974504612291\n",
            "Epoch 1, Loss: 0.8190477191639678\n",
            "Epoch 1, Loss: 0.8209581625126207\n",
            "Epoch 1, Loss: 0.8230219111418176\n",
            "Epoch 1, Loss: 0.82516273360728\n",
            "Epoch 1, Loss: 0.8273337494076975\n",
            "Epoch 1, Loss: 0.8296255120231063\n",
            "Epoch 1, Loss: 0.831506849402357\n",
            "Epoch 1, Loss: 0.8334328465144653\n",
            "Epoch 1, Loss: 0.8357797101940341\n",
            "Epoch 1, Loss: 0.837841433789724\n",
            "Epoch 1, Loss: 0.8398184143673734\n",
            "Epoch 1, Loss: 0.8417272579944347\n",
            "Epoch 1, Loss: 0.8440632713420312\n",
            "Epoch 1, Loss: 0.8462064267729249\n",
            "Epoch 1, Loss: 0.8484070147089946\n",
            "Epoch 1, Loss: 0.8505291492128007\n",
            "Epoch 1, Loss: 0.8526829407952935\n",
            "Epoch 1, Loss: 0.8547722099687133\n",
            "Epoch 1, Loss: 0.8568311882445879\n",
            "Epoch 1, Loss: 0.8588428067429291\n",
            "Epoch 1, Loss: 0.8609450314660816\n",
            "Epoch 1, Loss: 0.862928791271756\n",
            "Epoch 1, Loss: 0.8653052924844005\n",
            "Epoch 1, Loss: 0.8674860079879956\n",
            "Epoch 1, Loss: 0.8696835755996997\n",
            "Epoch 1, Loss: 0.8718145928724342\n",
            "Epoch 1, Loss: 0.8741669485636074\n",
            "Epoch 1, Loss: 0.8763197065924134\n",
            "Epoch 1, Loss: 0.8785833888651465\n",
            "Epoch 1, Loss: 0.8805842314229901\n",
            "Epoch 1, Loss: 0.8826080159762936\n",
            "Epoch 1, Loss: 0.884717793873204\n",
            "Epoch 1, Loss: 0.8869472046947235\n",
            "Epoch 1, Loss: 0.8891356191061952\n",
            "Epoch 1, Loss: 0.8912898741109901\n",
            "Epoch 1, Loss: 0.893324212345016\n",
            "Epoch 1, Loss: 0.8952939514918705\n",
            "Epoch 1, Loss: 0.8971144971640214\n",
            "Epoch 1, Loss: 0.899083908713992\n",
            "Epoch 1, Loss: 0.9011295556717211\n",
            "Epoch 1, Loss: 0.9031825387264456\n",
            "Epoch 1, Loss: 0.9051907163141938\n",
            "Epoch 1, Loss: 0.9071145732994275\n",
            "Epoch 1, Loss: 0.9091122325728921\n",
            "Epoch 1, Loss: 0.9111533515593585\n",
            "Epoch 1, Loss: 0.9134213287202294\n",
            "Epoch 1, Loss: 0.9156215515587945\n",
            "Epoch 1, Loss: 0.9176743440615857\n",
            "Epoch 1, Loss: 0.9196596759969317\n",
            "Epoch 1, Loss: 0.9219825144314095\n",
            "Epoch 1, Loss: 0.9240135108418477\n",
            "Epoch 1, Loss: 0.9260938553249135\n",
            "Epoch 1, Loss: 0.9283002093624886\n",
            "Epoch 1, Loss: 0.9302010845650187\n",
            "Epoch 1, Loss: 0.9321017225685022\n",
            "Epoch 1, Loss: 0.9342076022301793\n",
            "Epoch 1, Loss: 0.9363009438795202\n",
            "Epoch 1, Loss: 0.9384398560999604\n",
            "Epoch 1, Loss: 0.9404283827528015\n",
            "Epoch 1, Loss: 0.9423276250014829\n",
            "Epoch 1, Loss: 0.9443464420945443\n",
            "Epoch 1, Loss: 0.9464985469113225\n",
            "Epoch 1, Loss: 0.9485976258507165\n",
            "Epoch 1, Loss: 0.9508744764815816\n",
            "Epoch 1, Loss: 0.9527895330163219\n",
            "Epoch 1, Loss: 0.9548584526152257\n",
            "Epoch 1, Loss: 0.9570800198618409\n",
            "Epoch 1, Loss: 0.9594158999755255\n",
            "Epoch 1, Loss: 0.9616440897402556\n",
            "Epoch 1, Loss: 0.9635042654888709\n",
            "Epoch 1, Loss: 0.9655867770809652\n",
            "Epoch 1, Loss: 0.967851543365537\n",
            "Epoch 1, Loss: 0.9697861880292673\n",
            "Epoch 1, Loss: 0.9718697891210961\n",
            "Epoch 1, Loss: 0.9737511240612821\n",
            "Epoch 1, Loss: 0.9760104885796452\n",
            "Epoch 1, Loss: 0.978082498626026\n",
            "Epoch 1, Loss: 0.9800772583088302\n",
            "Epoch 1, Loss: 0.9820233853271855\n",
            "Epoch 1, Loss: 0.9839294904943012\n",
            "Epoch 1, Loss: 0.9859331012381922\n",
            "Epoch 1, Loss: 0.9882262073209523\n",
            "Epoch 1, Loss: 0.9902159836895935\n",
            "Epoch 1, Loss: 0.9924026822190151\n",
            "Epoch 1, Loss: 0.9946755804978978\n",
            "Epoch 1, Loss: 0.9967135771766038\n",
            "Epoch 1, Loss: 0.998698471147386\n",
            "Epoch 1, Loss: 1.0010536776479249\n",
            "Epoch 1, Loss: 1.0031418864379453\n",
            "Epoch 1, Loss: 1.0052476032920505\n",
            "Epoch 1, Loss: 1.0073631372293244\n",
            "Epoch 1, Loss: 1.009361575783976\n",
            "Epoch 1, Loss: 1.011504122363332\n",
            "Epoch 1, Loss: 1.013711727183798\n",
            "Epoch 1, Loss: 1.0156145545527757\n",
            "Epoch 1, Loss: 1.0174995066259829\n",
            "Epoch 1, Loss: 1.0193471504599236\n",
            "Epoch 1, Loss: 1.0216465313416307\n",
            "Epoch 1, Loss: 1.0235164807275738\n",
            "Epoch 1, Loss: 1.0254950023368192\n",
            "Epoch 1, Loss: 1.0276386251534952\n",
            "Epoch 1, Loss: 1.0297039431684158\n",
            "Epoch 1, Loss: 1.0318395216446703\n",
            "Epoch 1, Loss: 1.0339367400349864\n",
            "Epoch 1, Loss: 1.0357999181198647\n",
            "Epoch 1, Loss: 1.037988667292973\n",
            "Epoch 1, Loss: 1.0402175235321454\n",
            "Epoch 1, Loss: 1.0422068642228461\n",
            "Epoch 1, Loss: 1.0444096930496527\n",
            "Epoch 1, Loss: 1.046722199910742\n",
            "Epoch 1, Loss: 1.0488977110599313\n",
            "Epoch 1, Loss: 1.0509276124827391\n",
            "Epoch 1, Loss: 1.0529805141336777\n",
            "Epoch 1, Loss: 1.0549386594911365\n",
            "Epoch 1, Loss: 1.0571111253155467\n",
            "Epoch 1, Loss: 1.0592503814441163\n",
            "Epoch 1, Loss: 1.061082458709512\n",
            "Epoch 1, Loss: 1.0633153326980902\n",
            "Epoch 1, Loss: 1.0656560121289909\n",
            "Epoch 1, Loss: 1.067408290970356\n",
            "Epoch 1, Loss: 1.069439939220848\n",
            "Epoch 1, Loss: 1.0715891746303918\n",
            "Epoch 1, Loss: 1.0736917094196505\n",
            "Epoch 1, Loss: 1.0759229734730538\n",
            "Epoch 1, Loss: 1.0779339893699607\n",
            "Epoch 1, Loss: 1.0797826762089644\n",
            "Epoch 1, Loss: 1.0817703362316122\n",
            "Epoch 1, Loss: 1.083895327185121\n",
            "Epoch 1, Loss: 1.0860805131895157\n",
            "Epoch 1, Loss: 1.0879452751420648\n",
            "Epoch 1, Loss: 1.0901246738555792\n",
            "Epoch 1, Loss: 1.0921596555453736\n",
            "Epoch 1, Loss: 1.0942016129603471\n",
            "Epoch 1, Loss: 1.096332257208617\n",
            "Epoch 1, Loss: 1.0983820179539263\n",
            "Epoch 1, Loss: 1.1004162702109197\n",
            "Epoch 1, Loss: 1.1025155147018335\n",
            "Epoch 1, Loss: 1.1046651714598126\n",
            "Epoch 1, Loss: 1.1066731630688738\n",
            "Epoch 1, Loss: 1.1086963750517276\n",
            "Epoch 1, Loss: 1.110964845818327\n",
            "Epoch 1, Loss: 1.1131556877089888\n",
            "Epoch 1, Loss: 1.1151824624032316\n",
            "Epoch 1, Loss: 1.117188635079757\n",
            "Epoch 1, Loss: 1.1192263104116824\n",
            "Epoch 1, Loss: 1.1213121304426656\n",
            "Epoch 1, Loss: 1.1232362022180387\n",
            "Epoch 1, Loss: 1.1254901352440914\n",
            "Epoch 1, Loss: 1.1276782482786252\n",
            "Epoch 1, Loss: 1.129680450919949\n",
            "Epoch 1, Loss: 1.1318124363489468\n",
            "Epoch 1, Loss: 1.1337489493362738\n",
            "Epoch 1, Loss: 1.1354583539926182\n",
            "Epoch 1, Loss: 1.1371855979685284\n",
            "Epoch 1, Loss: 1.1390466888237487\n",
            "Epoch 1, Loss: 1.141218889552309\n",
            "Epoch 1, Loss: 1.1432276038867433\n",
            "Epoch 1, Loss: 1.1453165007979058\n",
            "Epoch 1, Loss: 1.1473588777320158\n",
            "Epoch 1, Loss: 1.1494556817862078\n",
            "Epoch 1, Loss: 1.1514459580106808\n",
            "Epoch 1, Loss: 1.1533076523819847\n",
            "Epoch 1, Loss: 1.1550324205547342\n",
            "Epoch 1, Loss: 1.1571696522595631\n",
            "Epoch 1, Loss: 1.1591919139218148\n",
            "Epoch 1, Loss: 1.1613075126467458\n",
            "Epoch 1, Loss: 1.163073600558064\n",
            "Epoch 1, Loss: 1.1652522802048022\n",
            "Epoch 1, Loss: 1.1672997171311732\n",
            "Epoch 1, Loss: 1.1692160858827478\n",
            "Epoch 1, Loss: 1.1713133631155008\n",
            "Epoch 1, Loss: 1.1732931996855285\n",
            "Epoch 1, Loss: 1.1752163485797775\n",
            "Epoch 1, Loss: 1.1773831184257937\n",
            "Epoch 1, Loss: 1.1794647798513818\n",
            "Epoch 1, Loss: 1.1815083138168316\n",
            "Epoch 1, Loss: 1.1834932542822856\n",
            "Epoch 1, Loss: 1.1855556754504932\n",
            "Epoch 1, Loss: 1.1874439536458086\n",
            "Epoch 1, Loss: 1.1897142826748626\n",
            "Epoch 1, Loss: 1.191724463039652\n",
            "Epoch 1, Loss: 1.19381142485782\n",
            "Epoch 1, Loss: 1.1960401056367722\n",
            "Epoch 1, Loss: 1.1985009088540626\n",
            "Epoch 1, Loss: 1.2007401798997084\n",
            "Epoch 1, Loss: 1.2028268106148372\n",
            "Epoch 1, Loss: 1.2046327754054837\n",
            "Epoch 1, Loss: 1.2065062373495468\n",
            "Epoch 1, Loss: 1.2085532908854277\n",
            "Epoch 1, Loss: 1.2106597560750858\n",
            "Epoch 1, Loss: 1.21296472089065\n",
            "Epoch 1, Loss: 1.2149391111815373\n",
            "Epoch 1, Loss: 1.2169576528127237\n",
            "Epoch 1, Loss: 1.218891030534759\n",
            "Epoch 1, Loss: 1.2210022418395332\n",
            "Epoch 1, Loss: 1.2228793367705382\n",
            "Epoch 1, Loss: 1.2249574764915134\n",
            "Epoch 1, Loss: 1.2269137474277136\n",
            "Epoch 1, Loss: 1.2289206390185734\n",
            "Epoch 1, Loss: 1.23096343150834\n",
            "Epoch 1, Loss: 1.2328427616897446\n",
            "Epoch 1, Loss: 1.2345632163764875\n",
            "Epoch 1, Loss: 1.236607474591726\n",
            "Epoch 1, Loss: 1.2386482489078552\n",
            "Epoch 1, Loss: 1.2408150321687275\n",
            "Epoch 1, Loss: 1.2429163718162595\n",
            "Epoch 1, Loss: 1.2449898898144207\n",
            "Epoch 1, Loss: 1.2466967223245469\n",
            "Epoch 1, Loss: 1.2486970692949222\n",
            "Epoch 1, Loss: 1.2508479567135082\n",
            "Epoch 1, Loss: 1.25296892092356\n",
            "Epoch 1, Loss: 1.2552064334035224\n",
            "Epoch 1, Loss: 1.2571413076442222\n",
            "Epoch 1, Loss: 1.2593403586646175\n",
            "Epoch 1, Loss: 1.2612929717659036\n",
            "Epoch 1, Loss: 1.2632655789480185\n",
            "Epoch 1, Loss: 1.2657013258055958\n",
            "Epoch 1, Loss: 1.2677501668710538\n",
            "Epoch 1, Loss: 1.269608453868905\n",
            "Epoch 1, Loss: 1.271407693090951\n",
            "Epoch 1, Loss: 1.2734349127620688\n",
            "Epoch 1, Loss: 1.2755022215111482\n",
            "Epoch 1, Loss: 1.2776283829108528\n",
            "Epoch 1, Loss: 1.2798094842440026\n",
            "Epoch 1, Loss: 1.2819824464180891\n",
            "Epoch 1, Loss: 1.2838820546789242\n",
            "Epoch 1, Loss: 1.2858922310802332\n",
            "Epoch 1, Loss: 1.2880631049575708\n",
            "Epoch 1, Loss: 1.2901363808785558\n",
            "Epoch 1, Loss: 1.29197016305021\n",
            "Epoch 1, Loss: 1.2938792389981888\n",
            "Epoch 1, Loss: 1.2957663144297002\n",
            "Epoch 1, Loss: 1.2979726709063402\n",
            "Epoch 1, Loss: 1.299991117871326\n",
            "Epoch 1, Loss: 1.3021655215326782\n",
            "Epoch 1, Loss: 1.3040558532680697\n",
            "Epoch 1, Loss: 1.3062833991197065\n",
            "Epoch 1, Loss: 1.3082597258755617\n",
            "Epoch 1, Loss: 1.310386092918913\n",
            "Epoch 1, Loss: 1.31231948359848\n",
            "Epoch 1, Loss: 1.314601738891943\n",
            "Epoch 1, Loss: 1.3164928121030177\n",
            "Epoch 1, Loss: 1.3185066111252437\n",
            "Epoch 1, Loss: 1.3205526926938225\n",
            "Epoch 1, Loss: 1.3224364616681852\n",
            "Epoch 1, Loss: 1.3242173306167584\n",
            "Epoch 1, Loss: 1.3263201208980493\n",
            "Epoch 1, Loss: 1.3285302285038296\n",
            "Epoch 1, Loss: 1.3303390532503347\n",
            "Epoch 1, Loss: 1.3323136383615186\n",
            "Epoch 1, Loss: 1.3341653063474104\n",
            "Epoch 1, Loss: 1.3363794746911128\n",
            "Epoch 1, Loss: 1.3381669292669467\n",
            "Epoch 1, Loss: 1.339959950093418\n",
            "Epoch 1, Loss: 1.342016085029563\n",
            "Epoch 1, Loss: 1.3440031788843063\n",
            "Epoch 1, Loss: 1.3458885256286777\n",
            "Epoch 1, Loss: 1.347869162059501\n",
            "Epoch 1, Loss: 1.3501501572711387\n",
            "Epoch 1, Loss: 1.3520088851299432\n",
            "Epoch 1, Loss: 1.3542588493403267\n",
            "Epoch 1, Loss: 1.3562138207123409\n",
            "Epoch 1, Loss: 1.3582223879406825\n",
            "Epoch 1, Loss: 1.3600664294284324\n",
            "Epoch 1, Loss: 1.3619918599153114\n",
            "Epoch 1, Loss: 1.363881610086202\n",
            "Epoch 1, Loss: 1.3659508487452632\n",
            "Epoch 1, Loss: 1.3680823994109699\n",
            "Epoch 1, Loss: 1.3702241857643322\n",
            "Epoch 1, Loss: 1.372098163723031\n",
            "Epoch 1, Loss: 1.3740831651651035\n",
            "Epoch 1, Loss: 1.3759064127112288\n",
            "Epoch 1, Loss: 1.3779378431227507\n",
            "Epoch 1, Loss: 1.3801143844719128\n",
            "Epoch 1, Loss: 1.3820412657449923\n",
            "Epoch 1, Loss: 1.3842561023924358\n",
            "Epoch 1, Loss: 1.3861717827179854\n",
            "Epoch 1, Loss: 1.3880582508223747\n",
            "Epoch 1, Loss: 1.389926768629752\n",
            "Epoch 1, Loss: 1.3915781736983668\n",
            "Epoch 1, Loss: 1.3934906311047353\n",
            "Epoch 1, Loss: 1.3954398211310892\n",
            "Epoch 1, Loss: 1.397451351975541\n",
            "Epoch 1, Loss: 1.3996808452679372\n",
            "Epoch 1, Loss: 1.4016403199156837\n",
            "Epoch 1, Loss: 1.4037484223275538\n",
            "Epoch 1, Loss: 1.4055901160630424\n",
            "Epoch 1, Loss: 1.4076081636311757\n",
            "Epoch 1, Loss: 1.4096330774714574\n",
            "Epoch 1, Loss: 1.4118308307569656\n",
            "Epoch 1, Loss: 1.4138942459964996\n",
            "Epoch 1, Loss: 1.4157478864235646\n",
            "Epoch 1, Loss: 1.4174455472880312\n",
            "Epoch 1, Loss: 1.419510811033761\n",
            "Epoch 1, Loss: 1.4213356252216622\n",
            "Epoch 1, Loss: 1.4235063323279475\n",
            "Epoch 1, Loss: 1.4254620201752315\n",
            "Epoch 1, Loss: 1.4275509071776935\n",
            "Epoch 1, Loss: 1.429519479232066\n",
            "Epoch 1, Loss: 1.43158352817111\n",
            "Epoch 1, Loss: 1.4335287202654592\n",
            "Epoch 1, Loss: 1.435410546982075\n",
            "Epoch 1, Loss: 1.43727343497069\n",
            "Epoch 1, Loss: 1.4391866026022244\n",
            "Epoch 1, Loss: 1.4409142366760528\n",
            "Epoch 1, Loss: 1.4426639413894595\n",
            "Epoch 1, Loss: 1.4446218242425748\n",
            "Epoch 1, Loss: 1.4466585245583674\n",
            "Epoch 1, Loss: 1.4486026395007472\n",
            "Epoch 1, Loss: 1.4505278038246858\n",
            "Epoch 1, Loss: 1.4524700390103529\n",
            "Epoch 1, Loss: 1.4544868249722454\n",
            "Epoch 1, Loss: 1.4566082218114067\n",
            "Epoch 1, Loss: 1.458424344392079\n",
            "Epoch 1, Loss: 1.4603933106602915\n",
            "Epoch 1, Loss: 1.4622757768691959\n",
            "Epoch 1, Loss: 1.464457219823852\n",
            "Epoch 1, Loss: 1.4663181899453672\n",
            "Epoch 1, Loss: 1.4681475353058038\n",
            "Epoch 1, Loss: 1.470187468449478\n",
            "Epoch 1, Loss: 1.471839001111667\n",
            "Epoch 1, Loss: 1.4737113127318184\n",
            "Epoch 1, Loss: 1.4755404457411803\n",
            "Epoch 1, Loss: 1.4774703766074022\n",
            "Epoch 1, Loss: 1.4793774877362849\n",
            "Epoch 1, Loss: 1.4814003887383833\n",
            "Epoch 1, Loss: 1.483528345137301\n",
            "Epoch 1, Loss: 1.4854545449966665\n",
            "Epoch 1, Loss: 1.4874220961500006\n",
            "Epoch 1, Loss: 1.4895676985726027\n",
            "Epoch 1, Loss: 1.4916018828406663\n",
            "Epoch 1, Loss: 1.4936302455185015\n",
            "Epoch 1, Loss: 1.495665652977536\n",
            "Epoch 1, Loss: 1.497513499253851\n",
            "Epoch 1, Loss: 1.4994431075537602\n",
            "Epoch 1, Loss: 1.5013650055126766\n",
            "Epoch 1, Loss: 1.5035269810720477\n",
            "Epoch 1, Loss: 1.5054769360500833\n",
            "Epoch 1, Loss: 1.507406130776076\n",
            "Epoch 1, Loss: 1.5092106854824154\n",
            "Epoch 1, Loss: 1.5114801106855387\n",
            "Epoch 1, Loss: 1.513310201027814\n",
            "Epoch 1, Loss: 1.5153603451636137\n",
            "Epoch 1, Loss: 1.5171853599645901\n",
            "Epoch 1, Loss: 1.5192969287447917\n",
            "Epoch 1, Loss: 1.5214274382347341\n",
            "Epoch 1, Loss: 1.523392622428172\n",
            "Epoch 1, Loss: 1.5250754694804511\n",
            "Epoch 1, Loss: 1.5271406297183707\n",
            "Epoch 1, Loss: 1.5288061728257962\n",
            "Epoch 1, Loss: 1.530715480789809\n",
            "Epoch 1, Loss: 1.5325922555935658\n",
            "Epoch 1, Loss: 1.5344827489169968\n",
            "Epoch 1, Loss: 1.5365290405500271\n",
            "Epoch 1, Loss: 1.5383179395095161\n",
            "Epoch 1, Loss: 1.5402241538247794\n",
            "Epoch 1, Loss: 1.542237743697203\n",
            "Epoch 1, Loss: 1.5440270202544035\n",
            "Epoch 1, Loss: 1.5459764735473087\n",
            "Epoch 1, Loss: 1.547998054558054\n",
            "Epoch 1, Loss: 1.5499303516219645\n",
            "Epoch 1, Loss: 1.5516910828897714\n",
            "Epoch 1, Loss: 1.5535429174942739\n",
            "Epoch 1, Loss: 1.5556900146062418\n",
            "Epoch 1, Loss: 1.5576236322712715\n",
            "Epoch 1, Loss: 1.5597489672853513\n",
            "Epoch 1, Loss: 1.5616404254113316\n",
            "Epoch 1, Loss: 1.563395380668933\n",
            "Epoch 1, Loss: 1.5651961066533842\n",
            "Epoch 1, Loss: 1.5669444803996464\n",
            "Epoch 1, Loss: 1.5687036209399132\n",
            "Epoch 1, Loss: 1.5706833980577377\n",
            "Epoch 1, Loss: 1.5726429338345442\n",
            "Epoch 1, Loss: 1.5744996535808533\n",
            "Epoch 1, Loss: 1.576363032736132\n",
            "Epoch 1, Loss: 1.5783638268175637\n",
            "Epoch 1, Loss: 1.5803416493298756\n",
            "Epoch 1, Loss: 1.581997719262262\n",
            "Epoch 1, Loss: 1.5839253224985068\n",
            "Epoch 1, Loss: 1.5856747547988697\n",
            "Epoch 1, Loss: 1.5879118422718\n",
            "Epoch 1, Loss: 1.589562543975118\n",
            "Epoch 1, Loss: 1.5916200585072608\n",
            "Epoch 1, Loss: 1.593581687306504\n",
            "Epoch 1, Loss: 1.5956071866747668\n",
            "Epoch 1, Loss: 1.5976248444498653\n",
            "Epoch 1, Loss: 1.59953904792171\n",
            "Epoch 1, Loss: 1.6012303893218565\n",
            "Epoch 1, Loss: 1.6031541786230434\n",
            "Epoch 1, Loss: 1.605062112783837\n",
            "Epoch 1, Loss: 1.606571973284797\n",
            "Epoch 1, Loss: 1.6085576468416491\n",
            "Epoch 1, Loss: 1.6104127712871716\n",
            "Epoch 1, Loss: 1.6123903980645378\n",
            "Epoch 1, Loss: 1.6143966785172368\n",
            "Epoch 1, Loss: 1.616231592414934\n",
            "Epoch 1, Loss: 1.6182065429285055\n",
            "Epoch 1, Loss: 1.6201416266238904\n",
            "Epoch 1, Loss: 1.621860914675476\n",
            "Epoch 1, Loss: 1.6238451839407997\n",
            "Epoch 1, Loss: 1.6257383454486232\n",
            "Epoch 1, Loss: 1.6274700032170777\n",
            "Epoch 1, Loss: 1.6296358017055579\n",
            "Epoch 1, Loss: 1.631921327784848\n",
            "Epoch 1, Loss: 1.6340798000850336\n",
            "Epoch 1, Loss: 1.6358016445813581\n",
            "Epoch 1, Loss: 1.63783123395632\n",
            "Epoch 1, Loss: 1.6396248933604307\n",
            "Epoch 1, Loss: 1.6415416288863667\n",
            "Epoch 1, Loss: 1.643774764921964\n",
            "Epoch 1, Loss: 1.6457823891468975\n",
            "Epoch 1, Loss: 1.6480152856968249\n",
            "Epoch 1, Loss: 1.6499854446676991\n",
            "Epoch 1, Loss: 1.6519498514092488\n",
            "Epoch 1, Loss: 1.6538121742970497\n",
            "Epoch 1, Loss: 1.6555394213217909\n",
            "Epoch 1, Loss: 1.6574193095916983\n",
            "Epoch 1, Loss: 1.6592551129858206\n",
            "Epoch 1, Loss: 1.6608967300876023\n",
            "Epoch 1, Loss: 1.6627690165548983\n",
            "Epoch 1, Loss: 1.6647046821196672\n",
            "Epoch 1, Loss: 1.6666588982962587\n",
            "Epoch 1, Loss: 1.6685208623366587\n",
            "Epoch 1, Loss: 1.670615240131193\n",
            "Epoch 1, Loss: 1.6726177232649626\n",
            "Epoch 1, Loss: 1.6743394212649607\n",
            "Epoch 1, Loss: 1.6761735813392094\n",
            "Epoch 1, Loss: 1.6781357503912944\n",
            "Epoch 1, Loss: 1.6797570734072829\n",
            "Epoch 1, Loss: 1.6815400271464491\n",
            "Epoch 1, Loss: 1.6836506713686696\n",
            "Epoch 1, Loss: 1.6854415508487341\n",
            "Epoch 1, Loss: 1.687238407561846\n",
            "Epoch 1, Loss: 1.6891160343614076\n",
            "Epoch 1, Loss: 1.6910056613595283\n",
            "Epoch 1, Loss: 1.692961591131547\n",
            "Epoch 1, Loss: 1.694780811934215\n",
            "Epoch 1, Loss: 1.6968109048236057\n",
            "Epoch 1, Loss: 1.698729097233404\n",
            "Epoch 1, Loss: 1.7005419856142205\n",
            "Epoch 1, Loss: 1.7026701766206784\n",
            "Epoch 1, Loss: 1.7045172073347183\n",
            "Epoch 1, Loss: 1.7062633997948884\n",
            "Epoch 2, Loss: 0.0019007319074762447\n",
            "Epoch 2, Loss: 0.0036730709893014425\n",
            "Epoch 2, Loss: 0.005456128083836392\n",
            "Epoch 2, Loss: 0.0072989791555477836\n",
            "Epoch 2, Loss: 0.009135117158865379\n",
            "Epoch 2, Loss: 0.010934252568218105\n",
            "Epoch 2, Loss: 0.012843336443157147\n",
            "Epoch 2, Loss: 0.014946137547797864\n",
            "Epoch 2, Loss: 0.017071080634661038\n",
            "Epoch 2, Loss: 0.01907503985992783\n",
            "Epoch 2, Loss: 0.020938223432701873\n",
            "Epoch 2, Loss: 0.022925721409985476\n",
            "Epoch 2, Loss: 0.025123258838263315\n",
            "Epoch 2, Loss: 0.026964708362393976\n",
            "Epoch 2, Loss: 0.02903802605236278\n",
            "Epoch 2, Loss: 0.03131866638007981\n",
            "Epoch 2, Loss: 0.03348434337264741\n",
            "Epoch 2, Loss: 0.03542634074950157\n",
            "Epoch 2, Loss: 0.0371923559462018\n",
            "Epoch 2, Loss: 0.0387102830440492\n",
            "Epoch 2, Loss: 0.040857867664083496\n",
            "Epoch 2, Loss: 0.042757188115278476\n",
            "Epoch 2, Loss: 0.044601590279727946\n",
            "Epoch 2, Loss: 0.04655640463694892\n",
            "Epoch 2, Loss: 0.048289454501608146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# Replace the existing device check and set\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Load CIFAR-10 dataset with standard normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Define Residual Block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.stride != 1 or identity.shape[1] != out.shape[1]:\n",
        "            identity = self.conv1(identity)\n",
        "            identity = self.bn1(identity)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Define ResNet-10\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(ResNet10, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels, stride=1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train_resnet_batch_norm(model, criterion, optimizer, train_loader, num_epochs=300, device='cpu'):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    start_time = time.time()\n",
        "    all_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        num = epoch + 1\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            all_losses.append(running_loss / len(train_loader))\n",
        "            print(f'Epoch {num}, Loss: {all_losses[-1]}')\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    print(f'Training Time: {training_time} seconds')\n",
        "    return all_losses\n",
        "\n",
        "# Instantiate ResNet-10 and set up optimizer and criterion\n",
        "resnet_model_batch_norm = ResNet10(ResidualBlock, [1, 1, 1, 1])\n",
        "criterion_resnet = nn.CrossEntropyLoss()\n",
        "optimizer_resnet_batch_norm = optim.SGD(resnet_model_batch_norm.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train ResNet-10 with batch normalization\n",
        "train_resnet_batch_norm(resnet_model_batch_norm, criterion_resnet, optimizer_resnet_batch_norm, train_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LqEdc6rD55OO",
        "outputId": "ce7a09ca-8709-439f-f53c-de195de279f7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 0.0030564030113122654\n",
            "Epoch 1, Loss: 0.006058505733909509\n",
            "Epoch 1, Loss: 0.009084796966494196\n",
            "Epoch 1, Loss: 0.01199699728690145\n",
            "Epoch 1, Loss: 0.01498146953485201\n",
            "Epoch 1, Loss: 0.0179373635660352\n",
            "Epoch 1, Loss: 0.020882867486275675\n",
            "Epoch 1, Loss: 0.02372193366975126\n",
            "Epoch 1, Loss: 0.026581239212504434\n",
            "Epoch 1, Loss: 0.02944641954758588\n",
            "Epoch 1, Loss: 0.03232048966390703\n",
            "Epoch 1, Loss: 0.03515490577044084\n",
            "Epoch 1, Loss: 0.038001936414967415\n",
            "Epoch 1, Loss: 0.0408029980061914\n",
            "Epoch 1, Loss: 0.04360861881919529\n",
            "Epoch 1, Loss: 0.04638413120718563\n",
            "Epoch 1, Loss: 0.04917191087132525\n",
            "Epoch 1, Loss: 0.05200240435197835\n",
            "Epoch 1, Loss: 0.054790925491801305\n",
            "Epoch 1, Loss: 0.05748368620567614\n",
            "Epoch 1, Loss: 0.060196305174961724\n",
            "Epoch 1, Loss: 0.06303510245154886\n",
            "Epoch 1, Loss: 0.0657043085073876\n",
            "Epoch 1, Loss: 0.06830515641995404\n",
            "Epoch 1, Loss: 0.07098352116392091\n",
            "Epoch 1, Loss: 0.07363279396310791\n",
            "Epoch 1, Loss: 0.07630804341162561\n",
            "Epoch 1, Loss: 0.0789356990848356\n",
            "Epoch 1, Loss: 0.08154367242017975\n",
            "Epoch 1, Loss: 0.08410272634852572\n",
            "Epoch 1, Loss: 0.08681438950931325\n",
            "Epoch 1, Loss: 0.08954837803950395\n",
            "Epoch 1, Loss: 0.09220626378608177\n",
            "Epoch 1, Loss: 0.09472446917268017\n",
            "Epoch 1, Loss: 0.09715861539401667\n",
            "Epoch 1, Loss: 0.09982498420778747\n",
            "Epoch 1, Loss: 0.10239789720691378\n",
            "Epoch 1, Loss: 0.10493943544909777\n",
            "Epoch 1, Loss: 0.10731880439211951\n",
            "Epoch 1, Loss: 0.10987419865625289\n",
            "Epoch 1, Loss: 0.11233508403953689\n",
            "Epoch 1, Loss: 0.11471612618097564\n",
            "Epoch 1, Loss: 0.11713616210786278\n",
            "Epoch 1, Loss: 0.11964538106528085\n",
            "Epoch 1, Loss: 0.12222427922441527\n",
            "Epoch 1, Loss: 0.1247711719759285\n",
            "Epoch 1, Loss: 0.12739710262059556\n",
            "Epoch 1, Loss: 0.1299419342099553\n",
            "Epoch 1, Loss: 0.13241374523133573\n",
            "Epoch 1, Loss: 0.13477002415815584\n",
            "Epoch 1, Loss: 0.13717567234697853\n",
            "Epoch 1, Loss: 0.13952275569481618\n",
            "Epoch 1, Loss: 0.14181488310284626\n",
            "Epoch 1, Loss: 0.14435690504205806\n",
            "Epoch 1, Loss: 0.14677419915528553\n",
            "Epoch 1, Loss: 0.14929357407343052\n",
            "Epoch 1, Loss: 0.15182577420378585\n",
            "Epoch 1, Loss: 0.15421660248275912\n",
            "Epoch 1, Loss: 0.15645690159419615\n",
            "Epoch 1, Loss: 0.1588063988539264\n",
            "Epoch 1, Loss: 0.16106568959058093\n",
            "Epoch 1, Loss: 0.1634765898480135\n",
            "Epoch 1, Loss: 0.165678628722725\n",
            "Epoch 1, Loss: 0.1679160599513432\n",
            "Epoch 1, Loss: 0.1701161832455784\n",
            "Epoch 1, Loss: 0.17229013293600448\n",
            "Epoch 1, Loss: 0.17453640790851524\n",
            "Epoch 1, Loss: 0.17674105917401325\n",
            "Epoch 1, Loss: 0.17915609318886877\n",
            "Epoch 1, Loss: 0.18152803731391498\n",
            "Epoch 1, Loss: 0.18379446566867097\n",
            "Epoch 1, Loss: 0.18597315446190213\n",
            "Epoch 1, Loss: 0.18830223705457605\n",
            "Epoch 1, Loss: 0.190517750542487\n",
            "Epoch 1, Loss: 0.19279365024298353\n",
            "Epoch 1, Loss: 0.19502216181181886\n",
            "Epoch 1, Loss: 0.19714161563102547\n",
            "Epoch 1, Loss: 0.19923068647799286\n",
            "Epoch 1, Loss: 0.20164378677182795\n",
            "Epoch 1, Loss: 0.203981853507059\n",
            "Epoch 1, Loss: 0.20616358107008287\n",
            "Epoch 1, Loss: 0.20851519635266355\n",
            "Epoch 1, Loss: 0.2106443730461628\n",
            "Epoch 1, Loss: 0.21293189671948132\n",
            "Epoch 1, Loss: 0.21490859589003541\n",
            "Epoch 1, Loss: 0.21732967360245298\n",
            "Epoch 1, Loss: 0.21946515527832539\n",
            "Epoch 1, Loss: 0.2216792201142177\n",
            "Epoch 1, Loss: 0.2237329888526741\n",
            "Epoch 1, Loss: 0.22613607632839466\n",
            "Epoch 1, Loss: 0.22847839023755945\n",
            "Epoch 1, Loss: 0.23077562138857438\n",
            "Epoch 1, Loss: 0.23315868841107848\n",
            "Epoch 1, Loss: 0.23528130081913357\n",
            "Epoch 1, Loss: 0.23734118231117268\n",
            "Epoch 1, Loss: 0.2395786557660993\n",
            "Epoch 1, Loss: 0.24165077785701702\n",
            "Epoch 1, Loss: 0.24361268409987544\n",
            "Epoch 1, Loss: 0.24595152798211178\n",
            "Epoch 1, Loss: 0.2484199990092031\n",
            "Epoch 1, Loss: 0.2506239309030421\n",
            "Epoch 1, Loss: 0.25273059975460666\n",
            "Epoch 1, Loss: 0.2548709257179514\n",
            "Epoch 1, Loss: 0.25678440250094287\n",
            "Epoch 1, Loss: 0.25898155364234127\n",
            "Epoch 1, Loss: 0.2610051374301276\n",
            "Epoch 1, Loss: 0.26328865402494855\n",
            "Epoch 1, Loss: 0.26549317190409316\n",
            "Epoch 1, Loss: 0.2675118917394477\n",
            "Epoch 1, Loss: 0.26967145689308186\n",
            "Epoch 1, Loss: 0.27158727655020515\n",
            "Epoch 1, Loss: 0.2736228239505797\n",
            "Epoch 1, Loss: 0.27596842526169996\n",
            "Epoch 1, Loss: 0.2781952980839078\n",
            "Epoch 1, Loss: 0.28054617814090854\n",
            "Epoch 1, Loss: 0.2826689307951866\n",
            "Epoch 1, Loss: 0.2846274958242236\n",
            "Epoch 1, Loss: 0.2869203098289802\n",
            "Epoch 1, Loss: 0.28875593105545433\n",
            "Epoch 1, Loss: 0.29091226856421937\n",
            "Epoch 1, Loss: 0.29309644281406844\n",
            "Epoch 1, Loss: 0.29531724556632666\n",
            "Epoch 1, Loss: 0.29730141071407384\n",
            "Epoch 1, Loss: 0.2995843666288859\n",
            "Epoch 1, Loss: 0.3016929079199691\n",
            "Epoch 1, Loss: 0.30398859956380353\n",
            "Epoch 1, Loss: 0.30611615107797296\n",
            "Epoch 1, Loss: 0.30823973255694065\n",
            "Epoch 1, Loss: 0.31030281898005846\n",
            "Epoch 1, Loss: 0.31256085588499105\n",
            "Epoch 1, Loss: 0.3145753753459667\n",
            "Epoch 1, Loss: 0.3164508646101598\n",
            "Epoch 1, Loss: 0.318490627933951\n",
            "Epoch 1, Loss: 0.32051712503213714\n",
            "Epoch 1, Loss: 0.32259367143406587\n",
            "Epoch 1, Loss: 0.32459997048463357\n",
            "Epoch 1, Loss: 0.3267828523350494\n",
            "Epoch 1, Loss: 0.3287922971693756\n",
            "Epoch 1, Loss: 0.3309504162624974\n",
            "Epoch 1, Loss: 0.332852259926174\n",
            "Epoch 1, Loss: 0.3349298683883589\n",
            "Epoch 1, Loss: 0.3368955693586403\n",
            "Epoch 1, Loss: 0.3391105743015514\n",
            "Epoch 1, Loss: 0.34141026570668914\n",
            "Epoch 1, Loss: 0.34345937949007427\n",
            "Epoch 1, Loss: 0.34574295904325403\n",
            "Epoch 1, Loss: 0.3478527369401644\n",
            "Epoch 1, Loss: 0.3499968553443089\n",
            "Epoch 1, Loss: 0.35202807142301595\n",
            "Epoch 1, Loss: 0.3539632802729107\n",
            "Epoch 1, Loss: 0.35606762439088746\n",
            "Epoch 1, Loss: 0.3583484857588473\n",
            "Epoch 1, Loss: 0.3605541353640349\n",
            "Epoch 1, Loss: 0.3628801293385303\n",
            "Epoch 1, Loss: 0.3650096793613775\n",
            "Epoch 1, Loss: 0.3673505461429391\n",
            "Epoch 1, Loss: 0.36918198742220165\n",
            "Epoch 1, Loss: 0.3711425751981223\n",
            "Epoch 1, Loss: 0.3731999007027472\n",
            "Epoch 1, Loss: 0.3750888264697531\n",
            "Epoch 1, Loss: 0.37707343766146606\n",
            "Epoch 1, Loss: 0.3789223385284014\n",
            "Epoch 1, Loss: 0.3808530180350594\n",
            "Epoch 1, Loss: 0.38289166716358547\n",
            "Epoch 1, Loss: 0.38476341642687084\n",
            "Epoch 1, Loss: 0.3868618741669618\n",
            "Epoch 1, Loss: 0.389041523494379\n",
            "Epoch 1, Loss: 0.3910194141480624\n",
            "Epoch 1, Loss: 0.3929849259383843\n",
            "Epoch 1, Loss: 0.39495098804268997\n",
            "Epoch 1, Loss: 0.3968980281859103\n",
            "Epoch 1, Loss: 0.3990956233895343\n",
            "Epoch 1, Loss: 0.4010099442413701\n",
            "Epoch 1, Loss: 0.4030425275683098\n",
            "Epoch 1, Loss: 0.4050700137072512\n",
            "Epoch 1, Loss: 0.40702652992189997\n",
            "Epoch 1, Loss: 0.40935678006438037\n",
            "Epoch 1, Loss: 0.4113790220616724\n",
            "Epoch 1, Loss: 0.4132669200677701\n",
            "Epoch 1, Loss: 0.4153059704224472\n",
            "Epoch 1, Loss: 0.4173566494756343\n",
            "Epoch 1, Loss: 0.4194237533432748\n",
            "Epoch 1, Loss: 0.42146035411473737\n",
            "Epoch 1, Loss: 0.42364344435274753\n",
            "Epoch 1, Loss: 0.42585601358462477\n",
            "Epoch 1, Loss: 0.4279080228427487\n",
            "Epoch 1, Loss: 0.4298416644411014\n",
            "Epoch 1, Loss: 0.4319216538878048\n",
            "Epoch 1, Loss: 0.43374511652895253\n",
            "Epoch 1, Loss: 0.43584247943385485\n",
            "Epoch 1, Loss: 0.4379741895534193\n",
            "Epoch 1, Loss: 0.43998115980411734\n",
            "Epoch 1, Loss: 0.4419733717313508\n",
            "Epoch 1, Loss: 0.4438009820021022\n",
            "Epoch 1, Loss: 0.44589478478712197\n",
            "Epoch 1, Loss: 0.44787107038375973\n",
            "Epoch 1, Loss: 0.45015471823075237\n",
            "Epoch 1, Loss: 0.45195954016712314\n",
            "Epoch 1, Loss: 0.4541016658553687\n",
            "Epoch 1, Loss: 0.4562203714914639\n",
            "Epoch 1, Loss: 0.4580230943077361\n",
            "Epoch 1, Loss: 0.46011901099968444\n",
            "Epoch 1, Loss: 0.46207927864835696\n",
            "Epoch 1, Loss: 0.4640537703128727\n",
            "Epoch 1, Loss: 0.46608030140552376\n",
            "Epoch 1, Loss: 0.46816756444818836\n",
            "Epoch 1, Loss: 0.47050357932020026\n",
            "Epoch 1, Loss: 0.4727731258667948\n",
            "Epoch 1, Loss: 0.4746102213554675\n",
            "Epoch 1, Loss: 0.4766599967351655\n",
            "Epoch 1, Loss: 0.47869450402686664\n",
            "Epoch 1, Loss: 0.4806425351925823\n",
            "Epoch 1, Loss: 0.48252035040989555\n",
            "Epoch 1, Loss: 0.4843746977084128\n",
            "Epoch 1, Loss: 0.48610045461703444\n",
            "Epoch 1, Loss: 0.48830006052466\n",
            "Epoch 1, Loss: 0.4901679368580089\n",
            "Epoch 1, Loss: 0.49221008192852633\n",
            "Epoch 1, Loss: 0.4941688794309221\n",
            "Epoch 1, Loss: 0.4962282168590809\n",
            "Epoch 1, Loss: 0.49818584025668367\n",
            "Epoch 1, Loss: 0.5002367754116692\n",
            "Epoch 1, Loss: 0.5020895659771112\n",
            "Epoch 1, Loss: 0.5039779955468824\n",
            "Epoch 1, Loss: 0.506005453026813\n",
            "Epoch 1, Loss: 0.5082817946553535\n",
            "Epoch 1, Loss: 0.5101197364994937\n",
            "Epoch 1, Loss: 0.5119526119488279\n",
            "Epoch 1, Loss: 0.5139943094509641\n",
            "Epoch 1, Loss: 0.5160910014606193\n",
            "Epoch 1, Loss: 0.5180457780123366\n",
            "Epoch 1, Loss: 0.5198431556182139\n",
            "Epoch 1, Loss: 0.5218483028204545\n",
            "Epoch 1, Loss: 0.5238953001054046\n",
            "Epoch 1, Loss: 0.5258419612789398\n",
            "Epoch 1, Loss: 0.5278884352320601\n",
            "Epoch 1, Loss: 0.5297400428510993\n",
            "Epoch 1, Loss: 0.5315614644523776\n",
            "Epoch 1, Loss: 0.5334869868615094\n",
            "Epoch 1, Loss: 0.5355698946491837\n",
            "Epoch 1, Loss: 0.5375795908596205\n",
            "Epoch 1, Loss: 0.5395795305061828\n",
            "Epoch 1, Loss: 0.5415608674059134\n",
            "Epoch 1, Loss: 0.5435338602651416\n",
            "Epoch 1, Loss: 0.545438743003494\n",
            "Epoch 1, Loss: 0.5473506036012069\n",
            "Epoch 1, Loss: 0.5491822616523488\n",
            "Epoch 1, Loss: 0.5513711611328223\n",
            "Epoch 1, Loss: 0.5533696306331078\n",
            "Epoch 1, Loss: 0.5552098787654086\n",
            "Epoch 1, Loss: 0.5572017627908751\n",
            "Epoch 1, Loss: 0.5590729768318898\n",
            "Epoch 1, Loss: 0.5607601991090019\n",
            "Epoch 1, Loss: 0.5628092074028367\n",
            "Epoch 1, Loss: 0.5648540548046531\n",
            "Epoch 1, Loss: 0.5667977243128335\n",
            "Epoch 1, Loss: 0.5688475887183948\n",
            "Epoch 1, Loss: 0.5706887428108078\n",
            "Epoch 1, Loss: 0.5725009122772899\n",
            "Epoch 1, Loss: 0.5744802031065802\n",
            "Epoch 1, Loss: 0.5764786995890195\n",
            "Epoch 1, Loss: 0.5784397311222828\n",
            "Epoch 1, Loss: 0.580326737345332\n",
            "Epoch 1, Loss: 0.5821618055138746\n",
            "Epoch 1, Loss: 0.5840759512103731\n",
            "Epoch 1, Loss: 0.585909264014505\n",
            "Epoch 1, Loss: 0.5878832236580227\n",
            "Epoch 1, Loss: 0.5896190872887517\n",
            "Epoch 1, Loss: 0.5915595235117256\n",
            "Epoch 1, Loss: 0.5934384570402258\n",
            "Epoch 1, Loss: 0.5953251695084145\n",
            "Epoch 1, Loss: 0.5971008267853876\n",
            "Epoch 1, Loss: 0.5989938042962643\n",
            "Epoch 1, Loss: 0.6011390114379356\n",
            "Epoch 1, Loss: 0.6030004928483987\n",
            "Epoch 1, Loss: 0.6048663193002686\n",
            "Epoch 1, Loss: 0.607158388780511\n",
            "Epoch 1, Loss: 0.6088283600099861\n",
            "Epoch 1, Loss: 0.6108832711453938\n",
            "Epoch 1, Loss: 0.6130014276870376\n",
            "Epoch 1, Loss: 0.6149469200912339\n",
            "Epoch 1, Loss: 0.616965320256665\n",
            "Epoch 1, Loss: 0.618928631095935\n",
            "Epoch 1, Loss: 0.6209416256841186\n",
            "Epoch 1, Loss: 0.6226861688791944\n",
            "Epoch 1, Loss: 0.6245422050776079\n",
            "Epoch 1, Loss: 0.6265916652081872\n",
            "Epoch 1, Loss: 0.6284307849681591\n",
            "Epoch 1, Loss: 0.6301633371111682\n",
            "Epoch 1, Loss: 0.6319097567092428\n",
            "Epoch 1, Loss: 0.6338165842968485\n",
            "Epoch 1, Loss: 0.6357121481310071\n",
            "Epoch 1, Loss: 0.6376035033589433\n",
            "Epoch 1, Loss: 0.6394042983994155\n",
            "Epoch 1, Loss: 0.6417487620392723\n",
            "Epoch 1, Loss: 0.6435296059874318\n",
            "Epoch 1, Loss: 0.6454343510710675\n",
            "Epoch 1, Loss: 0.6473350678868306\n",
            "Epoch 1, Loss: 0.6490609813529207\n",
            "Epoch 1, Loss: 0.650944132329253\n",
            "Epoch 1, Loss: 0.6527946179785082\n",
            "Epoch 1, Loss: 0.6547432948866159\n",
            "Epoch 1, Loss: 0.6567750812491493\n",
            "Epoch 1, Loss: 0.6588492329468203\n",
            "Epoch 1, Loss: 0.6606810739278184\n",
            "Epoch 1, Loss: 0.6624822659260782\n",
            "Epoch 1, Loss: 0.6643410855546936\n",
            "Epoch 1, Loss: 0.6658987352610244\n",
            "Epoch 1, Loss: 0.6677840291081792\n",
            "Epoch 1, Loss: 0.6695765555667146\n",
            "Epoch 1, Loss: 0.6715707382582643\n",
            "Epoch 1, Loss: 0.6732927016589952\n",
            "Epoch 1, Loss: 0.6751676814635391\n",
            "Epoch 1, Loss: 0.6769035859486027\n",
            "Epoch 1, Loss: 0.6786315186554209\n",
            "Epoch 1, Loss: 0.6805490961160197\n",
            "Epoch 1, Loss: 0.6824428330906822\n",
            "Epoch 1, Loss: 0.6842485908657083\n",
            "Epoch 1, Loss: 0.6859388552663271\n",
            "Epoch 1, Loss: 0.688010488629646\n",
            "Epoch 1, Loss: 0.6901445547333154\n",
            "Epoch 1, Loss: 0.6916856928859525\n",
            "Epoch 1, Loss: 0.6935202936687128\n",
            "Epoch 1, Loss: 0.6953998491587237\n",
            "Epoch 1, Loss: 0.6971255559140764\n",
            "Epoch 1, Loss: 0.6989616068732708\n",
            "Epoch 1, Loss: 0.7006243389585743\n",
            "Epoch 1, Loss: 0.7024619951272559\n",
            "Epoch 1, Loss: 0.7041436836237798\n",
            "Epoch 1, Loss: 0.7059091003349675\n",
            "Epoch 1, Loss: 0.7076593794481224\n",
            "Epoch 1, Loss: 0.7095836786662831\n",
            "Epoch 1, Loss: 0.711455656134564\n",
            "Epoch 1, Loss: 0.7133119453859451\n",
            "Epoch 1, Loss: 0.7151268071225841\n",
            "Epoch 1, Loss: 0.7167423585491717\n",
            "Epoch 1, Loss: 0.7183702121610227\n",
            "Epoch 1, Loss: 0.72053823477167\n",
            "Epoch 1, Loss: 0.7222746147219178\n",
            "Epoch 1, Loss: 0.7238194982109167\n",
            "Epoch 1, Loss: 0.7258690336476201\n",
            "Epoch 1, Loss: 0.7277489876198342\n",
            "Epoch 1, Loss: 0.7295366684189233\n",
            "Epoch 1, Loss: 0.7314909492307307\n",
            "Epoch 1, Loss: 0.7333707842985382\n",
            "Epoch 1, Loss: 0.7350194027356785\n",
            "Epoch 1, Loss: 0.7366723506651875\n",
            "Epoch 1, Loss: 0.7383158272489563\n",
            "Epoch 1, Loss: 0.7406271235717227\n",
            "Epoch 1, Loss: 0.7422108993201\n",
            "Epoch 1, Loss: 0.7440439471808236\n",
            "Epoch 1, Loss: 0.7455677345890523\n",
            "Epoch 1, Loss: 0.7473605465706047\n",
            "Epoch 1, Loss: 0.7492665570715199\n",
            "Epoch 1, Loss: 0.7510147439244458\n",
            "Epoch 1, Loss: 0.7529743527207533\n",
            "Epoch 1, Loss: 0.7547842078196728\n",
            "Epoch 1, Loss: 0.7565612501805395\n",
            "Epoch 1, Loss: 0.7581831561330029\n",
            "Epoch 1, Loss: 0.7598080174697329\n",
            "Epoch 1, Loss: 0.7618477765251609\n",
            "Epoch 1, Loss: 0.7635760010050996\n",
            "Epoch 1, Loss: 0.765235518250624\n",
            "Epoch 1, Loss: 0.767064053536681\n",
            "Epoch 1, Loss: 0.769068900886399\n",
            "Epoch 1, Loss: 0.771089145442104\n",
            "Epoch 1, Loss: 0.7727953062947753\n",
            "Epoch 1, Loss: 0.77465052555894\n",
            "Epoch 1, Loss: 0.7761697481050516\n",
            "Epoch 1, Loss: 0.7778821645490349\n",
            "Epoch 1, Loss: 0.7798295153681275\n",
            "Epoch 1, Loss: 0.781624917636442\n",
            "Epoch 1, Loss: 0.7831902653360001\n",
            "Epoch 1, Loss: 0.7850695415530973\n",
            "Epoch 1, Loss: 0.7870100097887961\n",
            "Epoch 1, Loss: 0.7889614461937828\n",
            "Epoch 1, Loss: 0.7908160002030376\n",
            "Epoch 1, Loss: 0.7927909485824273\n",
            "Epoch 1, Loss: 0.7943447793231291\n",
            "Epoch 1, Loss: 0.7960676220067017\n",
            "Epoch 1, Loss: 0.7976403875119241\n",
            "Epoch 1, Loss: 0.7994135804188526\n",
            "Epoch 1, Loss: 0.801381335996301\n",
            "Epoch 1, Loss: 0.8031022318488802\n",
            "Epoch 1, Loss: 0.8045915997851535\n",
            "Epoch 1, Loss: 0.8064050425958755\n",
            "Epoch 1, Loss: 0.8082338804784028\n",
            "Epoch 1, Loss: 0.8100433777970122\n",
            "Epoch 1, Loss: 0.8115670060562661\n",
            "Epoch 1, Loss: 0.8132534600279825\n",
            "Epoch 1, Loss: 0.8150372314636055\n",
            "Epoch 1, Loss: 0.8171578464300736\n",
            "Epoch 1, Loss: 0.8187952698649043\n",
            "Epoch 1, Loss: 0.8204436210720131\n",
            "Epoch 1, Loss: 0.8218412672162361\n",
            "Epoch 1, Loss: 0.823277913396011\n",
            "Epoch 1, Loss: 0.8251420373806868\n",
            "Epoch 1, Loss: 0.8267818526233859\n",
            "Epoch 1, Loss: 0.8283501496095487\n",
            "Epoch 1, Loss: 0.8300724212470871\n",
            "Epoch 1, Loss: 0.8324119840436579\n",
            "Epoch 1, Loss: 0.8340195171973285\n",
            "Epoch 1, Loss: 0.8355909663697948\n",
            "Epoch 1, Loss: 0.8372714021016875\n",
            "Epoch 1, Loss: 0.8391561773427002\n",
            "Epoch 1, Loss: 0.8411591078924097\n",
            "Epoch 1, Loss: 0.8428340938390063\n",
            "Epoch 1, Loss: 0.8448847099338346\n",
            "Epoch 1, Loss: 0.8467915212101949\n",
            "Epoch 1, Loss: 0.8485375040632379\n",
            "Epoch 1, Loss: 0.8505001325741448\n",
            "Epoch 1, Loss: 0.8523816551698749\n",
            "Epoch 1, Loss: 0.8541114769323402\n",
            "Epoch 1, Loss: 0.8558613840881211\n",
            "Epoch 1, Loss: 0.8576083140604941\n",
            "Epoch 1, Loss: 0.8592133886368988\n",
            "Epoch 1, Loss: 0.8609252834259091\n",
            "Epoch 1, Loss: 0.8625796135429227\n",
            "Epoch 1, Loss: 0.8641921906824916\n",
            "Epoch 1, Loss: 0.8660090881235459\n",
            "Epoch 1, Loss: 0.8679096521928792\n",
            "Epoch 1, Loss: 0.8698689434534449\n",
            "Epoch 1, Loss: 0.8716095236256299\n",
            "Epoch 1, Loss: 0.8734425391687457\n",
            "Epoch 1, Loss: 0.8749364577900723\n",
            "Epoch 1, Loss: 0.8768735764276646\n",
            "Epoch 1, Loss: 0.878652778427924\n",
            "Epoch 1, Loss: 0.8804279254830402\n",
            "Epoch 1, Loss: 0.8822119030196344\n",
            "Epoch 1, Loss: 0.8838455745631166\n",
            "Epoch 1, Loss: 0.885506895192139\n",
            "Epoch 1, Loss: 0.8872675163971494\n",
            "Epoch 1, Loss: 0.8890481518052727\n",
            "Epoch 1, Loss: 0.8909748650877677\n",
            "Epoch 1, Loss: 0.8927386018931104\n",
            "Epoch 1, Loss: 0.8947277189520619\n",
            "Epoch 1, Loss: 0.8965404556535393\n",
            "Epoch 1, Loss: 0.8981423112742432\n",
            "Epoch 1, Loss: 0.8997875163927103\n",
            "Epoch 1, Loss: 0.9012556152270578\n",
            "Epoch 1, Loss: 0.9033082007142283\n",
            "Epoch 1, Loss: 0.9053096355074812\n",
            "Epoch 1, Loss: 0.9073161815133546\n",
            "Epoch 1, Loss: 0.9092031029789039\n",
            "Epoch 1, Loss: 0.911023104739616\n",
            "Epoch 1, Loss: 0.9130327841814827\n",
            "Epoch 1, Loss: 0.9148389832747866\n",
            "Epoch 1, Loss: 0.9167062312440799\n",
            "Epoch 1, Loss: 0.9183743364365814\n",
            "Epoch 1, Loss: 0.9199738793665796\n",
            "Epoch 1, Loss: 0.9217660012452499\n",
            "Epoch 1, Loss: 0.923638760586224\n",
            "Epoch 1, Loss: 0.9254498237844013\n",
            "Epoch 1, Loss: 0.9271068627877004\n",
            "Epoch 1, Loss: 0.9290578732710055\n",
            "Epoch 1, Loss: 0.9308673259242416\n",
            "Epoch 1, Loss: 0.9326187149642984\n",
            "Epoch 1, Loss: 0.9343581100558991\n",
            "Epoch 1, Loss: 0.9361664619287262\n",
            "Epoch 1, Loss: 0.9382046044939925\n",
            "Epoch 1, Loss: 0.9399473552813615\n",
            "Epoch 1, Loss: 0.9418404814227462\n",
            "Epoch 1, Loss: 0.943714801765159\n",
            "Epoch 1, Loss: 0.9451775204799974\n",
            "Epoch 1, Loss: 0.9468292017726947\n",
            "Epoch 1, Loss: 0.9484751230615485\n",
            "Epoch 1, Loss: 0.9502811236759586\n",
            "Epoch 1, Loss: 0.9522823069406592\n",
            "Epoch 1, Loss: 0.9540202666426558\n",
            "Epoch 1, Loss: 0.9558610614303433\n",
            "Epoch 1, Loss: 0.9572757308745323\n",
            "Epoch 1, Loss: 0.9588735216413923\n",
            "Epoch 1, Loss: 0.9609057709689031\n",
            "Epoch 1, Loss: 0.9624853030495022\n",
            "Epoch 1, Loss: 0.9641626195224655\n",
            "Epoch 1, Loss: 0.9658331194192248\n",
            "Epoch 1, Loss: 0.9675346588539651\n",
            "Epoch 1, Loss: 0.9692728536208267\n",
            "Epoch 1, Loss: 0.9713227181788295\n",
            "Epoch 1, Loss: 0.9730403600141521\n",
            "Epoch 1, Loss: 0.974673174073934\n",
            "Epoch 1, Loss: 0.9763159322006928\n",
            "Epoch 1, Loss: 0.9781290343045579\n",
            "Epoch 1, Loss: 0.9800317372812335\n",
            "Epoch 1, Loss: 0.9818074360223072\n",
            "Epoch 1, Loss: 0.9836755445241319\n",
            "Epoch 1, Loss: 0.9852039836861594\n",
            "Epoch 1, Loss: 0.9867264636032417\n",
            "Epoch 1, Loss: 0.9884153333161493\n",
            "Epoch 1, Loss: 0.9899335520346756\n",
            "Epoch 1, Loss: 0.9914641509885374\n",
            "Epoch 1, Loss: 0.9932310381508849\n",
            "Epoch 1, Loss: 0.9949760952264147\n",
            "Epoch 1, Loss: 0.9967283420550549\n",
            "Epoch 1, Loss: 0.9985145626165678\n",
            "Epoch 1, Loss: 1.0003889710701945\n",
            "Epoch 1, Loss: 1.0021164257203221\n",
            "Epoch 1, Loss: 1.003719568709888\n",
            "Epoch 1, Loss: 1.005547902925545\n",
            "Epoch 1, Loss: 1.007176287643745\n",
            "Epoch 1, Loss: 1.0091012275737266\n",
            "Epoch 1, Loss: 1.0110118084246544\n",
            "Epoch 1, Loss: 1.0126078746202962\n",
            "Epoch 1, Loss: 1.0142732227549833\n",
            "Epoch 1, Loss: 1.0159079601697605\n",
            "Epoch 1, Loss: 1.0174541801137997\n",
            "Epoch 1, Loss: 1.0191414324218964\n",
            "Epoch 1, Loss: 1.0208563947921518\n",
            "Epoch 1, Loss: 1.0226086327791823\n",
            "Epoch 1, Loss: 1.0240597014536943\n",
            "Epoch 1, Loss: 1.0255682276337958\n",
            "Epoch 1, Loss: 1.0272944554343553\n",
            "Epoch 1, Loss: 1.0288884661081807\n",
            "Epoch 1, Loss: 1.0306996538511017\n",
            "Epoch 1, Loss: 1.0323074540823622\n",
            "Epoch 1, Loss: 1.033865790842744\n",
            "Epoch 1, Loss: 1.0358410652946024\n",
            "Epoch 1, Loss: 1.0372911956913942\n",
            "Epoch 1, Loss: 1.0389908024722048\n",
            "Epoch 1, Loss: 1.0406343496364097\n",
            "Epoch 1, Loss: 1.0422013270885437\n",
            "Epoch 1, Loss: 1.0441901439900898\n",
            "Epoch 1, Loss: 1.0458273064449926\n",
            "Epoch 1, Loss: 1.0474888450654267\n",
            "Epoch 1, Loss: 1.048934491089238\n",
            "Epoch 1, Loss: 1.050915117154036\n",
            "Epoch 1, Loss: 1.0524258923042766\n",
            "Epoch 1, Loss: 1.0538990049410963\n",
            "Epoch 1, Loss: 1.055501240293693\n",
            "Epoch 1, Loss: 1.057139663897512\n",
            "Epoch 1, Loss: 1.0586185340991106\n",
            "Epoch 1, Loss: 1.0601816127062453\n",
            "Epoch 1, Loss: 1.0620676907127167\n",
            "Epoch 1, Loss: 1.0635071607197033\n",
            "Epoch 1, Loss: 1.0650265576589444\n",
            "Epoch 1, Loss: 1.0669009729724406\n",
            "Epoch 1, Loss: 1.068392599939995\n",
            "Epoch 1, Loss: 1.070013347489145\n",
            "Epoch 1, Loss: 1.0715336624313803\n",
            "Epoch 1, Loss: 1.073236363013382\n",
            "Epoch 1, Loss: 1.074894061631254\n",
            "Epoch 1, Loss: 1.0764119815643487\n",
            "Epoch 1, Loss: 1.0781807660141869\n",
            "Epoch 1, Loss: 1.079807204358718\n",
            "Epoch 1, Loss: 1.0815538271613743\n",
            "Epoch 1, Loss: 1.0832046488361895\n",
            "Epoch 1, Loss: 1.0845906345740608\n",
            "Epoch 1, Loss: 1.0864357772995443\n",
            "Epoch 1, Loss: 1.088064201804988\n",
            "Epoch 1, Loss: 1.0895870064225648\n",
            "Epoch 1, Loss: 1.0914432662527274\n",
            "Epoch 1, Loss: 1.093247567296333\n",
            "Epoch 1, Loss: 1.0947520645987956\n",
            "Epoch 1, Loss: 1.0964428436420763\n",
            "Epoch 1, Loss: 1.0980706574666836\n",
            "Epoch 1, Loss: 1.099600276831166\n",
            "Epoch 1, Loss: 1.1011936231647306\n",
            "Epoch 1, Loss: 1.1027651703571115\n",
            "Epoch 1, Loss: 1.104390056694255\n",
            "Epoch 1, Loss: 1.1058803827256498\n",
            "Epoch 1, Loss: 1.1073327973065779\n",
            "Epoch 1, Loss: 1.1087694367789247\n",
            "Epoch 1, Loss: 1.11036123781253\n",
            "Epoch 1, Loss: 1.1122037474151767\n",
            "Epoch 1, Loss: 1.1141967136231834\n",
            "Epoch 1, Loss: 1.1157269561687089\n",
            "Epoch 1, Loss: 1.1175080295413962\n",
            "Epoch 1, Loss: 1.1192652273665913\n",
            "Epoch 1, Loss: 1.1211787489673974\n",
            "Epoch 1, Loss: 1.1227906366138507\n",
            "Epoch 1, Loss: 1.1244719850132838\n",
            "Epoch 1, Loss: 1.126207714647893\n",
            "Epoch 1, Loss: 1.1277074710182522\n",
            "Epoch 1, Loss: 1.129405414661788\n",
            "Epoch 1, Loss: 1.1311141081783167\n",
            "Epoch 1, Loss: 1.132812209446412\n",
            "Epoch 1, Loss: 1.1343356433426937\n",
            "Epoch 1, Loss: 1.1358692833529713\n",
            "Epoch 1, Loss: 1.1376180967406544\n",
            "Epoch 1, Loss: 1.1394365454268882\n",
            "Epoch 1, Loss: 1.1411625406016475\n",
            "Epoch 1, Loss: 1.1428920245536454\n",
            "Epoch 1, Loss: 1.1447059242316828\n",
            "Epoch 1, Loss: 1.1461485093816772\n",
            "Epoch 1, Loss: 1.147666419558513\n",
            "Epoch 1, Loss: 1.1493000287534025\n",
            "Epoch 1, Loss: 1.1507739546658742\n",
            "Epoch 1, Loss: 1.1524325763173116\n",
            "Epoch 1, Loss: 1.1540337142432133\n",
            "Epoch 1, Loss: 1.155886638042567\n",
            "Epoch 1, Loss: 1.1575087255529126\n",
            "Epoch 1, Loss: 1.159119492144231\n",
            "Epoch 1, Loss: 1.1606247614106864\n",
            "Epoch 1, Loss: 1.1623447070951047\n",
            "Epoch 1, Loss: 1.164455255584034\n",
            "Epoch 1, Loss: 1.1661785176343016\n",
            "Epoch 1, Loss: 1.167513605273898\n",
            "Epoch 1, Loss: 1.1692525421262092\n",
            "Epoch 1, Loss: 1.170961387017194\n",
            "Epoch 1, Loss: 1.1724217572175633\n",
            "Epoch 1, Loss: 1.174061714535784\n",
            "Epoch 1, Loss: 1.1758193774601382\n",
            "Epoch 1, Loss: 1.1773927554754955\n",
            "Epoch 1, Loss: 1.1791769679245132\n",
            "Epoch 1, Loss: 1.1807751347646689\n",
            "Epoch 1, Loss: 1.1823131271335474\n",
            "Epoch 1, Loss: 1.1838242025936352\n",
            "Epoch 1, Loss: 1.185396440193781\n",
            "Epoch 1, Loss: 1.1869700505300556\n",
            "Epoch 1, Loss: 1.1888745832626166\n",
            "Epoch 1, Loss: 1.1903375219506072\n",
            "Epoch 1, Loss: 1.191894583689892\n",
            "Epoch 1, Loss: 1.1935413630722125\n",
            "Epoch 1, Loss: 1.1951910970765915\n",
            "Epoch 1, Loss: 1.1968942225131842\n",
            "Epoch 1, Loss: 1.1986440833267349\n",
            "Epoch 1, Loss: 1.1999918515115138\n",
            "Epoch 1, Loss: 1.2016515626626856\n",
            "Epoch 1, Loss: 1.203244559295342\n",
            "Epoch 1, Loss: 1.2049356854480247\n",
            "Epoch 1, Loss: 1.206701959185588\n",
            "Epoch 1, Loss: 1.2082312078122288\n",
            "Epoch 1, Loss: 1.2096470175191874\n",
            "Epoch 1, Loss: 1.2110744697968368\n",
            "Epoch 1, Loss: 1.2125201490529054\n",
            "Epoch 1, Loss: 1.2138543157931179\n",
            "Epoch 1, Loss: 1.2154823405968258\n",
            "Epoch 1, Loss: 1.2169048677929832\n",
            "Epoch 1, Loss: 1.2186495047396102\n",
            "Epoch 1, Loss: 1.220156427539523\n",
            "Epoch 1, Loss: 1.2216917378518282\n",
            "Epoch 1, Loss: 1.2234797787178509\n",
            "Epoch 1, Loss: 1.2252351632508476\n",
            "Epoch 1, Loss: 1.2270496178161152\n",
            "Epoch 1, Loss: 1.2285110176066913\n",
            "Epoch 1, Loss: 1.2301560443685489\n",
            "Epoch 1, Loss: 1.231736776773887\n",
            "Epoch 1, Loss: 1.2332958603454063\n",
            "Epoch 1, Loss: 1.2348639202849638\n",
            "Epoch 1, Loss: 1.236661902321574\n",
            "Epoch 1, Loss: 1.238347323959136\n",
            "Epoch 1, Loss: 1.2399087321118016\n",
            "Epoch 1, Loss: 1.2414916468703228\n",
            "Epoch 1, Loss: 1.2430896018167286\n",
            "Epoch 1, Loss: 1.2445368899408813\n",
            "Epoch 1, Loss: 1.2460448363857806\n",
            "Epoch 1, Loss: 1.2476576752674855\n",
            "Epoch 1, Loss: 1.2489715448730743\n",
            "Epoch 1, Loss: 1.2506092158729767\n",
            "Epoch 1, Loss: 1.252416073056438\n",
            "Epoch 1, Loss: 1.2537443985414627\n",
            "Epoch 1, Loss: 1.255104147717166\n",
            "Epoch 1, Loss: 1.256976702329143\n",
            "Epoch 1, Loss: 1.2587660147101067\n",
            "Epoch 1, Loss: 1.260365603372569\n",
            "Epoch 1, Loss: 1.2617748449830448\n",
            "Epoch 1, Loss: 1.2631178902238227\n",
            "Epoch 1, Loss: 1.264835738312558\n",
            "Epoch 1, Loss: 1.2661637025111168\n",
            "Epoch 1, Loss: 1.2678316723355247\n",
            "Epoch 1, Loss: 1.2694372077427252\n",
            "Epoch 1, Loss: 1.2710234184399285\n",
            "Epoch 1, Loss: 1.272670212609079\n",
            "Epoch 1, Loss: 1.2741023677084453\n",
            "Epoch 1, Loss: 1.2758436680144971\n",
            "Epoch 1, Loss: 1.2771453343693862\n",
            "Epoch 1, Loss: 1.2785043086847077\n",
            "Epoch 1, Loss: 1.2800567467194384\n",
            "Epoch 1, Loss: 1.2815079748478082\n",
            "Epoch 1, Loss: 1.2831021016820923\n",
            "Epoch 1, Loss: 1.2847090605884561\n",
            "Epoch 1, Loss: 1.2863331963034237\n",
            "Epoch 1, Loss: 1.2879497882960094\n",
            "Epoch 1, Loss: 1.2897633695236557\n",
            "Epoch 1, Loss: 1.2911249588212699\n",
            "Epoch 1, Loss: 1.2926729380932\n",
            "Epoch 1, Loss: 1.2942616591977951\n",
            "Epoch 1, Loss: 1.2956753924984457\n",
            "Epoch 1, Loss: 1.2973256135535667\n",
            "Epoch 1, Loss: 1.2986707431276132\n",
            "Epoch 1, Loss: 1.3001690020646586\n",
            "Epoch 1, Loss: 1.3018966567180956\n",
            "Epoch 1, Loss: 1.3032903486810377\n",
            "Epoch 1, Loss: 1.3049411694412036\n",
            "Epoch 1, Loss: 1.3064503515772807\n",
            "Epoch 1, Loss: 1.3082853984040068\n",
            "Epoch 1, Loss: 1.3097014939388656\n",
            "Epoch 1, Loss: 1.3111991274082446\n",
            "Epoch 1, Loss: 1.3129590998220322\n",
            "Epoch 1, Loss: 1.3144074393355327\n",
            "Epoch 1, Loss: 1.3159086495409231\n",
            "Epoch 1, Loss: 1.3174015381147184\n",
            "Epoch 1, Loss: 1.3187587055403862\n",
            "Epoch 1, Loss: 1.320434812847001\n",
            "Epoch 1, Loss: 1.3219161266865938\n",
            "Epoch 1, Loss: 1.323324087788077\n",
            "Epoch 1, Loss: 1.3247760203488343\n",
            "Epoch 1, Loss: 1.3265788158797243\n",
            "Epoch 1, Loss: 1.3282571627051019\n",
            "Epoch 1, Loss: 1.3297011009262651\n",
            "Epoch 1, Loss: 1.331220991318793\n",
            "Epoch 1, Loss: 1.333051289896221\n",
            "Epoch 1, Loss: 1.3345761497307311\n",
            "Epoch 1, Loss: 1.3364067466362664\n",
            "Epoch 1, Loss: 1.337918776532878\n",
            "Epoch 1, Loss: 1.3398981859616916\n",
            "Epoch 1, Loss: 1.3413901164403657\n",
            "Epoch 1, Loss: 1.3430832083267934\n",
            "Epoch 1, Loss: 1.344652297399233\n",
            "Epoch 1, Loss: 1.3463203756095807\n",
            "Epoch 1, Loss: 1.3477128417900457\n",
            "Epoch 1, Loss: 1.3494226897464079\n",
            "Epoch 1, Loss: 1.3511181856360277\n",
            "Epoch 1, Loss: 1.3525691983644919\n",
            "Epoch 1, Loss: 1.3540342325139838\n",
            "Epoch 1, Loss: 1.355450609455938\n",
            "Epoch 1, Loss: 1.3569894483327256\n",
            "Epoch 1, Loss: 1.3588322699832185\n",
            "Epoch 1, Loss: 1.3600761846965537\n",
            "Epoch 1, Loss: 1.361530349763763\n",
            "Epoch 1, Loss: 1.3629868397170015\n",
            "Epoch 1, Loss: 1.3644734473941882\n",
            "Epoch 1, Loss: 1.3660136805013623\n",
            "Epoch 1, Loss: 1.3675226397678981\n",
            "Epoch 1, Loss: 1.3690497168647053\n",
            "Epoch 1, Loss: 1.370494543820086\n",
            "Epoch 1, Loss: 1.3718243466161402\n",
            "Epoch 1, Loss: 1.3733498796325205\n",
            "Epoch 1, Loss: 1.375097341168567\n",
            "Epoch 1, Loss: 1.3764766673450275\n",
            "Epoch 1, Loss: 1.3781473849283155\n",
            "Epoch 1, Loss: 1.3795295309685076\n",
            "Epoch 1, Loss: 1.3812323196617233\n",
            "Epoch 1, Loss: 1.3831333267261914\n",
            "Epoch 1, Loss: 1.3847201139573246\n",
            "Epoch 1, Loss: 1.3861970236081906\n",
            "Epoch 1, Loss: 1.3877609295155995\n",
            "Epoch 1, Loss: 1.389350381729853\n",
            "Epoch 1, Loss: 1.390949119463601\n",
            "Epoch 1, Loss: 1.3923568990072022\n",
            "Epoch 1, Loss: 1.3941769272927433\n",
            "Epoch 1, Loss: 1.3955817746231929\n",
            "Epoch 1, Loss: 1.3972463256410321\n",
            "Epoch 1, Loss: 1.3986641048927746\n",
            "Epoch 1, Loss: 1.4002316956172514\n",
            "Epoch 1, Loss: 1.4016059570757629\n",
            "Epoch 1, Loss: 1.4034171763740841\n",
            "Epoch 1, Loss: 1.4048355914595183\n",
            "Epoch 1, Loss: 1.4066264314572219\n",
            "Epoch 1, Loss: 1.4081126005600786\n",
            "Epoch 1, Loss: 1.409848914350695\n",
            "Epoch 1, Loss: 1.4112977512809626\n",
            "Epoch 1, Loss: 1.413010495626713\n",
            "Epoch 1, Loss: 1.4144741392044156\n",
            "Epoch 1, Loss: 1.415946270270116\n",
            "Epoch 1, Loss: 1.4176142365883684\n",
            "Epoch 1, Loss: 1.4189405425277817\n",
            "Epoch 1, Loss: 1.4203079974712314\n",
            "Epoch 1, Loss: 1.4221461469407581\n",
            "Epoch 1, Loss: 1.4236356991788615\n",
            "Epoch 1, Loss: 1.4249145374121264\n",
            "Epoch 1, Loss: 1.4261806070652154\n",
            "Epoch 1, Loss: 1.4275900997469186\n",
            "Epoch 1, Loss: 1.4290719480465746\n",
            "Epoch 1, Loss: 1.4304378249151322\n",
            "Epoch 1, Loss: 1.432070812605836\n",
            "Epoch 1, Loss: 1.4335443847014775\n",
            "Epoch 1, Loss: 1.435122044037675\n",
            "Epoch 1, Loss: 1.4366750451914794\n",
            "Epoch 1, Loss: 1.438098796188374\n",
            "Epoch 1, Loss: 1.4393919678905127\n",
            "Epoch 1, Loss: 1.4407581147330497\n",
            "Epoch 1, Loss: 1.442084397501348\n",
            "Epoch 1, Loss: 1.44367134921691\n",
            "Epoch 1, Loss: 1.4450796909649353\n",
            "Epoch 1, Loss: 1.4465313904425676\n",
            "Epoch 1, Loss: 1.447683890914673\n",
            "Epoch 1, Loss: 1.4490722341610647\n",
            "Epoch 1, Loss: 1.4505402592137038\n",
            "Epoch 1, Loss: 1.4520601771981514\n",
            "Epoch 1, Loss: 1.4535860827816722\n",
            "Epoch 1, Loss: 1.4552289732276935\n",
            "Epoch 2, Loss: 0.0017772656877327454\n",
            "Epoch 2, Loss: 0.003096118759926018\n",
            "Epoch 2, Loss: 0.0044259189644737925\n",
            "Epoch 2, Loss: 0.00636362861794279\n",
            "Epoch 2, Loss: 0.007888347901346739\n",
            "Epoch 2, Loss: 0.009121321046443852\n",
            "Epoch 2, Loss: 0.010985235423992967\n",
            "Epoch 2, Loss: 0.012384180217752677\n",
            "Epoch 2, Loss: 0.013578381227410358\n",
            "Epoch 2, Loss: 0.014999137052794551\n",
            "Epoch 2, Loss: 0.01668381553781612\n",
            "Epoch 2, Loss: 0.01803619645135787\n",
            "Epoch 2, Loss: 0.019477490878775906\n",
            "Epoch 2, Loss: 0.02086903798915541\n",
            "Epoch 2, Loss: 0.022414566001014026\n",
            "Epoch 2, Loss: 0.02389105460832796\n",
            "Epoch 2, Loss: 0.025299362057005354\n",
            "Epoch 2, Loss: 0.026857601712121988\n",
            "Epoch 2, Loss: 0.028467481703404577\n",
            "Epoch 2, Loss: 0.030090437368358796\n",
            "Epoch 2, Loss: 0.03175624953511426\n",
            "Epoch 2, Loss: 0.03323662982267492\n",
            "Epoch 2, Loss: 0.034763612100840224\n",
            "Epoch 2, Loss: 0.036443442030026175\n",
            "Epoch 2, Loss: 0.03799891410886174\n",
            "Epoch 2, Loss: 0.03929868012742923\n",
            "Epoch 2, Loss: 0.04055189811969962\n",
            "Epoch 2, Loss: 0.0420101653889317\n",
            "Epoch 2, Loss: 0.04328518961091785\n",
            "Epoch 2, Loss: 0.04489885000011805\n",
            "Epoch 2, Loss: 0.04617356575663437\n",
            "Epoch 2, Loss: 0.047353786047157426\n",
            "Epoch 2, Loss: 0.049050491865333695\n",
            "Epoch 2, Loss: 0.05047191995793901\n",
            "Epoch 2, Loss: 0.05191874786106217\n",
            "Epoch 2, Loss: 0.05354700559545356\n",
            "Epoch 2, Loss: 0.054904035366404695\n",
            "Epoch 2, Loss: 0.05633285763623464\n",
            "Epoch 2, Loss: 0.057785347523286824\n",
            "Epoch 2, Loss: 0.059229595658114496\n",
            "Epoch 2, Loss: 0.06047040955794742\n",
            "Epoch 2, Loss: 0.061645345690915045\n",
            "Epoch 2, Loss: 0.06294816191239125\n",
            "Epoch 2, Loss: 0.06445047579457998\n",
            "Epoch 2, Loss: 0.06584988035204466\n",
            "Epoch 2, Loss: 0.06721584639890725\n",
            "Epoch 2, Loss: 0.06898252456389425\n",
            "Epoch 2, Loss: 0.07006449071342682\n",
            "Epoch 2, Loss: 0.0713714919126857\n",
            "Epoch 2, Loss: 0.07272458838684784\n",
            "Epoch 2, Loss: 0.07409544795980234\n",
            "Epoch 2, Loss: 0.07545149494010164\n",
            "Epoch 2, Loss: 0.07697051138524204\n",
            "Epoch 2, Loss: 0.07837211643643391\n",
            "Epoch 2, Loss: 0.07983043279184406\n",
            "Epoch 2, Loss: 0.08118222756763858\n",
            "Epoch 2, Loss: 0.0828597208727961\n",
            "Epoch 2, Loss: 0.08422514407531075\n",
            "Epoch 2, Loss: 0.08544109910345443\n",
            "Epoch 2, Loss: 0.08687224610687216\n",
            "Epoch 2, Loss: 0.08837066358312622\n",
            "Epoch 2, Loss: 0.08981927108886602\n",
            "Epoch 2, Loss: 0.09141839419484443\n",
            "Epoch 2, Loss: 0.09311264600900128\n",
            "Epoch 2, Loss: 0.0947505453663409\n",
            "Epoch 2, Loss: 0.09605429132881067\n",
            "Epoch 2, Loss: 0.09762135369088644\n",
            "Epoch 2, Loss: 0.09890084117269882\n",
            "Epoch 2, Loss: 0.10035357892970599\n",
            "Epoch 2, Loss: 0.10187534801185588\n",
            "Epoch 2, Loss: 0.10309434523972709\n",
            "Epoch 2, Loss: 0.10461977977886834\n",
            "Epoch 2, Loss: 0.10612881557106058\n",
            "Epoch 2, Loss: 0.10762007073368259\n",
            "Epoch 2, Loss: 0.10907870538704231\n",
            "Epoch 2, Loss: 0.11062461922845572\n",
            "Epoch 2, Loss: 0.11184112106442756\n",
            "Epoch 2, Loss: 0.1131356985825102\n",
            "Epoch 2, Loss: 0.11473564441551638\n",
            "Epoch 2, Loss: 0.11630727971911126\n",
            "Epoch 2, Loss: 0.11761042658630234\n",
            "Epoch 2, Loss: 0.11926786910237559\n",
            "Epoch 2, Loss: 0.12073933552293216\n",
            "Epoch 2, Loss: 0.122215445465444\n",
            "Epoch 2, Loss: 0.12385256340741502\n",
            "Epoch 2, Loss: 0.12528426971886775\n",
            "Epoch 2, Loss: 0.1266040330195366\n",
            "Epoch 2, Loss: 0.12780529748448324\n",
            "Epoch 2, Loss: 0.12935092976635984\n",
            "Epoch 2, Loss: 0.13083805101911736\n",
            "Epoch 2, Loss: 0.13228993662787825\n",
            "Epoch 2, Loss: 0.13362945208464133\n",
            "Epoch 2, Loss: 0.13506380447646235\n",
            "Epoch 2, Loss: 0.13636705180263275\n",
            "Epoch 2, Loss: 0.1376558862378835\n",
            "Epoch 2, Loss: 0.13900555186259472\n",
            "Epoch 2, Loss: 0.14076681240745212\n",
            "Epoch 2, Loss: 0.14227720066104704\n",
            "Epoch 2, Loss: 0.14363092260287547\n",
            "Epoch 2, Loss: 0.1448619496029661\n",
            "Epoch 2, Loss: 0.14648427751363086\n",
            "Epoch 2, Loss: 0.14783958896346713\n",
            "Epoch 2, Loss: 0.14942052518315327\n",
            "Epoch 2, Loss: 0.15092250110243288\n",
            "Epoch 2, Loss: 0.15234624905049649\n",
            "Epoch 2, Loss: 0.1535885023796345\n",
            "Epoch 2, Loss: 0.15535677965644681\n",
            "Epoch 2, Loss: 0.1566640343659979\n",
            "Epoch 2, Loss: 0.15803432594174924\n",
            "Epoch 2, Loss: 0.15949813446120534\n",
            "Epoch 2, Loss: 0.16073325878518926\n",
            "Epoch 2, Loss: 0.16195168443348096\n",
            "Epoch 2, Loss: 0.16348236829728421\n",
            "Epoch 2, Loss: 0.16469764351235022\n",
            "Epoch 2, Loss: 0.16629619206613896\n",
            "Epoch 2, Loss: 0.16787305260863145\n",
            "Epoch 2, Loss: 0.1694468240756208\n",
            "Epoch 2, Loss: 0.17092534975932383\n",
            "Epoch 2, Loss: 0.1725106310204167\n",
            "Epoch 2, Loss: 0.17400580652229622\n",
            "Epoch 2, Loss: 0.1756076740334406\n",
            "Epoch 2, Loss: 0.17711081994159142\n",
            "Epoch 2, Loss: 0.17837010541230516\n",
            "Epoch 2, Loss: 0.1797487263941704\n",
            "Epoch 2, Loss: 0.18096990757586096\n",
            "Epoch 2, Loss: 0.18222939754690964\n",
            "Epoch 2, Loss: 0.18339212128268484\n",
            "Epoch 2, Loss: 0.18490743553242112\n",
            "Epoch 2, Loss: 0.18630172537110956\n",
            "Epoch 2, Loss: 0.1875970719567955\n",
            "Epoch 2, Loss: 0.1890809832479033\n",
            "Epoch 2, Loss: 0.19028661104724232\n",
            "Epoch 2, Loss: 0.191660608781878\n",
            "Epoch 2, Loss: 0.19294315469844261\n",
            "Epoch 2, Loss: 0.19439949800291328\n",
            "Epoch 2, Loss: 0.19561374416131802\n",
            "Epoch 2, Loss: 0.19728927112296415\n",
            "Epoch 2, Loss: 0.19862082806389655\n",
            "Epoch 2, Loss: 0.20004042716282408\n",
            "Epoch 2, Loss: 0.20157339490588058\n",
            "Epoch 2, Loss: 0.2028656197935724\n",
            "Epoch 2, Loss: 0.20422303417454596\n",
            "Epoch 2, Loss: 0.20578342782871803\n",
            "Epoch 2, Loss: 0.2072244536541307\n",
            "Epoch 2, Loss: 0.20855272776635406\n",
            "Epoch 2, Loss: 0.21000291349942726\n",
            "Epoch 2, Loss: 0.21133691316370465\n",
            "Epoch 2, Loss: 0.2127471421380787\n",
            "Epoch 2, Loss: 0.21422807777019412\n",
            "Epoch 2, Loss: 0.21568745969201597\n",
            "Epoch 2, Loss: 0.2172178972102797\n",
            "Epoch 2, Loss: 0.21843130044315173\n",
            "Epoch 2, Loss: 0.21984690465890538\n",
            "Epoch 2, Loss: 0.22129381945371018\n",
            "Epoch 2, Loss: 0.2225294993509112\n",
            "Epoch 2, Loss: 0.22412552118606274\n",
            "Epoch 2, Loss: 0.22545541834343424\n",
            "Epoch 2, Loss: 0.22700252946075575\n",
            "Epoch 2, Loss: 0.2283193158828999\n",
            "Epoch 2, Loss: 0.2297679508281181\n",
            "Epoch 2, Loss: 0.23101679763525648\n",
            "Epoch 2, Loss: 0.23235911237614235\n",
            "Epoch 2, Loss: 0.23392430321334878\n",
            "Epoch 2, Loss: 0.2351716098273197\n",
            "Epoch 2, Loss: 0.2367118722032708\n",
            "Epoch 2, Loss: 0.2380024868509044\n",
            "Epoch 2, Loss: 0.23930892828480363\n",
            "Epoch 2, Loss: 0.2407833874378058\n",
            "Epoch 2, Loss: 0.24220773859706987\n",
            "Epoch 2, Loss: 0.24348411337493936\n",
            "Epoch 2, Loss: 0.2448680024317768\n",
            "Epoch 2, Loss: 0.24643091121902855\n",
            "Epoch 2, Loss: 0.24784585109452154\n",
            "Epoch 2, Loss: 0.2492346115734266\n",
            "Epoch 2, Loss: 0.2505759166939484\n",
            "Epoch 2, Loss: 0.25171193289939703\n",
            "Epoch 2, Loss: 0.2530224053451167\n",
            "Epoch 2, Loss: 0.2544096054323494\n",
            "Epoch 2, Loss: 0.2557691509461464\n",
            "Epoch 2, Loss: 0.2571320724304375\n",
            "Epoch 2, Loss: 0.2582548320903193\n",
            "Epoch 2, Loss: 0.2593626408168422\n",
            "Epoch 2, Loss: 0.2605484422972745\n",
            "Epoch 2, Loss: 0.2618536041368304\n",
            "Epoch 2, Loss: 0.26357572333282214\n",
            "Epoch 2, Loss: 0.2648445952426442\n",
            "Epoch 2, Loss: 0.26623111986138326\n",
            "Epoch 2, Loss: 0.26762477600056195\n",
            "Epoch 2, Loss: 0.26892693702827025\n",
            "Epoch 2, Loss: 0.2702481171206745\n",
            "Epoch 2, Loss: 0.2714318987506125\n",
            "Epoch 2, Loss: 0.27310405736384186\n",
            "Epoch 2, Loss: 0.27439238371141733\n",
            "Epoch 2, Loss: 0.27585955814022545\n",
            "Epoch 2, Loss: 0.27734070155016904\n",
            "Epoch 2, Loss: 0.27861469061783206\n",
            "Epoch 2, Loss: 0.27986184905861955\n",
            "Epoch 2, Loss: 0.2813950038474539\n",
            "Epoch 2, Loss: 0.28303465856920423\n",
            "Epoch 2, Loss: 0.2843223012164426\n",
            "Epoch 2, Loss: 0.28597361360059675\n",
            "Epoch 2, Loss: 0.28752094941675815\n",
            "Epoch 2, Loss: 0.28869270081715204\n",
            "Epoch 2, Loss: 0.2899942496396087\n",
            "Epoch 2, Loss: 0.29150076130466995\n",
            "Epoch 2, Loss: 0.29279568593215455\n",
            "Epoch 2, Loss: 0.2942190062816796\n",
            "Epoch 2, Loss: 0.295459129849968\n",
            "Epoch 2, Loss: 0.2969468381551221\n",
            "Epoch 2, Loss: 0.29825631591974927\n",
            "Epoch 2, Loss: 0.2997255586754635\n",
            "Epoch 2, Loss: 0.30105239785540744\n",
            "Epoch 2, Loss: 0.3026442398957889\n",
            "Epoch 2, Loss: 0.30418360743986067\n",
            "Epoch 2, Loss: 0.305435894166722\n",
            "Epoch 2, Loss: 0.3067700563336882\n",
            "Epoch 2, Loss: 0.30827308653870505\n",
            "Epoch 2, Loss: 0.30982241400367466\n",
            "Epoch 2, Loss: 0.3112078190916944\n",
            "Epoch 2, Loss: 0.3124473787024808\n",
            "Epoch 2, Loss: 0.31353110463722894\n",
            "Epoch 2, Loss: 0.31470701243261545\n",
            "Epoch 2, Loss: 0.3162226846151035\n",
            "Epoch 2, Loss: 0.317332787205801\n",
            "Epoch 2, Loss: 0.3186915990184335\n",
            "Epoch 2, Loss: 0.3196944991493469\n",
            "Epoch 2, Loss: 0.3212080934011113\n",
            "Epoch 2, Loss: 0.322537293290848\n",
            "Epoch 2, Loss: 0.3238003836263476\n",
            "Epoch 2, Loss: 0.32502510480563657\n",
            "Epoch 2, Loss: 0.3263459191907702\n",
            "Epoch 2, Loss: 0.32772465983924964\n",
            "Epoch 2, Loss: 0.32872892035852613\n",
            "Epoch 2, Loss: 0.32995747788178037\n",
            "Epoch 2, Loss: 0.3313803445652623\n",
            "Epoch 2, Loss: 0.33268285773294354\n",
            "Epoch 2, Loss: 0.3342526077919299\n",
            "Epoch 2, Loss: 0.3356960389925086\n",
            "Epoch 2, Loss: 0.33690194050064476\n",
            "Epoch 2, Loss: 0.33812216595005806\n",
            "Epoch 2, Loss: 0.33933510705638115\n",
            "Epoch 2, Loss: 0.34074698880200494\n",
            "Epoch 2, Loss: 0.34225710830115297\n",
            "Epoch 2, Loss: 0.3434257713882515\n",
            "Epoch 2, Loss: 0.34463657015729743\n",
            "Epoch 2, Loss: 0.3458299795380029\n",
            "Epoch 2, Loss: 0.347078550654604\n",
            "Epoch 2, Loss: 0.3486342184683856\n",
            "Epoch 2, Loss: 0.3498713275050873\n",
            "Epoch 2, Loss: 0.3513724206353697\n",
            "Epoch 2, Loss: 0.3527116511788819\n",
            "Epoch 2, Loss: 0.3540816945797952\n",
            "Epoch 2, Loss: 0.35570488835844544\n",
            "Epoch 2, Loss: 0.3568836327861337\n",
            "Epoch 2, Loss: 0.35824727684335633\n",
            "Epoch 2, Loss: 0.35946915155786385\n",
            "Epoch 2, Loss: 0.360599983836074\n",
            "Epoch 2, Loss: 0.3621929208640857\n",
            "Epoch 2, Loss: 0.36393171289692755\n",
            "Epoch 2, Loss: 0.36523108775048607\n",
            "Epoch 2, Loss: 0.3664017635233262\n",
            "Epoch 2, Loss: 0.36778353806346886\n",
            "Epoch 2, Loss: 0.3690802401593884\n",
            "Epoch 2, Loss: 0.37040517031384246\n",
            "Epoch 2, Loss: 0.371456781692822\n",
            "Epoch 2, Loss: 0.3726640462570483\n",
            "Epoch 2, Loss: 0.3739825770678118\n",
            "Epoch 2, Loss: 0.3751140861102687\n",
            "Epoch 2, Loss: 0.37646938536478125\n",
            "Epoch 2, Loss: 0.3781899848709936\n",
            "Epoch 2, Loss: 0.3796315693946751\n",
            "Epoch 2, Loss: 0.38082300031276617\n",
            "Epoch 2, Loss: 0.3822519161817058\n",
            "Epoch 2, Loss: 0.38339490178600905\n",
            "Epoch 2, Loss: 0.3847259719810827\n",
            "Epoch 2, Loss: 0.38612607548303923\n",
            "Epoch 2, Loss: 0.3876874997945088\n",
            "Epoch 2, Loss: 0.3892429640988255\n",
            "Epoch 2, Loss: 0.39056959809244746\n",
            "Epoch 2, Loss: 0.39209971167242436\n",
            "Epoch 2, Loss: 0.39323453288858806\n",
            "Epoch 2, Loss: 0.39455516114259315\n",
            "Epoch 2, Loss: 0.3960512075430292\n",
            "Epoch 2, Loss: 0.39761046444058723\n",
            "Epoch 2, Loss: 0.39900381783085404\n",
            "Epoch 2, Loss: 0.400473028421402\n",
            "Epoch 2, Loss: 0.40159149083030193\n",
            "Epoch 2, Loss: 0.4028253543102528\n",
            "Epoch 2, Loss: 0.40404311996286785\n",
            "Epoch 2, Loss: 0.4052841786838249\n",
            "Epoch 2, Loss: 0.4066668878430906\n",
            "Epoch 2, Loss: 0.40777920144598195\n",
            "Epoch 2, Loss: 0.4091110765324224\n",
            "Epoch 2, Loss: 0.41024185400789653\n",
            "Epoch 2, Loss: 0.4114230637964995\n",
            "Epoch 2, Loss: 0.41299214722860195\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-e28e9f706dec>\u001b[0m in \u001b[0;36m<cell line: 125>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;31m# Train ResNet-10 with batch normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mtrain_resnet_batch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_model_batch_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_resnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_resnet_batch_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-e28e9f706dec>\u001b[0m in \u001b[0;36mtrain_resnet_batch_norm\u001b[0;34m(model, criterion, optimizer, train_loader, num_epochs, device)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOzQotUG6POJQlrZISksH0g",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}